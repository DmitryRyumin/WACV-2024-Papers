# WACV-2024-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/generative_models_for_image_video_3d.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/visualization.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Vision + Language and/or Other Modalities

![Section Papers](https://img.shields.io/badge/Section%20Papers-soon-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-soon-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Zero-Shot Video Moment Retrieval from Frozen Vision-Language Models |  |  |  |
| Investigating the Role of Attribute Context in Vision-Language Models for Object Recognition and Detection |  |  |  |
| Benchmarking Out-of-Distribution Detection in Visual Question Answering |  |  |  |
| Sound3DVDet: 3D Sound Source Detection using Multiview Microphone Array and RGB Images |  |  |  |
| LAVSS: Location-Guided Audio-Visual Spatial Audio Separation |  |  |  |
| Augment the Pairs: Semantics-Preserving Image-Caption Pair Augmentation for Grounding-based Vision and Language Models |  |  |  |
| CLID: Controlled-Length Image Descriptions with Limited Data |  |  |  |
| STYLIP: Multi-Scale Style-Conditioned Prompt Learning for CLIP-based Domain Generalization |  |  |  |
| THInImg: Cross-Modal Steganography for Presenting Talking Heads in Images |  |  |  |
| Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining |  |  |  |
| Temporal Context Enhanced Referring Video Object Segmentation |  |  |  |
| Fine-Grained Alignment for Cross-Modal Recipe Retrieval |  |  |  |
| Learning to Adapt CLIP for Few-Shot Monocular Depth Estimation |  |  |  |
| Annotation-Free Audio-Visual Segmentation |  |  |  |
| Rethink Cross-Modal Fusion in Weakly-Supervised Audio-Visual Video Parsing |  |  |  |
| SDNet: An Extremely Efficient Portrait Matting Model via Self-Distillation |  |  |  |
| FELGA: Unsupervised Fragment Embedding for Fine-Grained Cross-Modal Association |  |  |  |
| Modality-Aware Representation Learning for Zero-Shot Sketch-based Image Retrieval |  |  |  |
| Multitask Vision-Language Prompt Tuning |  |  |  |
| EASUM: Enhancing Affective State Understanding through Joint Sentiment and Emotion Modeling for Multimodal Tasks |  |  |  |
| Complementary-Contradictory Feature Regularization Against Multimodal Overfitting |  |  |  |
| FuseCap: Leveraging Large Language Models for Enriched Fused Image Captions |  |  |  |
| Describe Images in a Boring Way: Towards Cross-Modal Sarcasm Generation |  |  |  |
| Can CLIP Help Sound Source Localization? |  |  |  |
| Domain Aligned CLIP for Few-Shot Classification |  |  |  |
| SCoRD: Subject-Conditional Relation Detection with Text-Augmented Data |  |  |  |
| Simple Token-Level Confidence Improves Caption Correctness |  |  |  |
| Bi-Directional Training for Composed Image Retrieval via Text Prompt Learning |  |  |  |
| MOPA: Modular Object Navigation with PointGoal Agents |  |  |  |
| GIPCOL: Graph-Injected Soft Prompting for Compositional Zero-Shot Learning |  |  |  |
| Text-Guided Face Recognition using Multi-Granularity Cross-Modal Contrastive Learning |  |  |  |
| Leveraging Task-Specific Pre-Training to Reason Across Images and Videos |  |  |  |
| VD-GR: Boosting Visual Dialog with Cascaded Spatial-Temporal Multi-Modal Graphs |  |  |  |
| TriCoLo: Trimodal Contrastive Loss for Text to Shape Retrieval |  |  |  |
