# WACV-2024-Papers

<div align="center">
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/oral_p_a_m_e_r.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/low-level_and_physics-based-vision.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Image Recognition & Understanding

![Section Papers](https://img.shields.io/badge/Section%20Papers-60-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-36-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-33-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-4-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| [Semi-Supervised Semantic Depth Estimation using Symbiotic Transformer and NearFarMix Augmentation](https://openaccess.thecvf.com/content/WACV2024/html/Rahman_Semi-Supervised_Semantic_Depth_Estimation_Using_Symbiotic_Transformer_and_NearFarMix_Augmentation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Rahman_Semi-Supervised_Semantic_Depth_Estimation_Using_Symbiotic_Transformer_and_NearFarMix_Augmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.14400-b31b1b.svg)](http://arxiv.org/abs/2308.14400) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-5YidVH9wIA) |
| [Training Ensembles with Inliers and Outliers for Semi-Supervised Active Learning](https://openaccess.thecvf.com/content/WACV2024/html/Stojnic_Training_Ensembles_With_Inliers_and_Outliers_for_Semi-Supervised_Active_Learning_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/vladan-stojnic/active-outliers?style=flat)](https://github.com/vladan-stojnic/active-outliers) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Stojnic_Training_Ensembles_With_Inliers_and_Outliers_for_Semi-Supervised_Active_Learning_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.03741-b31b1b.svg)](http://arxiv.org/abs/2307.03741) | :heavy_minus_sign: |
| [Multi-View Classification using Hybrid Fusion and Mutual Distillation](https://openaccess.thecvf.com/content/WACV2024/html/Black_Multi-View_Classification_Using_Hybrid_Fusion_and_Mutual_Distillation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/vidarlab/multi-view-hybrid?style=flat)](https://github.com/vidarlab/multi-view-hybrid) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Black_Multi-View_Classification_Using_Hybrid_Fusion_and_Mutual_Distillation_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Amodal Intra-Class Instance Segmentation: Synthetic Datasets and Benchmark](https://openaccess.thecvf.com/content/WACV2024/html/Ao_Amodal_Intra-Class_Instance_Segmentation_Synthetic_Datasets_and_Benchmark_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/saraao/amodal-dataset?style=flat)](https://github.com/saraao/amodal-dataset) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ao_Amodal_Intra-Class_Instance_Segmentation_Synthetic_Datasets_and_Benchmark_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06596-b31b1b.svg)](http://arxiv.org/abs/2303.06596) | :heavy_minus_sign: |
| [Prompting Classes: Exploring the Power of Prompt Class Learning in Weakly Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Murugesan_Prompting_Classes_Exploring_the_Power_of_Prompt_Class_Learning_in_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/Ruxie189/WSS_POLE?style=flat)](https://github.com/Ruxie189/WSS_POLE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Murugesan_Prompting_Classes_Exploring_the_Power_of_Prompt_Class_Learning_in_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.00097-b31b1b.svg)](http://arxiv.org/abs/2307.00097) | :heavy_minus_sign: |
| [RSMPNet: Relationship Guided Semantic Map Prediction](https://openaccess.thecvf.com/content/WACV2024/html/Sun_RSMPNet_Relationship_Guided_Semantic_Map_Prediction_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/jws39/semantic-map-prediction?style=flat)](https://github.com/jws39/semantic-map-prediction) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Sun_RSMPNet_Relationship_Guided_Semantic_Map_Prediction_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [3SD: Self-Supervised Saliency Detection with No Labels](https://openaccess.thecvf.com/content/WACV2024/html/Yasarla_3SD_Self-Supervised_Saliency_Detection_With_No_Labels_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/rajeevyasarla/3SD?style=flat)](https://github.com/rajeevyasarla/3SD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Yasarla_3SD_Self-Supervised_Saliency_Detection_With_No_Labels_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2203.04478-b31b1b.svg)](http://arxiv.org/abs/2203.04478) | :heavy_minus_sign: |
| [Training-Free Object Counting with Prompts](https://openaccess.thecvf.com/content/WACV2024/html/Shi_Training-Free_Object_Counting_With_Prompts_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/shizenglin/training-free-object-counter?style=flat)](https://github.com/shizenglin/training-free-object-counter) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Shi_Training-Free_Object_Counting_With_Prompts_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.00038-b31b1b.svg)](http://arxiv.org/abs/2307.00038) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KhN7-pf-6mE) |
| [Unsupervised and Semi-Supervised Co-Salient Object Detection via Segmentation Frequency Statistics](https://openaccess.thecvf.com/content/WACV2024/html/Chakraborty_Unsupervised_and_Semi-Supervised_Co-Salient_Object_Detection_via_Segmentation_Frequency_Statistics_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/sourachakra/uscosod-sscosod?style=flat)](https://github.com/sourachakra/uscosod-sscosod) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Chakraborty_Unsupervised_and_Semi-Supervised_Co-Salient_Object_Detection_via_Segmentation_Frequency_Statistics_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.06654-b31b1b.svg)](http://arxiv.org/abs/2311.06654) | :heavy_minus_sign: |
| [Glance to Count: Learning to Rank with Anchors for Weakly-Supervised Crowd Counting](https://openaccess.thecvf.com/content/WACV2024/html/Xiong_Glance_To_Count_Learning_To_Rank_With_Anchors_for_Weakly-Supervised_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/pandaszzzzz/CCRanking?style=flat)](https://github.com/pandaszzzzz/CCRanking) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Xiong_Glance_To_Count_Learning_To_Rank_With_Anchors_for_Weakly-Supervised_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2205.14659-b31b1b.svg)](http://arxiv.org/abs/2205.14659) | :heavy_minus_sign: |
| [TransRadar: Adaptive-Directional Transformer for Real-Time Multi-View Radar Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Dalbah_TransRadar_Adaptive-Directional_Transformer_for_Real-Time_Multi-View_Radar_Semantic_Segmentation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/YahiDar/TransRadar?style=flat)](https://github.com/YahiDar/TransRadar) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Dalbah_TransRadar_Adaptive-Directional_Transformer_for_Real-Time_Multi-View_Radar_Semantic_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.02260-b31b1b.svg)](http://arxiv.org/abs/2310.02260) | :heavy_minus_sign: |
| [Booster-SHOT: Boosting Stacked Homography Transformations for Multiview Pedestrian Detection with Attention](https://openaccess.thecvf.com/content/WACV2024/html/Hwang_Booster-SHOT_Boosting_Stacked_Homography_Transformations_for_Multiview_Pedestrian_Detection_With_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/luorix1/Booster-SHOT?style=flat)](https://github.com/luorix1/Booster-SHOT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Hwang_Booster-SHOT_Boosting_Stacked_Homography_Transformations_for_Multiview_Pedestrian_Detection_With_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.09211-b31b1b.svg)](http://arxiv.org/abs/2208.09211) | :heavy_minus_sign: |
| [360BEV: Panoramic Semantic Mapping for Indoor Bird's-Eye View](https://openaccess.thecvf.com/content/WACV2024/html/Teng_360BEV_Panoramic_Semantic_Mapping_for_Indoor_Birds-Eye_View_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jamycheung.github.io/360BEV.html) <br /> [![GitHub](https://img.shields.io/github/stars/jamycheung/360BEV?style=flat)](https://github.com/jamycheung/360BEV) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Teng_360BEV_Panoramic_Semantic_Mapping_for_Indoor_Birds-Eye_View_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11910-b31b1b.svg)](http://arxiv.org/abs/2303.11910) | :heavy_minus_sign: |
| [Learning Saliency from Fixations](https://openaccess.thecvf.com/content/WACV2024/html/Djilali_Learning_Saliency_From_Fixations_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/YasserdahouML/SalTR?style=flat)](https://github.com/YasserdahouML/SalTR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Djilali_Learning_Saliency_From_Fixations_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.14073-b31b1b.svg)](http://arxiv.org/abs/2311.14073) | :heavy_minus_sign: |
| [Mitigate Domain Shift by Primary-Auxiliary Objectives Association for Generalizing Person ReID](https://openaccess.thecvf.com/content/WACV2024/html/Li_Mitigate_Domain_Shift_by_Primary-Auxiliary_Objectives_Association_for_Generalizing_Person_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Li_Mitigate_Domain_Shift_by_Primary-Auxiliary_Objectives_Association_for_Generalizing_Person_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.15913-b31b1b.svg)](http://arxiv.org/abs/2310.15913) | :heavy_minus_sign: |
| [MIST: Medical Image Segmentation Transformer with Convolutional Attention Mixing (CAM) Decoder](https://openaccess.thecvf.com/content/WACV2024/html/Rahman_MIST_Medical_Image_Segmentation_Transformer_With_Convolutional_Attention_Mixing_CAM_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/Rahman-Motiur/MIST?style=flat)](https://github.com/Rahman-Motiur/MIST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Rahman_MIST_Medical_Image_Segmentation_Transformer_With_Convolutional_Attention_Mixing_CAM_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.19898-b31b1b.svg)](http://arxiv.org/abs/2310.19898) | :heavy_minus_sign: |
| [Small Objects Matters in Weakly-Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Mun_Small_Objects_Matters_in_Weakly-Supervised_Semantic_Segmentation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Mun_Small_Objects_Matters_in_Weakly-Supervised_Semantic_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.14117-b31b1b.svg)](http://arxiv.org/abs/2309.14117) | :heavy_minus_sign: |
| [Gradient-Guided Knowledge Distillation for Object Detectors](https://openaccess.thecvf.com/content/WACV2024/html/Lan_Gradient-Guided_Knowledge_Distillation_for_Object_Detectors_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/lanqz7766/GKD?style=flat)](https://github.com/lanqz7766/GKD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Lan_Gradient-Guided_Knowledge_Distillation_for_Object_Detectors_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.04240-b31b1b.svg)](http://arxiv.org/abs/2303.04240) | :heavy_minus_sign: |
| [MetaSeg: MetaFormer-based Global Contexts-Aware Network for Efficient Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Kang_MetaSeg_MetaFormer-Based_Global_Contexts-Aware_Network_for_Efficient_Semantic_Segmentation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Kang_MetaSeg_MetaFormer-Based_Global_Contexts-Aware_Network_for_Efficient_Semantic_Segmentation_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [ClipSitu: Effectively Leveraging CLIP for Conditional Predictions in Situation Recognition](https://openaccess.thecvf.com/content/WACV2024/html/Roy_ClipSitu_Effectively_Leveraging_CLIP_for_Conditional_Predictions_in_Situation_Recognition_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Roy_ClipSitu_Effectively_Leveraging_CLIP_for_Conditional_Predictions_in_Situation_Recognition_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.00586-b31b1b.svg)](http://arxiv.org/abs/2307.00586) | :heavy_minus_sign: |
| [Panelformer: Sewing Pattern Reconstruction from 2D Garment Images](https://openaccess.thecvf.com/content/WACV2024/html/Chen_Panelformer_Sewing_Pattern_Reconstruction_From_2D_Garment_Images_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ericsujw.github.io/Panelformer/) <br /> [![GitHub](https://img.shields.io/github/stars/ericsujw/Panelformer?style=flat)](https://github.com/ericsujw/Panelformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Panelformer_Sewing_Pattern_Reconstruction_From_2D_Garment_Images_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [From Denoising Training to Test-Time Adaptation: Enhancing Domain Generalization for Medical Image Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Wen_From_Denoising_Training_To_Test-Time_Adaptation_Enhancing_Domain_Generalization_for_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/WenRuxue/DeTTA?style=flat)](https://github.com/WenRuxue/DeTTA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wen_From_Denoising_Training_To_Test-Time_Adaptation_Enhancing_Domain_Generalization_for_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.20271-b31b1b.svg)](http://arxiv.org/abs/2310.20271) | :heavy_minus_sign: |
| [Guided Distillation for Semi-Supervised Instance Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Berrada_Guided_Distillation_for_Semi-Supervised_Instance_Segmentation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/facebookresearch/GuidedDistillation?style=flat)](https://github.com/facebookresearch/GuidedDistillation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Berrada_Guided_Distillation_for_Semi-Supervised_Instance_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.02668-b31b1b.svg)](http://arxiv.org/abs/2308.02668) | :heavy_minus_sign: |
| [Real-Time User-Guided Adaptive Colorization with Vision Transformer](https://openaccess.thecvf.com/content/WACV2024/html/Lee_Real-Time_User-Guided_Adaptive_Colorization_With_Vision_Transformer_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Lee_Real-Time_User-Guided_Adaptive_Colorization_With_Vision_Transformer_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Mining and Unifying Heterogeneous Contrastive Relations for Weakly-Supervised Actor-Action Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Duan_Mining_and_Unifying_Heterogeneous_Contrastive_Relations_for_Weakly-Supervised_Actor-Action_Segmentation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Duan_Mining_and_Unifying_Heterogeneous_Contrastive_Relations_for_Weakly-Supervised_Actor-Action_Segmentation_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Graph Neural Networks for End-to-End Information Extraction from Handwritten Documents](https://openaccess.thecvf.com/content/WACV2024/html/Khanfir_Graph_Neural_Networks_for_End-to-End_Information_Extraction_From_Handwritten_Documents_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Khanfir_Graph_Neural_Networks_for_End-to-End_Information_Extraction_From_Handwritten_Documents_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [CPSeg: Finer-Grained Image Semantic Segmentation via Chain-of-Thought Language Prompting](https://openaccess.thecvf.com/content/WACV2024/html/Li_CPSeg_Finer-Grained_Image_Semantic_Segmentation_via_Chain-of-Thought_Language_Prompting_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Li_CPSeg_Finer-Grained_Image_Semantic_Segmentation_via_Chain-of-Thought_Language_Prompting_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.16069-b31b1b.svg)](http://arxiv.org/abs/2310.16069) | :heavy_minus_sign: |
| [Foundation Model Assisted Weakly Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Yang_Foundation_Model_Assisted_Weakly_Supervised_Semantic_Segmentation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/HAL-42/FMA-WSSS?style=flat)](https://github.com/HAL-42/FMA-WSSS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Yang_Foundation_Model_Assisted_Weakly_Supervised_Semantic_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.03585-b31b1b.svg)](http://arxiv.org/abs/2312.03585) | :heavy_minus_sign: |
| [On the Importance of Large Objects in CNN based Object Detection Algorithms](https://openaccess.thecvf.com/content/WACV2024/html/Saad_On_the_Importance_of_Large_Objects_in_CNN_Based_Object_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Saad_On_the_Importance_of_Large_Objects_in_CNN_Based_Object_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.11714-b31b1b.svg)](http://arxiv.org/abs/2311.11714) | :heavy_minus_sign: |
| [Deep Metric Learning with Chance Constraints](https://openaccess.thecvf.com/content/WACV2024/html/Gurbuz_Deep_Metric_Learning_With_Chance_Constraints_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/yetigurbuz/ccp-dml?style=flat)](https://github.com/yetigurbuz/ccp-dml) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Gurbuz_Deep_Metric_Learning_With_Chance_Constraints_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.09060-b31b1b.svg)](http://arxiv.org/abs/2209.09060) | :heavy_minus_sign: |
| [TransFed: A Way to Epitomize Focal Modulation using Transformer-based Federated Learning](https://openaccess.thecvf.com/content/WACV2024/html/Ashraf_TransFed_A_Way_To_Epitomize_Focal_Modulation_Using_Transformer-Based_Federated_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ashraf_TransFed_A_Way_To_Epitomize_Focal_Modulation_Using_Transformer-Based_Federated_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Learning Better Keypoints for Multi-Object 6DoF Pose Estimation](https://openaccess.thecvf.com/content/WACV2024/html/Wu_Learning_Better_Keypoints_for_Multi-Object_6DoF_Pose_Estimation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/aaronWool/keygnet?style=flat)](https://github.com/aaronWool/keygnet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wu_Learning_Better_Keypoints_for_Multi-Object_6DoF_Pose_Estimation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07827-b31b1b.svg)](http://arxiv.org/abs/2308.07827) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=05FPOrqu_94) |
| [Object Aware Contrastive Prior for Interactive Image Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Mathur_Object_Aware_Contrastive_Prior_for_Interactive_Image_Segmentation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Mathur_Object_Aware_Contrastive_Prior_for_Interactive_Image_Segmentation_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Beyond Classification: Definition and Density-based Estimation of Calibration in Object Detection](https://openaccess.thecvf.com/content/WACV2024/html/Popordanoska_Beyond_Classification_Definition_and_Density-Based_Estimation_of_Calibration_in_Object_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/tpopordanoska/calibration-object-detection?style=flat)](https://github.com/tpopordanoska/calibration-object-detection) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Popordanoska_Beyond_Classification_Definition_and_Density-Based_Estimation_of_Calibration_in_Object_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.06645-b31b1b.svg)](http://arxiv.org/abs/2312.06645) | :heavy_minus_sign: |
| [FAKD: Feature Augmented Knowledge Distillation for Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Yuan_FAKD_Feature_Augmented_Knowledge_Distillation_for_Semantic_Segmentation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/jianlong-yuan/FAKD?style=flat)](https://github.com/jianlong-yuan/FAKD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Yuan_FAKD_Feature_Augmented_Knowledge_Distillation_for_Semantic_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.14143-b31b1b.svg)](http://arxiv.org/abs/2208.14143) | :heavy_minus_sign: |
| [Efficient MAE Towards Large-Scale Vision Transformers](https://openaccess.thecvf.com/content/WACV2024/html/Han_Efficient_MAE_Towards_Large-Scale_Vision_Transformers_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Han_Efficient_MAE_Towards_Large-Scale_Vision_Transformers_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [MS-EVS: Multispectral Event-based Vision for Deep Learning based Face Detection](https://openaccess.thecvf.com/content/WACV2024/html/Himmi_MS-EVS_Multispectral_Event-Based_Vision_for_Deep_Learning_Based_Face_Detection_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://saadhimmi.github.io/ms-evs.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/saadhimmi/ms-evs.github.io?style=flat)](https://github.com/saadhimmi/ms-evs.github.io) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Himmi_MS-EVS_Multispectral_Event-Based_Vision_for_Deep_Learning_Based_Face_Detection_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Interactive Network Perturbation Between Teacher and Students for Semi-Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Cho_Interactive_Network_Perturbation_Between_Teacher_and_Students_for_Semi-Supervised_Semantic_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Cho_Interactive_Network_Perturbation_Between_Teacher_and_Students_for_Semi-Supervised_Semantic_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times and Location Reasoning](https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Can_Vision-Language_Models_Be_a_Good_Guesser_Exploring_VLMs_for_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/gengyuanmax/WikiTiLo?style=flat)](https://github.com/gengyuanmax/WikiTiLo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Can_Vision-Language_Models_Be_a_Good_Guesser_Exploring_VLMs_for_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.06166-b31b1b.svg)](http://arxiv.org/abs/2307.06166) | :heavy_minus_sign: |
| [USDN: A Unified Sample-Wise Dynamic Network with Mixed-Precision and Early-Exit](https://openaccess.thecvf.com/content/WACV2024/html/Jeon_USDN_A_Unified_Sample-Wise_Dynamic_Network_With_Mixed-Precision_and_Early-Exit_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Jeon_USDN_A_Unified_Sample-Wise_Dynamic_Network_With_Mixed-Precision_and_Early-Exit_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Beyond Fusion: Modality Hallucination-based Multispectral Fusion for Pedestrian Detection](https://openaccess.thecvf.com/content/WACV2024/html/Xie_Beyond_Fusion_Modality_Hallucination-Based_Multispectral_Fusion_for_Pedestrian_Detection_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Xie_Beyond_Fusion_Modality_Hallucination-Based_Multispectral_Fusion_for_Pedestrian_Detection_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [DocReal: Robust Document Dewarping of Real-Life Images via Attention-Enhanced Control Point Prediction](https://openaccess.thecvf.com/content/WACV2024/html/Yu_DocReal_Robust_Document_Dewarping_of_Real-Life_Images_via_Attention-Enhanced_Control_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/irisXcoding/DocReal?style=flat)](https://github.com/irisXcoding/DocReal) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Yu_DocReal_Robust_Document_Dewarping_of_Real-Life_Images_via_Attention-Enhanced_Control_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Learning to Recognize Occluded and Small Objects with Partial Inputs](https://openaccess.thecvf.com/content/WACV2024/html/Zunair_Learning_To_Recognize_Occluded_and_Small_Objects_With_Partial_Inputs_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://hasibzunair.github.io/msl-recognition/) <br /> [![GitHub](https://img.shields.io/github/stars/hasibzunair/msl-recognition?style=flat)](https://github.com/hasibzunair/msl-recognition) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-demo-FFD21F.svg)](https://huggingface.co/spaces/hasibzunair/msl-recognition-demo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zunair_Learning_To_Recognize_Occluded_and_Small_Objects_With_Partial_Inputs_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.18517-b31b1b.svg)](http://arxiv.org/abs/2310.18517) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=koxIipyvmZk) |
| [Temporally-Consistent Video Semantic Segmentation with Bidirectional Occlusion-Guided Feature Propagation](https://openaccess.thecvf.com/content/WACV2024/html/Baghbaderani_Temporally-Consistent_Video_Semantic_Segmentation_With_Bidirectional_Occlusion-Guided_Feature_Propagation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Baghbaderani_Temporally-Consistent_Video_Semantic_Segmentation_With_Bidirectional_Occlusion-Guided_Feature_Propagation_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Towards Domain-Aware Knowledge Distillation for Continual Model Generalization](https://openaccess.thecvf.com/content/WACV2024/html/Reddy_Towards_Domain-Aware_Knowledge_Distillation_for_Continual_Model_Generalization_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dose-iitd.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Reddy_Towards_Domain-Aware_Knowledge_Distillation_for_Continual_Model_Generalization_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Let's Observe them Over Time: An Improved Pedestrian Attribute Recognition Approach](https://openaccess.thecvf.com/content/WACV2024/html/Thakare_Lets_Observe_Them_Over_Time_An_Improved_Pedestrian_Attribute_Recognition_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Thakare_Lets_Observe_Them_Over_Time_An_Improved_Pedestrian_Attribute_Recognition_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Harnessing the Power of Multi-Lingual Datasets for Pre-Training: Towards Enhancing Text Spotting Performance](https://openaccess.thecvf.com/content/WACV2024/html/Das_Harnessing_the_Power_of_Multi-Lingual_Datasets_for_Pre-Training_Towards_Enhancing_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/alloydas/TESTR_Eval?style=flat)](https://github.com/alloydas/TESTR_Eval) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Das_Harnessing_the_Power_of_Multi-Lingual_Datasets_for_Pre-Training_Towards_Enhancing_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.00917-b31b1b.svg)](http://arxiv.org/abs/2310.00917) | :heavy_minus_sign: |
| [Patch-based Selection and Refinement for Early Object Detection](https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Patch-Based_Selection_and_Refinement_for_Early_Object_Detection_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/destiny301/dpr?style=flat)](https://github.com/destiny301/dpr) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Patch-Based_Selection_and_Refinement_for_Early_Object_Detection_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.02274-b31b1b.svg)](http://arxiv.org/abs/2311.02274) | :heavy_minus_sign: |
| [Boosting Weakly Supervised Object Detection using Fusion and Priors from Hallucinated Depth](https://openaccess.thecvf.com/content/WACV2024/html/Gungor_Boosting_Weakly_Supervised_Object_Detection_Using_Fusion_and_Priors_From_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cagrigungor.github.io/WSOD-AMPLIFIER/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Gungor_Boosting_Weakly_Supervised_Object_Detection_Using_Fusion_and_Priors_From_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10937-b31b1b.svg)](http://arxiv.org/abs/2303.10937) | :heavy_minus_sign: |
| [C2AIR: Consolidated Compact Aerial Image Haze Removal](https://openaccess.thecvf.com/content/WACV2024/html/Kulkarni_C2AIR_Consolidated_Compact_Aerial_Image_Haze_Removal_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/AshutoshKulkarni4998/C2AIR?style=flat)](https://github.com/AshutoshKulkarni4998/C2AIR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Kulkarni_C2AIR_Consolidated_Compact_Aerial_Image_Haze_Removal_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Favoring One Among Equals - Not a Good Idea: Many-to-One Matching for Robust Transformer based Pedestrian Detection](https://openaccess.thecvf.com/content/WACV2024/html/Shastry_Favoring_One_Among_Equals_-_Not_a_Good_Idea_Many-to-One_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ajayshastry08.github.io/flow_matcher) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Shastry_Favoring_One_Among_Equals_-_Not_a_Good_Idea_Many-to-One_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Improving Vision-and-Language Reasoning via Spatial Relations Modeling](https://openaccess.thecvf.com/content/WACV2024/html/Yang_Improving_Vision-and-Language_Reasoning_via_Spatial_Relations_Modeling_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Yang_Improving_Vision-and-Language_Reasoning_via_Spatial_Relations_Modeling_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.05298-b31b1b.svg)](http://arxiv.org/abs/2311.05298) | :heavy_minus_sign: |
| [LP-OVOD: Open-Vocabulary Object Detection by Linear Probing](https://openaccess.thecvf.com/content/WACV2024/html/Pham_LP-OVOD_Open-Vocabulary_Object_Detection_by_Linear_Probing_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Pham_LP-OVOD_Open-Vocabulary_Object_Detection_by_Linear_Probing_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.17109-b31b1b.svg)](http://arxiv.org/abs/2310.17109) | :heavy_minus_sign: |
| [Continuous Adaptation for Interactive Segmentation using Teacher-Student Architecture](https://openaccess.thecvf.com/content/WACV2024/html/Atanyan_Continuous_Adaptation_for_Interactive_Segmentation_Using_Teacher-Student_Architecture_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Atanyan_Continuous_Adaptation_for_Interactive_Segmentation_Using_Teacher-Student_Architecture_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Interpretable Object Recognition by Semantic Prototype Analysis](https://openaccess.thecvf.com/content/WACV2024/html/Wan_Interpretable_Object_Recognition_by_Semantic_Prototype_Analysis_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/WanQiyang/SPANet?style=flat)](https://github.com/WanQiyang/SPANet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wan_Interpretable_Object_Recognition_by_Semantic_Prototype_Analysis_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [RecycleNet: Latent Feature Recycling Leads to Iterative Decision Refinement](https://openaccess.thecvf.com/content/WACV2024/html/Kohler_RecycleNet_Latent_Feature_Recycling_Leads_to_Iterative_Decision_Refinement_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Kohler_RecycleNet_Latent_Feature_Recycling_Leads_to_Iterative_Decision_Refinement_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.07513-b31b1b.svg)](http://arxiv.org/abs/2309.07513) | :heavy_minus_sign: |
| [Learning to Detour: Shortcut Mitigating Augmentation for Weakly Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Kwon_Learning_to_Detour_Shortcut_Mitigating_Augmentation_for_Weakly_Supervised_Semantic_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Kwon_Learning_to_Detour_Shortcut_Mitigating_Augmentation_for_Weakly_Supervised_Semantic_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Elusive Images: Beyond Coarse Analysis for Fine-Grained Recognition](https://openaccess.thecvf.com/content/WACV2024/html/Anderson_Elusive_Images_Beyond_Coarse_Analysis_for_Fine-Grained_Recognition_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://catalys1.github.io/elusive-images-fgvc/) <br /> [![GitHub](https://img.shields.io/github/stars/catalys1/elusive-images-fgvc?style=flat)](https://github.com/catalys1/elusive-images-fgvc) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Anderson_Elusive_Images_Beyond_Coarse_Analysis_for_Fine-Grained_Recognition_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Understanding Dark Scenes by Contrasting Multi-Modal Observations](https://openaccess.thecvf.com/content/WACV2024/html/Dong_Understanding_Dark_Scenes_by_Contrasting_Multi-Modal_Observations_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/palmdong/SMMCL?style=flat)](https://github.com/palmdong/SMMCL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Dong_Understanding_Dark_Scenes_by_Contrasting_Multi-Modal_Observations_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.12320-b31b1b.svg)](http://arxiv.org/abs/2308.12320) | :heavy_minus_sign: |
| [MaskConver: Revisiting Pure Convolution Model for Panoptic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Rashwan_MaskConver_Revisiting_Pure_Convolution_Model_for_Panoptic_Segmentation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Rashwan_MaskConver_Revisiting_Pure_Convolution_Model_for_Panoptic_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.06052-b31b1b.svg)](http://arxiv.org/abs/2312.06052) | :heavy_minus_sign: |
| Masked Collaborative Contrast for Weakly Supervised Semantic Segmentation |  |  |  |
| Universal Semi-Supervised Model Adaptation via Collaborative Consistency Training |  |  |  |
| STEP - Towards Structured Scene-Text Spotting |  |  |  |
| Efficient Feature Distillation for Zero-Shot Annotation Object Detection |  |  |  |
| Hierarchical Text Spotter for Joint Text Spotting and Layout Analysis |  |  |  |
| iBARLE: imBalance-Aware Room Layout Estimation |  |  |  |
| TSP-Transformer: Task-Specific Prompts Boosted Transformer for Holistic Scene Understanding |  |  |  |
| Implicit Neural Representation for Change Detection |  |  |  |
| Label-Free Synthetic Pretraining of Object Detectors |  |  |  |
| Improved Techniques for Quantizing Deep Networks with Adaptive Bit-Widths |  |  |  |
| What's Outside the Intersection? Fine-Grained Error Analysis for Semantic Segmentation Beyond IoU |  |  |  |
| Pixel Matching Network for Cross-Domain Few-Shot Segmentation |  |  |  |
| EResFD: Rediscovery of the Effectiveness of Standard Convolution for Lightweight Face Detection |  |  |  |
| Framework-Agnostic Semantically-Aware Global Reasoning for Segmentation |  |  |  |
| High-Fidelity Pseudo-Labels for Boosting Weakly-Supervised Segmentation |  |  |  |
| Missing Modality Robustness in Semi-Supervised Multi-Modal Semantic Segmentation |  |  |  |
| Unsupervised Graphic Layout Grouping with Transformers |  |  |  |
| Contrastive Viewpoint-Aware Shape Learning for Long-Term Person Re-Identification |  |  |  |
| PolyMaX: General Dense Prediction with Mask Transformer |  |  |  |
| BPKD: Boundary Privileged Knowledge Distillation for Semantic Segmentation |  |  |  |
| Label Shift Estimation for Class-Imbalance Problem: A Bayesian Approach |  |  |  |
| Query-Guided Attention in Vision Transformers for Localizing Objects using a Single Sketch |  |  |  |
| PromptAD: Zero-Shot Anomaly Detection using Text Prompts |  |  |  |
| Learning Quality Labels for Robust Image Classification |  |  |  |
| Learning Transferable Representations for Image Anomaly Localization using Dense Pretraining |  |  |  |
| SBCFormer: Lightweight Network Capable of Full-Size ImageNet Classification at 1 FPS on Single Board Computers |  |  |  |
| High-Fidelity Zero-Shot Texture Anomaly Localization using Feature Correspondence Analysis |  |  |  |
| Grafting Vision Transformers |  |  |  |
| Rethinking Knowledge Distillation with Raw Features for Semantic Segmentation |  |  |  |
| Efficient Expansion and Gradient based Task Inference for Replay Free Incremental Learning |  |  |  |
| CLRerNet: Improving Confidence of Lane Detection with LaneIoU |  |  |  |
| Multi-Modal Gaze Following in Conversational Scenarios |  |  |  |
| Enhancing Multi-View Pedestrian Detection through Generalized 3D Feature Pulling |  |  |  |
| Self-Supervised Learning for Visual Relationship Detection through Masked Bounding Box Reconstruction |  |  |  |
| The Background also Matters: Background-Aware Motion-Guided Objects Discovery |  |  |  |
| Semi-Supervised Scene Change Detection by Distillation from Feature-Metric Alignment |  |  |  |
| OmniVec: Learning Robust Representations with Cross Modal Sharing |  |  |  |
| Cross-Attention between Satellite and Ground Views for Enhanced Fine-Grained Robot Geo-Localization |  |  |  |
| Data Augmentation for Object Detection via Controllable Diffusion Models |  |  |  |
| Physical-Space Multi-Body Mesh Detection Achieved by Local Alignment and Global Dense Learning |  |  |  |
| Multi-Source Domain Adaptation for Object Detection with Prototype-based Mean Teacher |  |  |  |
| Beyond Self-Attention: Deformable Large Kernel Attention for Medical Image Segmentation |  |  |  |
| INCODE: Implicit Neural Conditioning with Prior Knowledge Embeddings |  |  |  |
| ProcSim: Proxy-based Confidence for Robust Similarity Learning |  |  |  |
| Refine and Redistribute: Multi-Domain Fusion and Dynamic Label Assignment for Unbiased Scene Graph Generation |  |  |  |
| Joint Depth Prediction and Semantic Segmentation with Multi-View SAM |  |  |  |
| Self-Supervised Relation Alignment for Scene Graph Generation |  |  |  |
| Semantic Transfer from Head to Tail: Enlarging Tail Margin for Long-Tailed Visual Recognition |  |  |  |
| PatchRefineNet: Improving Binary Segmentation by Incorporating Signals from Optimal Patch-Wise Binarization |  |  |  |
| Adaptive Deep Neural Network Inference Optimization with EENet |  |  |  |
| Token Fusion: Bridging the Gap between Token Pruning and Token Merging |  |  |  |
| Pruning from Scratch via Shared Pruning Module and Nuclear Norm-based Regularization |  |  |  |
| CLIP-DIY: CLIP Dense Inference Yields Open-Vocabulary Semantic Segmentation For-Free |  |  |  |
| Layer-Wise Auto-Weighting for Non-Stationary Test-Time Adaptation |  |  |  |
| Uncertainty Estimation in Instance Segmentation with Star-Convex Shapes |  |  |  |
| CamoFocus: Enhancing Camouflage Object Detection with Split-Feature Focal Modulation and Context Refinement |  |  |  |
| HalluciDet: Hallucinating RGB Modality for Person Detection through Privileged Information |  |  |  |
| Spectroformer: Multi-Domain Query Cascaded Transformer Network for Underwater Image Enhancement |  |  |  |
| FOSSIL: Free Open-Vocabulary Semantic Segmentation through Synthetic References Retrieval |  |  |  |
