# WACV-2024-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/oral_p_a_m_e_r.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/low-level-and-physics-based-vision.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Image Recognition and Understanding

![Section Papers](https://img.shields.io/badge/Section%20Papers-119-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-74-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-63-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-6-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| [Semi-Supervised Semantic Depth Estimation using Symbiotic Transformer and NearFarMix Augmentation](https://openaccess.thecvf.com/content/WACV2024/html/Rahman_Semi-Supervised_Semantic_Depth_Estimation_Using_Symbiotic_Transformer_and_NearFarMix_Augmentation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Rahman_Semi-Supervised_Semantic_Depth_Estimation_Using_Symbiotic_Transformer_and_NearFarMix_Augmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.14400-b31b1b.svg)](http://arxiv.org/abs/2308.14400) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-5YidVH9wIA) |
| [Training Ensembles with Inliers and Outliers for Semi-Supervised Active Learning](https://openaccess.thecvf.com/content/WACV2024/html/Stojnic_Training_Ensembles_With_Inliers_and_Outliers_for_Semi-Supervised_Active_Learning_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/vladan-stojnic/active-outliers?style=flat)](https://github.com/vladan-stojnic/active-outliers) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Stojnic_Training_Ensembles_With_Inliers_and_Outliers_for_Semi-Supervised_Active_Learning_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.03741-b31b1b.svg)](http://arxiv.org/abs/2307.03741) | :heavy_minus_sign: |
| [Multi-View Classification using Hybrid Fusion and Mutual Distillation](https://openaccess.thecvf.com/content/WACV2024/html/Black_Multi-View_Classification_Using_Hybrid_Fusion_and_Mutual_Distillation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/vidarlab/multi-view-hybrid?style=flat)](https://github.com/vidarlab/multi-view-hybrid) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Black_Multi-View_Classification_Using_Hybrid_Fusion_and_Mutual_Distillation_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Amodal Intra-Class Instance Segmentation: Synthetic Datasets and Benchmark](https://openaccess.thecvf.com/content/WACV2024/html/Ao_Amodal_Intra-Class_Instance_Segmentation_Synthetic_Datasets_and_Benchmark_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/saraao/amodal-dataset?style=flat)](https://github.com/saraao/amodal-dataset) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ao_Amodal_Intra-Class_Instance_Segmentation_Synthetic_Datasets_and_Benchmark_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06596-b31b1b.svg)](http://arxiv.org/abs/2303.06596) | :heavy_minus_sign: |
| [Prompting Classes: Exploring the Power of Prompt Class Learning in Weakly Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Murugesan_Prompting_Classes_Exploring_the_Power_of_Prompt_Class_Learning_in_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/Ruxie189/WSS_POLE?style=flat)](https://github.com/Ruxie189/WSS_POLE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Murugesan_Prompting_Classes_Exploring_the_Power_of_Prompt_Class_Learning_in_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.00097-b31b1b.svg)](http://arxiv.org/abs/2307.00097) | :heavy_minus_sign: |
| [RSMPNet: Relationship Guided Semantic Map Prediction](https://openaccess.thecvf.com/content/WACV2024/html/Sun_RSMPNet_Relationship_Guided_Semantic_Map_Prediction_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/jws39/semantic-map-prediction?style=flat)](https://github.com/jws39/semantic-map-prediction) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Sun_RSMPNet_Relationship_Guided_Semantic_Map_Prediction_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [3SD: Self-Supervised Saliency Detection with No Labels](https://openaccess.thecvf.com/content/WACV2024/html/Yasarla_3SD_Self-Supervised_Saliency_Detection_With_No_Labels_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/rajeevyasarla/3SD?style=flat)](https://github.com/rajeevyasarla/3SD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Yasarla_3SD_Self-Supervised_Saliency_Detection_With_No_Labels_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2203.04478-b31b1b.svg)](http://arxiv.org/abs/2203.04478) | :heavy_minus_sign: |
| [Training-Free Object Counting with Prompts](https://openaccess.thecvf.com/content/WACV2024/html/Shi_Training-Free_Object_Counting_With_Prompts_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/shizenglin/training-free-object-counter?style=flat)](https://github.com/shizenglin/training-free-object-counter) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Shi_Training-Free_Object_Counting_With_Prompts_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.00038-b31b1b.svg)](http://arxiv.org/abs/2307.00038) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KhN7-pf-6mE) |
| [Unsupervised and Semi-Supervised Co-Salient Object Detection via Segmentation Frequency Statistics](https://openaccess.thecvf.com/content/WACV2024/html/Chakraborty_Unsupervised_and_Semi-Supervised_Co-Salient_Object_Detection_via_Segmentation_Frequency_Statistics_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/sourachakra/uscosod-sscosod?style=flat)](https://github.com/sourachakra/uscosod-sscosod) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Chakraborty_Unsupervised_and_Semi-Supervised_Co-Salient_Object_Detection_via_Segmentation_Frequency_Statistics_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.06654-b31b1b.svg)](http://arxiv.org/abs/2311.06654) | :heavy_minus_sign: |
| [Glance to Count: Learning to Rank with Anchors for Weakly-Supervised Crowd Counting](https://openaccess.thecvf.com/content/WACV2024/html/Xiong_Glance_To_Count_Learning_To_Rank_With_Anchors_for_Weakly-Supervised_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/pandaszzzzz/CCRanking?style=flat)](https://github.com/pandaszzzzz/CCRanking) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Xiong_Glance_To_Count_Learning_To_Rank_With_Anchors_for_Weakly-Supervised_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2205.14659-b31b1b.svg)](http://arxiv.org/abs/2205.14659) | :heavy_minus_sign: |
| [TransRadar: Adaptive-Directional Transformer for Real-Time Multi-View Radar Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Dalbah_TransRadar_Adaptive-Directional_Transformer_for_Real-Time_Multi-View_Radar_Semantic_Segmentation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/YahiDar/TransRadar?style=flat)](https://github.com/YahiDar/TransRadar) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Dalbah_TransRadar_Adaptive-Directional_Transformer_for_Real-Time_Multi-View_Radar_Semantic_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.02260-b31b1b.svg)](http://arxiv.org/abs/2310.02260) | :heavy_minus_sign: |
| [Booster-SHOT: Boosting Stacked Homography Transformations for Multiview Pedestrian Detection with Attention](https://openaccess.thecvf.com/content/WACV2024/html/Hwang_Booster-SHOT_Boosting_Stacked_Homography_Transformations_for_Multiview_Pedestrian_Detection_With_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/luorix1/Booster-SHOT?style=flat)](https://github.com/luorix1/Booster-SHOT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Hwang_Booster-SHOT_Boosting_Stacked_Homography_Transformations_for_Multiview_Pedestrian_Detection_With_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.09211-b31b1b.svg)](http://arxiv.org/abs/2208.09211) | :heavy_minus_sign: |
| [360BEV: Panoramic Semantic Mapping for Indoor Bird's-Eye View](https://openaccess.thecvf.com/content/WACV2024/html/Teng_360BEV_Panoramic_Semantic_Mapping_for_Indoor_Birds-Eye_View_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jamycheung.github.io/360BEV.html) <br /> [![GitHub](https://img.shields.io/github/stars/jamycheung/360BEV?style=flat)](https://github.com/jamycheung/360BEV) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Teng_360BEV_Panoramic_Semantic_Mapping_for_Indoor_Birds-Eye_View_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11910-b31b1b.svg)](http://arxiv.org/abs/2303.11910) | :heavy_minus_sign: |
| [Learning Saliency from Fixations](https://openaccess.thecvf.com/content/WACV2024/html/Djilali_Learning_Saliency_From_Fixations_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/YasserdahouML/SalTR?style=flat)](https://github.com/YasserdahouML/SalTR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Djilali_Learning_Saliency_From_Fixations_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.14073-b31b1b.svg)](http://arxiv.org/abs/2311.14073) | :heavy_minus_sign: |
| [Mitigate Domain Shift by Primary-Auxiliary Objectives Association for Generalizing Person ReID](https://openaccess.thecvf.com/content/WACV2024/html/Li_Mitigate_Domain_Shift_by_Primary-Auxiliary_Objectives_Association_for_Generalizing_Person_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Li_Mitigate_Domain_Shift_by_Primary-Auxiliary_Objectives_Association_for_Generalizing_Person_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.15913-b31b1b.svg)](http://arxiv.org/abs/2310.15913) | :heavy_minus_sign: |
| [MIST: Medical Image Segmentation Transformer with Convolutional Attention Mixing (CAM) Decoder](https://openaccess.thecvf.com/content/WACV2024/html/Rahman_MIST_Medical_Image_Segmentation_Transformer_With_Convolutional_Attention_Mixing_CAM_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/Rahman-Motiur/MIST?style=flat)](https://github.com/Rahman-Motiur/MIST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Rahman_MIST_Medical_Image_Segmentation_Transformer_With_Convolutional_Attention_Mixing_CAM_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.19898-b31b1b.svg)](http://arxiv.org/abs/2310.19898) | :heavy_minus_sign: |
| [Small Objects Matters in Weakly-Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Mun_Small_Objects_Matters_in_Weakly-Supervised_Semantic_Segmentation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Mun_Small_Objects_Matters_in_Weakly-Supervised_Semantic_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.14117-b31b1b.svg)](http://arxiv.org/abs/2309.14117) | :heavy_minus_sign: |
| [Gradient-Guided Knowledge Distillation for Object Detectors](https://openaccess.thecvf.com/content/WACV2024/html/Lan_Gradient-Guided_Knowledge_Distillation_for_Object_Detectors_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/lanqz7766/GKD?style=flat)](https://github.com/lanqz7766/GKD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Lan_Gradient-Guided_Knowledge_Distillation_for_Object_Detectors_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.04240-b31b1b.svg)](http://arxiv.org/abs/2303.04240) | :heavy_minus_sign: |
| [MetaSeg: MetaFormer-based Global Contexts-Aware Network for Efficient Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Kang_MetaSeg_MetaFormer-Based_Global_Contexts-Aware_Network_for_Efficient_Semantic_Segmentation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Kang_MetaSeg_MetaFormer-Based_Global_Contexts-Aware_Network_for_Efficient_Semantic_Segmentation_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [ClipSitu: Effectively Leveraging CLIP for Conditional Predictions in Situation Recognition](https://openaccess.thecvf.com/content/WACV2024/html/Roy_ClipSitu_Effectively_Leveraging_CLIP_for_Conditional_Predictions_in_Situation_Recognition_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Roy_ClipSitu_Effectively_Leveraging_CLIP_for_Conditional_Predictions_in_Situation_Recognition_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.00586-b31b1b.svg)](http://arxiv.org/abs/2307.00586) | :heavy_minus_sign: |
| [Panelformer: Sewing Pattern Reconstruction from 2D Garment Images](https://openaccess.thecvf.com/content/WACV2024/html/Chen_Panelformer_Sewing_Pattern_Reconstruction_From_2D_Garment_Images_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ericsujw.github.io/Panelformer/) <br /> [![GitHub](https://img.shields.io/github/stars/ericsujw/Panelformer?style=flat)](https://github.com/ericsujw/Panelformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Panelformer_Sewing_Pattern_Reconstruction_From_2D_Garment_Images_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [From Denoising Training to Test-Time Adaptation: Enhancing Domain Generalization for Medical Image Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Wen_From_Denoising_Training_To_Test-Time_Adaptation_Enhancing_Domain_Generalization_for_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/WenRuxue/DeTTA?style=flat)](https://github.com/WenRuxue/DeTTA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wen_From_Denoising_Training_To_Test-Time_Adaptation_Enhancing_Domain_Generalization_for_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.20271-b31b1b.svg)](http://arxiv.org/abs/2310.20271) | :heavy_minus_sign: |
| [Guided Distillation for Semi-Supervised Instance Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Berrada_Guided_Distillation_for_Semi-Supervised_Instance_Segmentation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/facebookresearch/GuidedDistillation?style=flat)](https://github.com/facebookresearch/GuidedDistillation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Berrada_Guided_Distillation_for_Semi-Supervised_Instance_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.02668-b31b1b.svg)](http://arxiv.org/abs/2308.02668) | :heavy_minus_sign: |
| [Real-Time User-Guided Adaptive Colorization with Vision Transformer](https://openaccess.thecvf.com/content/WACV2024/html/Lee_Real-Time_User-Guided_Adaptive_Colorization_With_Vision_Transformer_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Lee_Real-Time_User-Guided_Adaptive_Colorization_With_Vision_Transformer_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Mining and Unifying Heterogeneous Contrastive Relations for Weakly-Supervised Actor-Action Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Duan_Mining_and_Unifying_Heterogeneous_Contrastive_Relations_for_Weakly-Supervised_Actor-Action_Segmentation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Duan_Mining_and_Unifying_Heterogeneous_Contrastive_Relations_for_Weakly-Supervised_Actor-Action_Segmentation_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Graph Neural Networks for End-to-End Information Extraction from Handwritten Documents](https://openaccess.thecvf.com/content/WACV2024/html/Khanfir_Graph_Neural_Networks_for_End-to-End_Information_Extraction_From_Handwritten_Documents_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Khanfir_Graph_Neural_Networks_for_End-to-End_Information_Extraction_From_Handwritten_Documents_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [CPSeg: Finer-Grained Image Semantic Segmentation via Chain-of-Thought Language Prompting](https://openaccess.thecvf.com/content/WACV2024/html/Li_CPSeg_Finer-Grained_Image_Semantic_Segmentation_via_Chain-of-Thought_Language_Prompting_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Li_CPSeg_Finer-Grained_Image_Semantic_Segmentation_via_Chain-of-Thought_Language_Prompting_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.16069-b31b1b.svg)](http://arxiv.org/abs/2310.16069) | :heavy_minus_sign: |
| [Foundation Model Assisted Weakly Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Yang_Foundation_Model_Assisted_Weakly_Supervised_Semantic_Segmentation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/HAL-42/FMA-WSSS?style=flat)](https://github.com/HAL-42/FMA-WSSS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Yang_Foundation_Model_Assisted_Weakly_Supervised_Semantic_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.03585-b31b1b.svg)](http://arxiv.org/abs/2312.03585) | :heavy_minus_sign: |
| [On the Importance of Large Objects in CNN based Object Detection Algorithms](https://openaccess.thecvf.com/content/WACV2024/html/Saad_On_the_Importance_of_Large_Objects_in_CNN_Based_Object_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Saad_On_the_Importance_of_Large_Objects_in_CNN_Based_Object_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.11714-b31b1b.svg)](http://arxiv.org/abs/2311.11714) | :heavy_minus_sign: |
| [Deep Metric Learning with Chance Constraints](https://openaccess.thecvf.com/content/WACV2024/html/Gurbuz_Deep_Metric_Learning_With_Chance_Constraints_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/yetigurbuz/ccp-dml?style=flat)](https://github.com/yetigurbuz/ccp-dml) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Gurbuz_Deep_Metric_Learning_With_Chance_Constraints_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2209.09060-b31b1b.svg)](http://arxiv.org/abs/2209.09060) | :heavy_minus_sign: |
| [TransFed: A Way to Epitomize Focal Modulation using Transformer-based Federated Learning](https://openaccess.thecvf.com/content/WACV2024/html/Ashraf_TransFed_A_Way_To_Epitomize_Focal_Modulation_Using_Transformer-Based_Federated_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ashraf_TransFed_A_Way_To_Epitomize_Focal_Modulation_Using_Transformer-Based_Federated_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Learning Better Keypoints for Multi-Object 6DoF Pose Estimation](https://openaccess.thecvf.com/content/WACV2024/html/Wu_Learning_Better_Keypoints_for_Multi-Object_6DoF_Pose_Estimation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/aaronWool/keygnet?style=flat)](https://github.com/aaronWool/keygnet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wu_Learning_Better_Keypoints_for_Multi-Object_6DoF_Pose_Estimation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07827-b31b1b.svg)](http://arxiv.org/abs/2308.07827) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=05FPOrqu_94) |
| [Object Aware Contrastive Prior for Interactive Image Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Mathur_Object_Aware_Contrastive_Prior_for_Interactive_Image_Segmentation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Mathur_Object_Aware_Contrastive_Prior_for_Interactive_Image_Segmentation_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Beyond Classification: Definition and Density-based Estimation of Calibration in Object Detection](https://openaccess.thecvf.com/content/WACV2024/html/Popordanoska_Beyond_Classification_Definition_and_Density-Based_Estimation_of_Calibration_in_Object_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/tpopordanoska/calibration-object-detection?style=flat)](https://github.com/tpopordanoska/calibration-object-detection) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Popordanoska_Beyond_Classification_Definition_and_Density-Based_Estimation_of_Calibration_in_Object_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.06645-b31b1b.svg)](http://arxiv.org/abs/2312.06645) | :heavy_minus_sign: |
| [FAKD: Feature Augmented Knowledge Distillation for Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Yuan_FAKD_Feature_Augmented_Knowledge_Distillation_for_Semantic_Segmentation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/jianlong-yuan/FAKD?style=flat)](https://github.com/jianlong-yuan/FAKD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Yuan_FAKD_Feature_Augmented_Knowledge_Distillation_for_Semantic_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.14143-b31b1b.svg)](http://arxiv.org/abs/2208.14143) | :heavy_minus_sign: |
| [Efficient MAE Towards Large-Scale Vision Transformers](https://openaccess.thecvf.com/content/WACV2024/html/Han_Efficient_MAE_Towards_Large-Scale_Vision_Transformers_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Han_Efficient_MAE_Towards_Large-Scale_Vision_Transformers_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [MS-EVS: Multispectral Event-based Vision for Deep Learning based Face Detection](https://openaccess.thecvf.com/content/WACV2024/html/Himmi_MS-EVS_Multispectral_Event-Based_Vision_for_Deep_Learning_Based_Face_Detection_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://saadhimmi.github.io/ms-evs.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/saadhimmi/ms-evs.github.io?style=flat)](https://github.com/saadhimmi/ms-evs.github.io) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Himmi_MS-EVS_Multispectral_Event-Based_Vision_for_Deep_Learning_Based_Face_Detection_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Interactive Network Perturbation Between Teacher and Students for Semi-Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Cho_Interactive_Network_Perturbation_Between_Teacher_and_Students_for_Semi-Supervised_Semantic_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Cho_Interactive_Network_Perturbation_Between_Teacher_and_Students_for_Semi-Supervised_Semantic_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times and Location Reasoning](https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Can_Vision-Language_Models_Be_a_Good_Guesser_Exploring_VLMs_for_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/gengyuanmax/WikiTiLo?style=flat)](https://github.com/gengyuanmax/WikiTiLo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Can_Vision-Language_Models_Be_a_Good_Guesser_Exploring_VLMs_for_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.06166-b31b1b.svg)](http://arxiv.org/abs/2307.06166) | :heavy_minus_sign: |
| [USDN: A Unified Sample-Wise Dynamic Network with Mixed-Precision and Early-Exit](https://openaccess.thecvf.com/content/WACV2024/html/Jeon_USDN_A_Unified_Sample-Wise_Dynamic_Network_With_Mixed-Precision_and_Early-Exit_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Jeon_USDN_A_Unified_Sample-Wise_Dynamic_Network_With_Mixed-Precision_and_Early-Exit_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Beyond Fusion: Modality Hallucination-based Multispectral Fusion for Pedestrian Detection](https://openaccess.thecvf.com/content/WACV2024/html/Xie_Beyond_Fusion_Modality_Hallucination-Based_Multispectral_Fusion_for_Pedestrian_Detection_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Xie_Beyond_Fusion_Modality_Hallucination-Based_Multispectral_Fusion_for_Pedestrian_Detection_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [DocReal: Robust Document Dewarping of Real-Life Images via Attention-Enhanced Control Point Prediction](https://openaccess.thecvf.com/content/WACV2024/html/Yu_DocReal_Robust_Document_Dewarping_of_Real-Life_Images_via_Attention-Enhanced_Control_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/irisXcoding/DocReal?style=flat)](https://github.com/irisXcoding/DocReal) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Yu_DocReal_Robust_Document_Dewarping_of_Real-Life_Images_via_Attention-Enhanced_Control_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Learning to Recognize Occluded and Small Objects with Partial Inputs](https://openaccess.thecvf.com/content/WACV2024/html/Zunair_Learning_To_Recognize_Occluded_and_Small_Objects_With_Partial_Inputs_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://hasibzunair.github.io/msl-recognition/) <br /> [![GitHub](https://img.shields.io/github/stars/hasibzunair/msl-recognition?style=flat)](https://github.com/hasibzunair/msl-recognition) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-demo-FFD21F.svg)](https://huggingface.co/spaces/hasibzunair/msl-recognition-demo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zunair_Learning_To_Recognize_Occluded_and_Small_Objects_With_Partial_Inputs_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.18517-b31b1b.svg)](http://arxiv.org/abs/2310.18517) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=koxIipyvmZk) |
| [Temporally-Consistent Video Semantic Segmentation with Bidirectional Occlusion-Guided Feature Propagation](https://openaccess.thecvf.com/content/WACV2024/html/Baghbaderani_Temporally-Consistent_Video_Semantic_Segmentation_With_Bidirectional_Occlusion-Guided_Feature_Propagation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Baghbaderani_Temporally-Consistent_Video_Semantic_Segmentation_With_Bidirectional_Occlusion-Guided_Feature_Propagation_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Towards Domain-Aware Knowledge Distillation for Continual Model Generalization](https://openaccess.thecvf.com/content/WACV2024/html/Reddy_Towards_Domain-Aware_Knowledge_Distillation_for_Continual_Model_Generalization_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dose-iitd.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Reddy_Towards_Domain-Aware_Knowledge_Distillation_for_Continual_Model_Generalization_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Let's Observe them Over Time: An Improved Pedestrian Attribute Recognition Approach](https://openaccess.thecvf.com/content/WACV2024/html/Thakare_Lets_Observe_Them_Over_Time_An_Improved_Pedestrian_Attribute_Recognition_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Thakare_Lets_Observe_Them_Over_Time_An_Improved_Pedestrian_Attribute_Recognition_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Harnessing the Power of Multi-Lingual Datasets for Pre-Training: Towards Enhancing Text Spotting Performance](https://openaccess.thecvf.com/content/WACV2024/html/Das_Harnessing_the_Power_of_Multi-Lingual_Datasets_for_Pre-Training_Towards_Enhancing_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/alloydas/TESTR_Eval?style=flat)](https://github.com/alloydas/TESTR_Eval) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Das_Harnessing_the_Power_of_Multi-Lingual_Datasets_for_Pre-Training_Towards_Enhancing_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.00917-b31b1b.svg)](http://arxiv.org/abs/2310.00917) | :heavy_minus_sign: |
| [Patch-based Selection and Refinement for Early Object Detection](https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Patch-Based_Selection_and_Refinement_for_Early_Object_Detection_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/destiny301/dpr?style=flat)](https://github.com/destiny301/dpr) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Patch-Based_Selection_and_Refinement_for_Early_Object_Detection_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.02274-b31b1b.svg)](http://arxiv.org/abs/2311.02274) | :heavy_minus_sign: |
| [Boosting Weakly Supervised Object Detection using Fusion and Priors from Hallucinated Depth](https://openaccess.thecvf.com/content/WACV2024/html/Gungor_Boosting_Weakly_Supervised_Object_Detection_Using_Fusion_and_Priors_From_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cagrigungor.github.io/WSOD-AMPLIFIER/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Gungor_Boosting_Weakly_Supervised_Object_Detection_Using_Fusion_and_Priors_From_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10937-b31b1b.svg)](http://arxiv.org/abs/2303.10937) | :heavy_minus_sign: |
| [C<sup>2</sup>AIR: Consolidated Compact Aerial Image Haze Removal](https://openaccess.thecvf.com/content/WACV2024/html/Kulkarni_C2AIR_Consolidated_Compact_Aerial_Image_Haze_Removal_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/AshutoshKulkarni4998/C2AIR?style=flat)](https://github.com/AshutoshKulkarni4998/C2AIR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Kulkarni_C2AIR_Consolidated_Compact_Aerial_Image_Haze_Removal_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Favoring One Among Equals - Not a Good Idea: Many-to-One Matching for Robust Transformer based Pedestrian Detection](https://openaccess.thecvf.com/content/WACV2024/html/Shastry_Favoring_One_Among_Equals_-_Not_a_Good_Idea_Many-to-One_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ajayshastry08.github.io/flow_matcher) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Shastry_Favoring_One_Among_Equals_-_Not_a_Good_Idea_Many-to-One_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Improving Vision-and-Language Reasoning via Spatial Relations Modeling](https://openaccess.thecvf.com/content/WACV2024/html/Yang_Improving_Vision-and-Language_Reasoning_via_Spatial_Relations_Modeling_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Yang_Improving_Vision-and-Language_Reasoning_via_Spatial_Relations_Modeling_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.05298-b31b1b.svg)](http://arxiv.org/abs/2311.05298) | :heavy_minus_sign: |
| [LP-OVOD: Open-Vocabulary Object Detection by Linear Probing](https://openaccess.thecvf.com/content/WACV2024/html/Pham_LP-OVOD_Open-Vocabulary_Object_Detection_by_Linear_Probing_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Pham_LP-OVOD_Open-Vocabulary_Object_Detection_by_Linear_Probing_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.17109-b31b1b.svg)](http://arxiv.org/abs/2310.17109) | :heavy_minus_sign: |
| [Continuous Adaptation for Interactive Segmentation using Teacher-Student Architecture](https://openaccess.thecvf.com/content/WACV2024/html/Atanyan_Continuous_Adaptation_for_Interactive_Segmentation_Using_Teacher-Student_Architecture_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Atanyan_Continuous_Adaptation_for_Interactive_Segmentation_Using_Teacher-Student_Architecture_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Interpretable Object Recognition by Semantic Prototype Analysis](https://openaccess.thecvf.com/content/WACV2024/html/Wan_Interpretable_Object_Recognition_by_Semantic_Prototype_Analysis_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/WanQiyang/SPANet?style=flat)](https://github.com/WanQiyang/SPANet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wan_Interpretable_Object_Recognition_by_Semantic_Prototype_Analysis_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [RecycleNet: Latent Feature Recycling Leads to Iterative Decision Refinement](https://openaccess.thecvf.com/content/WACV2024/html/Kohler_RecycleNet_Latent_Feature_Recycling_Leads_to_Iterative_Decision_Refinement_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Kohler_RecycleNet_Latent_Feature_Recycling_Leads_to_Iterative_Decision_Refinement_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.07513-b31b1b.svg)](http://arxiv.org/abs/2309.07513) | :heavy_minus_sign: |
| [Learning to Detour: Shortcut Mitigating Augmentation for Weakly Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Kwon_Learning_to_Detour_Shortcut_Mitigating_Augmentation_for_Weakly_Supervised_Semantic_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Kwon_Learning_to_Detour_Shortcut_Mitigating_Augmentation_for_Weakly_Supervised_Semantic_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Elusive Images: Beyond Coarse Analysis for Fine-Grained Recognition](https://openaccess.thecvf.com/content/WACV2024/html/Anderson_Elusive_Images_Beyond_Coarse_Analysis_for_Fine-Grained_Recognition_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://catalys1.github.io/elusive-images-fgvc/) <br /> [![GitHub](https://img.shields.io/github/stars/catalys1/elusive-images-fgvc?style=flat)](https://github.com/catalys1/elusive-images-fgvc) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Anderson_Elusive_Images_Beyond_Coarse_Analysis_for_Fine-Grained_Recognition_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Understanding Dark Scenes by Contrasting Multi-Modal Observations](https://openaccess.thecvf.com/content/WACV2024/html/Dong_Understanding_Dark_Scenes_by_Contrasting_Multi-Modal_Observations_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/palmdong/SMMCL?style=flat)](https://github.com/palmdong/SMMCL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Dong_Understanding_Dark_Scenes_by_Contrasting_Multi-Modal_Observations_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.12320-b31b1b.svg)](http://arxiv.org/abs/2308.12320) | :heavy_minus_sign: |
| [MaskConver: Revisiting Pure Convolution Model for Panoptic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Rashwan_MaskConver_Revisiting_Pure_Convolution_Model_for_Panoptic_Segmentation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Rashwan_MaskConver_Revisiting_Pure_Convolution_Model_for_Panoptic_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.06052-b31b1b.svg)](http://arxiv.org/abs/2312.06052) | :heavy_minus_sign: |
| [Masked Collaborative Contrast for Weakly Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Wu_Masked_Collaborative_Contrast_for_Weakly_Supervised_Semantic_Segmentation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/fwu11/MCC?style=flat)](https://github.com/fwu11/MCC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wu_Masked_Collaborative_Contrast_for_Weakly_Supervised_Semantic_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.08491-b31b1b.svg)](http://arxiv.org/abs/2305.08491) | :heavy_minus_sign: |
| [Universal Semi-Supervised Model Adaptation via Collaborative Consistency Training](https://openaccess.thecvf.com/content/WACV2024/html/Yan_Universal_Semi-Supervised_Model_Adaptation_via_Collaborative_Consistency_Training_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Yan_Universal_Semi-Supervised_Model_Adaptation_via_Collaborative_Consistency_Training_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.03449-b31b1b.svg)](http://arxiv.org/abs/2307.03449) | :heavy_minus_sign: |
| [STEP - Towards Structured Scene-Text Spotting](https://openaccess.thecvf.com/content/WACV2024/html/Garcia-Bordils_STEP_-_Towards_Structured_Scene-Text_Spotting_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/CVC-DAG/STEP?style=flat)](https://github.com/CVC-DAG/STEP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Garcia-Bordils_STEP_-_Towards_Structured_Scene-Text_Spotting_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.02356-b31b1b.svg)](http://arxiv.org/abs/2309.02356) | :heavy_minus_sign: |
| [Efficient Feature Distillation for Zero-Shot Annotation Object Detection](https://openaccess.thecvf.com/content/WACV2024/html/Liu_Efficient_Feature_Distillation_for_Zero-Shot_Annotation_Object_Detection_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/dragonlzm/EZAD?style=flat)](https://github.com/dragonlzm/EZAD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Efficient_Feature_Distillation_for_Zero-Shot_Annotation_Object_Detection_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12145-b31b1b.svg)](http://arxiv.org/abs/2303.12145) | :heavy_minus_sign: |
| [Hierarchical Text Spotter for Joint Text Spotting and Layout Analysis](https://openaccess.thecvf.com/content/WACV2024/html/Long_Hierarchical_Text_Spotter_for_Joint_Text_Spotting_and_Layout_Analysis_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Long_Hierarchical_Text_Spotter_for_Joint_Text_Spotting_and_Layout_Analysis_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.17674-b31b1b.svg)](http://arxiv.org/abs/2310.17674) | :heavy_minus_sign: |
| [iBARLE: imBalance-Aware Room Layout Estimation](https://openaccess.thecvf.com/content/WACV2024/html/Jing_iBARLE_imBalance-Aware_Room_Layout_Estimation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Jing_iBARLE_imBalance-Aware_Room_Layout_Estimation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.15050-b31b1b.svg)](http://arxiv.org/abs/2308.15050) | :heavy_minus_sign: |
| [TSP-Transformer: Task-Specific Prompts Boosted Transformer for Holistic Scene Understanding](https://openaccess.thecvf.com/content/WACV2024/html/Wang_TSP-Transformer_Task-Specific_Prompts_Boosted_Transformer_for_Holistic_Scene_Understanding_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/tb2-sy/TSP-Transformer?style=flat)](https://github.com/tb2-sy/TSP-Transformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_TSP-Transformer_Task-Specific_Prompts_Boosted_Transformer_for_Holistic_Scene_Understanding_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.03427-b31b1b.svg)](http://arxiv.org/abs/2311.03427) | :heavy_minus_sign: |
| [Implicit Neural Representation for Change Detection](https://openaccess.thecvf.com/content/WACV2024/html/Naylor_Implicit_Neural_Representation_for_Change_Detection_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Naylor_Implicit_Neural_Representation_for_Change_Detection_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.15428-b31b1b.svg)](http://arxiv.org/abs/2307.15428) | :heavy_minus_sign: |
| [Label-Free Synthetic Pretraining of Object Detectors](https://openaccess.thecvf.com/content/WACV2024/html/Law_Label-Free_Synthetic_Pretraining_of_Object_Detectors_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/princeton-vl/SOLID?style=flat)](https://github.com/princeton-vl/SOLID) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Law_Label-Free_Synthetic_Pretraining_of_Object_Detectors_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.04268-b31b1b.svg)](http://arxiv.org/abs/2208.04268) | :heavy_minus_sign: |
| [Improved Techniques for Quantizing Deep Networks with Adaptive Bit-Widths](https://openaccess.thecvf.com/content/WACV2024/html/Sun_Improved_Techniques_for_Quantizing_Deep_Networks_With_Adaptive_Bit-Widths_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Sun_Improved_Techniques_for_Quantizing_Deep_Networks_With_Adaptive_Bit-Widths_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2103.01435-b31b1b.svg)](http://arxiv.org/abs/2103.01435) | :heavy_minus_sign: |
| [What's Outside the Intersection? Fine-Grained Error Analysis for Semantic Segmentation Beyond IoU](https://openaccess.thecvf.com/content/WACV2024/html/Bernhard_Whats_Outside_the_Intersection_Fine-Grained_Error_Analysis_for_Semantic_Segmentation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/mxbh/beyond-iou?style=flat)](https://github.com/mxbh/beyond-iou) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Bernhard_Whats_Outside_the_Intersection_Fine-Grained_Error_Analysis_for_Semantic_Segmentation_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Pixel Matching Network for Cross-Domain Few-Shot Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Chen_Pixel_Matching_Network_for_Cross-Domain_Few-Shot_Segmentation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/chenhao-zju/PMNet?style=flat)](https://github.com/chenhao-zju/PMNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Pixel_Matching_Network_for_Cross-Domain_Few-Shot_Segmentation_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [EResFD: Rediscovery of the Effectiveness of Standard Convolution for Lightweight Face Detection](https://openaccess.thecvf.com/content/WACV2024/html/Jeong_EResFD_Rediscovery_of_the_Effectiveness_of_Standard_Convolution_for_Lightweight_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/clovaai/EResFD?style=flat)](https://github.com/clovaai/EResFD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Jeong_EResFD_Rediscovery_of_the_Effectiveness_of_Standard_Convolution_for_Lightweight_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2204.01209-b31b1b.svg)](http://arxiv.org/abs/2204.01209) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Ir93_4nzk0Y) |
| [Framework-Agnostic Semantically-Aware Global Reasoning for Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Hossain_Framework-Agnostic_Semantically-Aware_Global_Reasoning_for_Segmentation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Hossain_Framework-Agnostic_Semantically-Aware_Global_Reasoning_for_Segmentation_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [High-Fidelity Pseudo-Labels for Boosting Weakly-Supervised Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Jonnarth_High-Fidelity_Pseudo-Labels_for_Boosting_Weakly-Supervised_Segmentation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/arvijj/hfpl?style=flat)](https://github.com/arvijj/hfpl) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Jonnarth_High-Fidelity_Pseudo-Labels_for_Boosting_Weakly-Supervised_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02621-b31b1b.svg)](http://arxiv.org/abs/2304.02621) | :heavy_minus_sign: |
| [Missing Modality Robustness in Semi-Supervised Multi-Modal Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Maheshwari_Missing_Modality_Robustness_in_Semi-Supervised_Multi-Modal_Semantic_Segmentation_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://harshm121.github.io/projects/m3l.html) <br /> [![GitHub](https://img.shields.io/github/stars/harshm121/M3L?style=flat)](https://github.com/harshm121/M3L) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-demo-FFD21F.svg)](https://harshm121-m3l.hf.space/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Maheshwari_Missing_Modality_Robustness_in_Semi-Supervised_Multi-Modal_Semantic_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10756-b31b1b.svg)](http://arxiv.org/abs/2304.10756) | :heavy_minus_sign: |
| [Unsupervised Graphic Layout Grouping with Transformers](https://openaccess.thecvf.com/content/WACV2024/html/Zhu_Unsupervised_Graphic_Layout_Grouping_With_Transformers_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zhu_Unsupervised_Graphic_Layout_Grouping_With_Transformers_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Contrastive Viewpoint-Aware Shape Learning for Long-Term Person Re-Identification](https://openaccess.thecvf.com/content/WACV2024/html/Nguyen_Contrastive_Viewpoint-Aware_Shape_Learning_for_Long-Term_Person_Re-Identification_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Nguyen_Contrastive_Viewpoint-Aware_Shape_Learning_for_Long-Term_Person_Re-Identification_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [PolyMaX: General Dense Prediction with Mask Transformer](https://openaccess.thecvf.com/content/WACV2024/html/Yang_PolyMaX_General_Dense_Prediction_With_Mask_Transformer_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Yang_PolyMaX_General_Dense_Prediction_With_Mask_Transformer_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.05770-b31b1b.svg)](http://arxiv.org/abs/2311.05770) | :heavy_minus_sign: |
| [BPKD: Boundary Privileged Knowledge Distillation for Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Liu_BPKD_Boundary_Privileged_Knowledge_Distillation_for_Semantic_Segmentation_WACV_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://bpkd.vmv.re/) <br /> [![GitHub](https://img.shields.io/github/stars/AkideLiu/BPKD?style=flat)](https://github.com/AkideLiu/BPKD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Liu_BPKD_Boundary_Privileged_Knowledge_Distillation_for_Semantic_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.08075-b31b1b.svg)](http://arxiv.org/abs/2306.08075) | :heavy_minus_sign: |
| [Label Shift Estimation for Class-Imbalance Problem: A Bayesian Approach](https://openaccess.thecvf.com/content/WACV2024/html/Ye_Label_Shift_Estimation_for_Class-Imbalance_Problem_A_Bayesian_Approach_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/ChangkunYe/MAPLS?style=flat)](https://github.com/ChangkunYe/MAPLS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ye_Label_Shift_Estimation_for_Class-Imbalance_Problem_A_Bayesian_Approach_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Query-Guided Attention in Vision Transformers for Localizing Objects using a Single Sketch](https://openaccess.thecvf.com/content/WACV2024/html/Tripathi_Query-Guided_Attention_in_Vision_Transformers_for_Localizing_Objects_Using_a_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://vcl-iisc.github.io/locformer/) <br /> [![GitHub](https://img.shields.io/github/stars/vcl-iisc/locformer-SGOL?style=flat)](https://github.com/vcl-iisc/locformer-SGOL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Tripathi_Query-Guided_Attention_in_Vision_Transformers_for_Localizing_Objects_Using_a_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08784-b31b1b.svg)](http://arxiv.org/abs/2303.08784) | :heavy_minus_sign: |
| [PromptAD: Zero-Shot Anomaly Detection using Text Prompts](https://openaccess.thecvf.com/content/WACV2024/html/Li_PromptAD_Zero-Shot_Anomaly_Detection_Using_Text_Prompts_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Li_PromptAD_Zero-Shot_Anomaly_Detection_Using_Text_Prompts_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Learning Quality Labels for Robust Image Classification](https://openaccess.thecvf.com/content/WACV2024/html/Wang_Learning_Quality_Labels_for_Robust_Image_Classification_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Learning_Quality_Labels_for_Robust_Image_Classification_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Learning Transferable Representations for Image Anomaly Localization using Dense Pretraining](https://openaccess.thecvf.com/content/WACV2024/html/He_Learning_Transferable_Representations_for_Image_Anomaly_Localization_Using_Dense_Pretraining_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/terrlo/DS2?style=flat)](https://github.com/terrlo/DS2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/He_Learning_Transferable_Representations_for_Image_Anomaly_Localization_Using_Dense_Pretraining_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [SBCFormer: Lightweight Network Capable of Full-Size ImageNet Classification at 1 FPS on Single Board Computers](https://openaccess.thecvf.com/content/WACV2024/html/Lu_SBCFormer_Lightweight_Network_Capable_of_Full-Size_ImageNet_Classification_at_1_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/xyongLu/SBCFormer?style=flat)](https://github.com/xyongLu/SBCFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Lu_SBCFormer_Lightweight_Network_Capable_of_Full-Size_ImageNet_Classification_at_1_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.03747-b31b1b.svg)](http://arxiv.org/abs/2311.03747) | :heavy_minus_sign: |
| [High-Fidelity Zero-Shot Texture Anomaly Localization using Feature Correspondence Analysis](https://openaccess.thecvf.com/content/WACV2024/html/Ardelean_High-Fidelity_Zero-Shot_Texture_Anomaly_Localization_Using_Feature_Correspondence_Analysis_WACV_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://reality.tf.fau.de/publications/2024/ardelean2024highfidelity/ardelean2024highfidelity.html) <br /> [![GitHub](https://img.shields.io/github/stars/TArdelean/AnomalyLocalizationFCA?style=flat)](https://github.com/TArdelean/AnomalyLocalizationFCA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ardelean_High-Fidelity_Zero-Shot_Texture_Anomaly_Localization_Using_Feature_Correspondence_Analysis_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.06433-b31b1b.svg)](http://arxiv.org/abs/2304.06433) | :heavy_minus_sign: |
| [Grafting Vision Transformers](https://openaccess.thecvf.com/content/WACV2024/html/Park_Grafting_Vision_Transformers_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/jongwoopark7978/Grafting-Vision-Transformer?style=flat)](https://github.com/jongwoopark7978/Grafting-Vision-Transformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Park_Grafting_Vision_Transformers_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.15943-b31b1b.svg)](http://arxiv.org/abs/2210.15943) | :heavy_minus_sign: |
| [Rethinking Knowledge Distillation with Raw Features for Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Liu_Rethinking_Knowledge_Distillation_With_Raw_Features_for_Semantic_Segmentation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Rethinking_Knowledge_Distillation_With_Raw_Features_for_Semantic_Segmentation_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Efficient Expansion and Gradient based Task Inference for Replay Free Incremental Learning](https://openaccess.thecvf.com/content/WACV2024/html/Roy_Efficient_Expansion_and_Gradient_Based_Task_Inference_for_Replay_Free_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Roy_Efficient_Expansion_and_Gradient_Based_Task_Inference_for_Replay_Free_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.01188-b31b1b.svg)](http://arxiv.org/abs/2312.01188) | :heavy_minus_sign: |
| [CLRerNet: Improving Confidence of Lane Detection with LaneIoU](https://openaccess.thecvf.com/content/WACV2024/html/Honda_CLRerNet_Improving_Confidence_of_Lane_Detection_With_LaneIoU_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/hirotomusiker/CLRerNet?style=flat)](https://github.com/hirotomusiker/CLRerNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Honda_CLRerNet_Improving_Confidence_of_Lane_Detection_With_LaneIoU_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.08366-b31b1b.svg)](http://arxiv.org/abs/2305.08366) | :heavy_minus_sign: |
| [Multi-Modal Gaze Following in Conversational Scenarios](https://openaccess.thecvf.com/content/WACV2024/html/Hou_Multi-Modal_Gaze_Following_in_Conversational_Scenarios_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Hou_Multi-Modal_Gaze_Following_in_Conversational_Scenarios_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.05669-b31b1b.svg)](http://arxiv.org/abs/2311.05669) | :heavy_minus_sign: |
| [Enhancing Multi-View Pedestrian Detection through Generalized 3D Feature Pulling](https://openaccess.thecvf.com/content/WACV2024/html/Aung_Enhancing_Multi-View_Pedestrian_Detection_Through_Generalized_3D_Feature_Pulling_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Aung_Enhancing_Multi-View_Pedestrian_Detection_Through_Generalized_3D_Feature_Pulling_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Self-Supervised Learning for Visual Relationship Detection through Masked Bounding Box Reconstruction](https://openaccess.thecvf.com/content/WACV2024/html/Anastasakis_Self-Supervised_Learning_for_Visual_Relationship_Detection_Through_Masked_Bounding_Box_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/deeplab-ai/SelfSupervisedVRD?style=flat)](https://github.com/deeplab-ai/SelfSupervisedVRD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Anastasakis_Self-Supervised_Learning_for_Visual_Relationship_Detection_Through_Masked_Bounding_Box_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.04834-b31b1b.svg)](http://arxiv.org/abs/2311.04834) | :heavy_minus_sign: |
| [The Background also Matters: Background-Aware Motion-Guided Objects Discovery](https://openaccess.thecvf.com/content/WACV2024/html/Kara_The_Background_Also_Matters_Background-Aware_Motion-Guided_Objects_Discovery_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Kara_The_Background_Also_Matters_Background-Aware_Motion-Guided_Objects_Discovery_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.02633-b31b1b.svg)](http://arxiv.org/abs/2311.02633) | :heavy_minus_sign: |
| [Semi-Supervised Scene Change Detection by Distillation from Feature-Metric Alignment](https://openaccess.thecvf.com/content/WACV2024/html/Lee_Semi-Supervised_Scene_Change_Detection_by_Distillation_From_Feature-Metric_Alignment_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Lee_Semi-Supervised_Scene_Change_Detection_by_Distillation_From_Feature-Metric_Alignment_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [OmniVec: Learning Robust Representations with Cross Modal Sharing](https://openaccess.thecvf.com/content/WACV2024/html/Srivastava_OmniVec_Learning_Robust_Representations_With_Cross_Modal_Sharing_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Srivastava_OmniVec_Learning_Robust_Representations_With_Cross_Modal_Sharing_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.05709-b31b1b.svg)](http://arxiv.org/abs/2311.05709) | :heavy_minus_sign: |
| [Cross-Attention between Satellite and Ground Views for Enhanced Fine-Grained Robot Geo-Localization](https://openaccess.thecvf.com/content/WACV2024/html/Yuan_Cross-Attention_Between_Satellite_and_Ground_Views_for_Enhanced_Fine-Grained_Robot_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Yuan_Cross-Attention_Between_Satellite_and_Ground_Views_for_Enhanced_Fine-Grained_Robot_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Data Augmentation for Object Detection via Controllable Diffusion Models](https://openaccess.thecvf.com/content/WACV2024/html/Fang_Data_Augmentation_for_Object_Detection_via_Controllable_Diffusion_Models_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/FANGAreNotGnu/ControlAug?style=flat)](https://github.com/FANGAreNotGnu/ControlAug) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Fang_Data_Augmentation_for_Object_Detection_via_Controllable_Diffusion_Models_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Physical-Space Multi-Body Mesh Detection Achieved by Local Alignment and Global Dense Learning](https://openaccess.thecvf.com/content/WACV2024/html/Dong_Physical-Space_Multi-Body_Mesh_Detection_Achieved_by_Local_Alignment_and_Global_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Dong_Physical-Space_Multi-Body_Mesh_Detection_Achieved_by_Local_Alignment_and_Global_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Multi-Source Domain Adaptation for Object Detection with Prototype-based Mean Teacher](https://openaccess.thecvf.com/content/WACV2024/html/Belal_Multi-Source_Domain_Adaptation_for_Object_Detection_With_Prototype-Based_Mean_Teacher_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/imatif17/Prototype-Mean-Teacher?style=flat)](https://github.com/imatif17/Prototype-Mean-Teacher) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Belal_Multi-Source_Domain_Adaptation_for_Object_Detection_With_Prototype-Based_Mean_Teacher_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.14950-b31b1b.svg)](http://arxiv.org/abs/2309.14950) | :heavy_minus_sign: |
| [Beyond Self-Attention: Deformable Large Kernel Attention for Medical Image Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Azad_Beyond_Self-Attention_Deformable_Large_Kernel_Attention_for_Medical_Image_Segmentation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/xmindflow/deformableLKA?style=flat)](https://github.com/xmindflow/deformableLKA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Azad_Beyond_Self-Attention_Deformable_Large_Kernel_Attention_for_Medical_Image_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.00121-b31b1b.svg)](http://arxiv.org/abs/2309.00121) | :heavy_minus_sign: |
| [INCODE: Implicit Neural Conditioning with Prior Knowledge Embeddings](https://openaccess.thecvf.com/content/WACV2024/html/Kazerouni_INCODE_Implicit_Neural_Conditioning_With_Prior_Knowledge_Embeddings_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://xmindflow.github.io/incode) <br /> [![GitHub](https://img.shields.io/github/stars/xmindflow/INCODE?style=flat)](https://github.com/xmindflow/INCODE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Kazerouni_INCODE_Implicit_Neural_Conditioning_With_Prior_Knowledge_Embeddings_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.18846-b31b1b.svg)](http://arxiv.org/abs/2310.18846) | :heavy_minus_sign: |
| [ProcSim: Proxy-based Confidence for Robust Similarity Learning](https://openaccess.thecvf.com/content/WACV2024/html/Barbany_ProcSim_Proxy-Based_Confidence_for_Robust_Similarity_Learning_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Barbany_ProcSim_Proxy-Based_Confidence_for_Robust_Similarity_Learning_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.00668-b31b1b.svg)](http://arxiv.org/abs/2311.00668) | :heavy_minus_sign: |
| [Refine and Redistribute: Multi-Domain Fusion and Dynamic Label Assignment for Unbiased Scene Graph Generation](https://openaccess.thecvf.com/content/WACV2024/html/Zang_Refine_and_Redistribute_Multi-Domain_Fusion_and_Dynamic_Label_Assignment_for_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zang_Refine_and_Redistribute_Multi-Domain_Fusion_and_Dynamic_Label_Assignment_for_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Joint Depth Prediction and Semantic Segmentation with Multi-View SAM](https://openaccess.thecvf.com/content/WACV2024/html/Shvets_Joint_Depth_Prediction_and_Semantic_Segmentation_With_Multi-View_SAM_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Shvets_Joint_Depth_Prediction_and_Semantic_Segmentation_With_Multi-View_SAM_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.00134-b31b1b.svg)](http://arxiv.org/abs/2311.00134) | :heavy_minus_sign: |
| [Self-Supervised Relation Alignment for Scene Graph Generation](https://openaccess.thecvf.com/content/WACV2024/html/Xu_Self-Supervised_Relation_Alignment_for_Scene_Graph_Generation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Xu_Self-Supervised_Relation_Alignment_for_Scene_Graph_Generation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.01403-b31b1b.svg)](http://arxiv.org/abs/2302.01403) | :heavy_minus_sign: |
| [Semantic Transfer from Head to Tail: Enlarging Tail Margin for Long-Tailed Visual Recognition](https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Semantic_Transfer_From_Head_to_Tail_Enlarging_Tail_Margin_for_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Semantic_Transfer_From_Head_to_Tail_Enlarging_Tail_Margin_for_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [PatchRefineNet: Improving Binary Segmentation by Incorporating Signals from Optimal Patch-Wise Binarization](https://openaccess.thecvf.com/content/WACV2024/html/Nagendra_PatchRefineNet_Improving_Binary_Segmentation_by_Incorporating_Signals_From_Optimal_Patch-Wise_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/savinay95n/PatchRefineNet?style=flat)](https://github.com/savinay95n/PatchRefineNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Nagendra_PatchRefineNet_Improving_Binary_Segmentation_by_Incorporating_Signals_From_Optimal_Patch-Wise_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.06560-b31b1b.svg)](http://arxiv.org/abs/2211.06560) | :heavy_minus_sign: |
| [Adaptive Deep Neural Network Inference Optimization with EENet](https://openaccess.thecvf.com/content/WACV2024/html/Ilhan_Adaptive_Deep_Neural_Network_Inference_Optimization_With_EENet_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/git-disl/EENet?style=flat)](https://github.com/git-disl/EENet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ilhan_Adaptive_Deep_Neural_Network_Inference_Optimization_With_EENet_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.07099-b31b1b.svg)](http://arxiv.org/abs/2301.07099) | :heavy_minus_sign: |
| [Token Fusion: Bridging the Gap between Token Pruning and Token Merging](https://openaccess.thecvf.com/content/WACV2024/html/Kim_Token_Fusion_Bridging_the_Gap_Between_Token_Pruning_and_Token_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Kim_Token_Fusion_Bridging_the_Gap_Between_Token_Pruning_and_Token_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.01026-b31b1b.svg)](http://arxiv.org/abs/2312.01026) | :heavy_minus_sign: |
| [Pruning from Scratch via Shared Pruning Module and Nuclear Norm-based Regularization](https://openaccess.thecvf.com/content/WACV2024/html/Lee_Pruning_From_Scratch_via_Shared_Pruning_Module_and_Nuclear_Norm-Based_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/jsleeg98/NuSPM?style=flat)](https://github.com/jsleeg98/NuSPM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Lee_Pruning_From_Scratch_via_Shared_Pruning_Module_and_Nuclear_Norm-Based_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [CLIP-DIY: CLIP Dense Inference Yields Open-Vocabulary Semantic Segmentation For-Free](https://openaccess.thecvf.com/content/WACV2024/html/Wysoczanska_CLIP-DIY_CLIP_Dense_Inference_Yields_Open-Vocabulary_Semantic_Segmentation_For-Free_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/wysoczanska/clip-diy?style=flat)](https://github.com/wysoczanska/clip-diy) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wysoczanska_CLIP-DIY_CLIP_Dense_Inference_Yields_Open-Vocabulary_Semantic_Segmentation_For-Free_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.14289-b31b1b.svg)](http://arxiv.org/abs/2309.14289) | :heavy_minus_sign: |
| [Layer-Wise Auto-Weighting for Non-Stationary Test-Time Adaptation](https://openaccess.thecvf.com/content/WACV2024/html/Park_Layer-Wise_Auto-Weighting_for_Non-Stationary_Test-Time_Adaptation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/junia3/LayerwiseTTA?style=flat)](https://github.com/junia3/LayerwiseTTA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Park_Layer-Wise_Auto-Weighting_for_Non-Stationary_Test-Time_Adaptation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.05858-b31b1b.svg)](http://arxiv.org/abs/2311.05858) | :heavy_minus_sign: |
| [Uncertainty Estimation in Instance Segmentation with Star-Convex Shapes](https://openaccess.thecvf.com/content/WACV2024/html/Siddiqui_Uncertainty_Estimation_in_Instance_Segmentation_With_Star-Convex_Shapes_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Siddiqui_Uncertainty_Estimation_in_Instance_Segmentation_With_Star-Convex_Shapes_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.10513-b31b1b.svg)](http://arxiv.org/abs/2309.10513) | :heavy_minus_sign: |
| [CamoFocus: Enhancing Camouflage Object Detection with Split-Feature Focal Modulation and Context Refinement](https://openaccess.thecvf.com/content/WACV2024/html/Khan_CamoFocus_Enhancing_Camouflage_Object_Detection_With_Split-Feature_Focal_Modulation_and_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Khan_CamoFocus_Enhancing_Camouflage_Object_Detection_With_Split-Feature_Focal_Modulation_and_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [HalluciDet: Hallucinating RGB Modality for Person Detection through Privileged Information](https://openaccess.thecvf.com/content/WACV2024/html/Medeiros_HalluciDet_Hallucinating_RGB_Modality_for_Person_Detection_Through_Privileged_Information_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/heitorrapela/HalluciDet?style=flat)](https://github.com/heitorrapela/HalluciDet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Medeiros_HalluciDet_Hallucinating_RGB_Modality_for_Person_Detection_Through_Privileged_Information_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.04662-b31b1b.svg)](http://arxiv.org/abs/2310.04662) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=spH6mHMHapw) |
| [Spectroformer: Multi-Domain Query Cascaded Transformer Network for Underwater Image Enhancement](https://openaccess.thecvf.com/content/WACV2024/html/Khan_Spectroformer_Multi-Domain_Query_Cascaded_Transformer_Network_for_Underwater_Image_Enhancement_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/Mdraqibkhan/Spectroformer?style=flat)](https://github.com/Mdraqibkhan/Spectroformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Khan_Spectroformer_Multi-Domain_Query_Cascaded_Transformer_Network_for_Underwater_Image_Enhancement_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [FOSSIL: Free Open-Vocabulary Semantic Segmentation through Synthetic References Retrieval](https://openaccess.thecvf.com/content/WACV2024/html/Barsellotti_FOSSIL_Free_Open-Vocabulary_Semantic_Segmentation_Through_Synthetic_References_Retrieval_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Barsellotti_FOSSIL_Free_Open-Vocabulary_Semantic_Segmentation_Through_Synthetic_References_Retrieval_WACV_2024_paper.pdf) | :heavy_minus_sign: |
