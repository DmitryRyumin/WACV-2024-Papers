# WACV-2024-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/visualization.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/agriculture.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Video Recognition and Understanding

![Section Papers](https://img.shields.io/badge/Section%20Papers-soon-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-soon-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| [Detecting Content Segments from Online Sports Streaming Events: Challenges and Solutions](https://openaccess.thecvf.com/content/WACV2024/html/Liu_Detecting_Content_Segments_From_Online_Sports_Streaming_Events_Challenges_and_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Detecting_Content_Segments_From_Online_Sports_Streaming_Events_Challenges_and_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Permutation-Aware Activity Segmentation via Unsupervised Frame-to-Segment Alignment](https://openaccess.thecvf.com/content/WACV2024/html/Tran_Permutation-Aware_Activity_Segmentation_via_Unsupervised_Frame-To-Segment_Alignment_WACV_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://retrocausal.ai/rp-5-permutation-aware-action-segmentation-via-unsupervised-frame-to-segment-alignment/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Tran_Permutation-Aware_Activity_Segmentation_via_Unsupervised_Frame-To-Segment_Alignment_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.19478-b31b1b.svg)](http://arxiv.org/abs/2305.19478) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ZgvbwD3h-fc) |
| [OTAS: Unsupervised Boundary Detection for Object-Centric Temporal Action Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Li_OTAS_Unsupervised_Boundary_Detection_for_Object-Centric_Temporal_Action_Segmentation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/yl596/OTAS?style=flat)](https://github.com/yl596/OTAS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Li_OTAS_Unsupervised_Boundary_Detection_for_Object-Centric_Temporal_Action_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.06276-b31b1b.svg)](http://arxiv.org/abs/2309.06276) | :heavy_minus_sign: |
| [Embodied Human Activity Recognition](https://openaccess.thecvf.com/content/WACV2024/html/Hu_Embodied_Human_Activity_Recognition_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Hu_Embodied_Human_Activity_Recognition_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Semantic-Aware Video Representation for Few-Shot Action Recognition](https://openaccess.thecvf.com/content/WACV2024/html/Tang_Semantic-Aware_Video_Representation_for_Few-Shot_Action_Recognition_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Tang_Semantic-Aware_Video_Representation_for_Few-Shot_Action_Recognition_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.06218-b31b1b.svg)](http://arxiv.org/abs/2311.06218) | :heavy_minus_sign: |
| [Leveraging the Power of Data Augmentation for Transformer-based Tracking](https://openaccess.thecvf.com/content/WACV2024/html/Zhao_Leveraging_the_Power_of_Data_Augmentation_for_Transformer-Based_Tracking_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/zj5559/DATr?style=flat)](https://github.com/zj5559/DATr) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zhao_Leveraging_the_Power_of_Data_Augmentation_for_Transformer-Based_Tracking_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.08264-b31b1b.svg)](http://arxiv.org/abs/2309.08264) | :heavy_minus_sign: |
| [CAMOT: Camera Angle-Aware Multi-Object Tracking](https://openaccess.thecvf.com/content/WACV2024/html/Limanta_CAMOT_Camera_Angle-Aware_Multi-Object_Tracking_WACV_2024_paper.html) | :heavy_minus_sign:  | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Limanta_CAMOT_Camera_Angle-Aware_Multi-Object_Tracking_WACV_2024_paper.pdf) | :heavy_minus_sign:  |
| [Detection Defenses: An Empty Promise Against Adversarial Patch Attacks on Optical Flow](https://openaccess.thecvf.com/content/WACV2024/html/Scheurer_Detection_Defenses_An_Empty_Promise_Against_Adversarial_Patch_Attacks_on_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/cv-stuttgart/DetectionDefenses?style=flat)](https://github.com/cv-stuttgart/DetectionDefenses) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Scheurer_Detection_Defenses_An_Empty_Promise_Against_Adversarial_Patch_Attacks_on_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.17403-b31b1b.svg)](http://arxiv.org/abs/2310.17403) | :heavy_minus_sign: |
| [Repetitive Action Counting with Motion Feature Learning](https://openaccess.thecvf.com/content/WACV2024/html/Li_Repetitive_Action_Counting_With_Motion_Feature_Learning_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Li_Repetitive_Action_Counting_With_Motion_Feature_Learning_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [United we Stand, Divided we Fall: UnityGraph for Unsupervised Procedure Learning from Videos](https://openaccess.thecvf.com/content/WACV2024/html/Bansal_United_We_Stand_Divided_We_Fall_UnityGraph_for_Unsupervised_Procedure_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Bansal_United_We_Stand_Divided_We_Fall_UnityGraph_for_Unsupervised_Procedure_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.03550-b31b1b.svg)](http://arxiv.org/abs/2311.03550) | :heavy_minus_sign: |
| Sequential Transformer for End-to-End Video Text Detection |  |  |  |
| Context in Human Action through Motion Complementarity |  |  |  |
| Egocentric Action Recognition by Capturing Hand-Object Contact and Object State |  |  |  |
| MIDAS: Mixing Ambiguous Data with Soft Labels for Dynamic Facial Expression Recognition |  |  |  |
| FRoG-MOT: Fast and Robust Generic Multiple-Object Tracking by IoU and Motion-State Associations |  |  |  |
| Density-based Flow Mask Integration via Deformable Convolution for Video People Flux Estimation |  |  |  |
| ConfTrack: Kalman Filter-based Multi-Person Tracking by Utilizing Confidence Score of Detection Box |  |  |  |
| CGAPoseNet+GCAN: A Geometric Clifford Algebra Network for Geometry-Aware Camera Pose Regression |  |  |  |
| Embedding Task Structure for Action Detection |  |  |  |
| Random Walks for Temporal Action Segmentation with Timestamp Supervision |  |  |  |
| MITFAS: Mutual Information based Temporal Feature Alignment and Sampling for Aerial Video Action Recognition |  |  |  |
| Do VSR Models Generalize Beyond LRS3? |  |  |  |
| PGVT: Pose-Guided Video Transformer for Fine-Grained Action Recognition |  |  |  |
| Differentially Private Video Activity Recognition |  |  |  |
| Video Instance Matting |  |  |  |
| VMFormer: End-to-End Video Matting with Transformer |  |  |  |
| DDAM-PS: Diligent Domain Adaptive Mixer for Person Search |  |  |  |
| Think Before you Simulate: Symbolic Reasoning to Orchestrate Neural Computation for Counterfactual Question Answering |  |  |  |
| Separable Self and Mixed Attention Transformers for Efficient Object Tracking |  |  |  |
| Restoring Degraded Old Films with Recursive Recurrent Transformer Networks |  |  |  |
| Holistic Representation Learning for Multitask Trajectory Anomaly Detection |  |  |  |
| Interaction Region Visual Transformer for Egocentric Action Anticipation |  |  |  |
| Object-Centric Video Representation for Long-Term Action Anticipation |  |  |  |
| A Hybrid Graph Network for Complex Activity Detection in Video |  |  |  |
| SSVOD: Semi-Supervised Video Object Detection with Sparse Annotations |  |  |  |
| Semantic Fusion Augmentation and Semantic Boundary Detection: A Novel Approach to Multi-Target Video Moment Retrieval |  |  |  |
| A Coarse-to-Fine Pseudo-Labeling (C2FPL) Framework for Unsupervised Video Anomaly Detection |  |  |  |
| PromptonomyViT: Multi-Task Prompt Learning Improves Video Transformers using Synthetic Scene Data |  |  |  |
| GLAD: Global-Local View Alignment and Background Debiasing for Unsupervised Video Domain Adaptation with Large Domain Gap |  |  |  |
| Beyond SOT: Tracking Multiple Generic Objects at Once |  |  |  |
| MFT: Long-Term Tracking of Every Pixel |  |  |  |
| Real-Time Weakly Supervised Video Anomaly Detection |  |  |  |
| Single-Image Deblurring, Trajectory and Shape Recovery of Fast Moving Objects with Denoising Diffusion Probabilistic Models |  |  |  |
| Contrastive Learning for Multi-Object Tracking with Transformers |  |  |  |
| Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders |  |  |  |
| JOADAA: Joint Online Action Detection and Action Anticipation |  |  |  |
| CCMR: High Resolution Optical Flow Estimation via Coarse-to-Fine Context-Guided Motion Reasoning |  |  |  |
| Weakly-Supervised Representation Learning for Video Alignment and Analysis |  |  |  |
| MotionAGFormer: Enhancing 3D Human Pose Estimation with a Transformer-GCNFormer Network |  |  |  |
| Leveraging Synthetic Data to Learn Video Stabilization Under Adverse Conditions |  |  |  |
| What's in the Flow? Exploiting Temporal Motion Cues for Unsupervised Generic Event Boundary Detection |  |  |  |
| Learning the what and how of Annotation in Video Object Segmentation |  |  |  |
| Lightweight Delivery Detection on Doorbell Cameras |  |  |  |
| Spatio-Temporal Filter Analysis Improves 3D-CNN for Action Classification |  |  |  |
| PMI Sampler: Patch Similarity Guided Frame Selection for Aerial Action Recognition |  |  |  |
| Optimizing Long-Term Robot Tracking with Multi-Platform Sensor Fusion |  |  |  |
| Learnable Cube-based Video Encryption for Privacy-Preserving Action Recognition |  |  |  |
| A*: Atrous Spatial Temporal Action Recognition for Real-Time Applications |  |  |  |
| SEMA: Semantic Attention for Capturing Long-Range Dependencies in Egocentric Lifelogs |  |  |  |
| Triplet Attention Transformer for Spatiotemporal Predictive Learning |  |  |  |
| ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection |  |  |  |
