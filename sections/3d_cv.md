# WACV-2024-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/ml_afa.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/adversarial_learning_adversarial_attack_defense_methods.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## 3D Computer Vision

![Section Papers](https://img.shields.io/badge/Section%20Papers-30-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-15-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-4-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-4-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| [Task-Oriented Human-Object Interactions Generation with Implicit Neural Representations](https://openaccess.thecvf.com/content/WACV2024/html/Li_Task-Oriented_Human-Object_Interactions_Generation_With_Implicit_Neural_Representations_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Li_Task-Oriented_Human-Object_Interactions_Generation_With_Implicit_Neural_Representations_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13129-b31b1b.svg)](http://arxiv.org/abs/2303.13129) | :heavy_minus_sign: |
| [Depth from Asymmetric Frame-Event Stereo: A Divide-and-Conquer Approach](https://openaccess.thecvf.com/content/WACV2024/html/Chen_Depth_From_Asymmetric_Frame-Event_Stereo_A_Divide-and-Conquer_Approach_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Depth_From_Asymmetric_Frame-Event_Stereo_A_Divide-and-Conquer_Approach_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [TriPlaneNet: An Encoder for EG3D Inversion](https://openaccess.thecvf.com/content/WACV2024/html/Bhattarai_TriPlaneNet_An_Encoder_for_EG3D_Inversion_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://anantarb.github.io/triplanenet) <br /> [![GitHub](https://img.shields.io/github/stars/anantarb/triplanenet?style=flat)](https://github.com/anantarb/triplanenet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Bhattarai_TriPlaneNet_An_Encoder_for_EG3D_Inversion_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13497-b31b1b.svg)](http://arxiv.org/abs/2303.13497) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=NttXgBE12ec) |
| [Attentive Prototypes for Source-Free Unsupervised Domain Adaptive 3D Object Detection](https://openaccess.thecvf.com/content/WACV2024/html/Hegde_Attentive_Prototypes_for_Source-Free_Unsupervised_Domain_Adaptive_3D_Object_Detection_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/deeptibhegde/AttentivePrototypeSFUDA?style=flat)](https://github.com/deeptibhegde/AttentivePrototypeSFUDA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Hegde_Attentive_Prototypes_for_Source-Free_Unsupervised_Domain_Adaptive_3D_Object_Detection_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2111.15656-b31b1b.svg)](http://arxiv.org/abs/2111.15656) | :heavy_minus_sign: |
| [FIRe: Fast Inverse Rendering using Directional and Signed Distance Functions](https://openaccess.thecvf.com/content/WACV2024/html/Yenamandra_FIRe_Fast_Inverse_Rendering_Using_Directional_and_Signed_Distance_Functions_WACV_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://cvg.cit.tum.de/research/geometry/fire) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Yenamandra_FIRe_Fast_Inverse_Rendering_Using_Directional_and_Signed_Distance_Functions_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2203.16284-b31b1b.svg)](http://arxiv.org/abs/2203.16284) | :heavy_minus_sign: |
| [A Generic and Flexible Regularization Framework for NeRFs](https://openaccess.thecvf.com/content/WACV2024/html/Ehret_A_Generic_and_Flexible_Regularization_Framework_for_NeRFs_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ehret_A_Generic_and_Flexible_Regularization_Framework_for_NeRFs_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Multi-View 3D Object Reconstruction and Uncertainty Modelling with Neural Shape Prior](https://openaccess.thecvf.com/content/WACV2024/html/Liao_Multi-View_3D_Object_Reconstruction_and_Uncertainty_Modelling_With_Neural_Shape_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Liao_Multi-View_3D_Object_Reconstruction_and_Uncertainty_Modelling_With_Neural_Shape_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.11739-b31b1b.svg)](http://arxiv.org/abs/2306.11739) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=N0lMUMJtA1M) |
| [Neural Textured Deformable Meshes for Robust Analysis-by-Synthesis](https://openaccess.thecvf.com/content/WACV2024/html/Wang_Neural_Textured_Deformable_Meshes_for_Robust_Analysis-by-Synthesis_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Neural_Textured_Deformable_Meshes_for_Robust_Analysis-by-Synthesis_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.00118-b31b1b.svg)](http://arxiv.org/abs/2306.00118) | :heavy_minus_sign: |
| [Ray Deformation Networks for Novel View Synthesis of Refractive Objects](https://openaccess.thecvf.com/content/WACV2024/html/Deng_Ray_Deformation_Networks_for_Novel_View_Synthesis_of_Refractive_Objects_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Deng_Ray_Deformation_Networks_for_Novel_View_Synthesis_of_Refractive_Objects_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EnBIWbdRQCE) |
| [Registered and Segmented Deformable Object Reconstruction from a Single View Point Cloud](https://openaccess.thecvf.com/content/WACV2024/html/Henrich_Registered_and_Segmented_Deformable_Object_Reconstruction_From_a_Single_View_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Henrich_Registered_and_Segmented_Deformable_Object_Reconstruction_From_a_Single_View_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.07357-b31b1b.svg)](http://arxiv.org/abs/2311.07357) | :heavy_minus_sign: |
| [SupeRVol: Super-Resolution Shape and Reflectance Estimation in Inverse Volume Rendering](https://openaccess.thecvf.com/content/WACV2024/html/Brahimi_SupeRVol_Super-Resolution_Shape_and_Reflectance_Estimation_in_Inverse_Volume_Rendering_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Brahimi_SupeRVol_Super-Resolution_Shape_and_Reflectance_Estimation_in_Inverse_Volume_Rendering_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04968-b31b1b.svg)](http://arxiv.org/abs/2212.04968) | :heavy_minus_sign: |
| [RIMeshGNN: A Rotation-Invariant Graph Neural Network for Mesh Classification](https://openaccess.thecvf.com/content/WACV2024/html/Shakibajahromi_RIMeshGNN_A_Rotation-Invariant_Graph_Neural_Network_for_Mesh_Classification_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Shakibajahromi_RIMeshGNN_A_Rotation-Invariant_Graph_Neural_Network_for_Mesh_Classification_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [OptFlow: Fast Optimization-based Scene Flow Estimation without Supervision](https://openaccess.thecvf.com/content/WACV2024/html/Ahuja_OptFlow_Fast_Optimization-Based_Scene_Flow_Estimation_Without_Supervision_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ahuja_OptFlow_Fast_Optimization-Based_Scene_Flow_Estimation_Without_Supervision_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.02550-b31b1b.svg)](http://arxiv.org/abs/2401.02550) | :heavy_minus_sign: |
| [Point-DynRF: Point-based Dynamic Radiance Fields from a Monocular Video](https://openaccess.thecvf.com/content/WACV2024/html/Park_Point-DynRF_Point-Based_Dynamic_Radiance_Fields_From_a_Monocular_Video_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Park_Point-DynRF_Point-Based_Dynamic_Radiance_Fields_From_a_Monocular_Video_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.09647-b31b1b.svg)](http://arxiv.org/abs/2310.09647) | :heavy_minus_sign: |
| [LensNeRF: Rethinking Volume Rendering based on Thin-Lens Camera Model](https://openaccess.thecvf.com/content/WACV2024/html/Kim_LensNeRF_Rethinking_Volume_Rendering_Based_on_Thin-Lens_Camera_Model_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Kim_LensNeRF_Rethinking_Volume_Rendering_Based_on_Thin-Lens_Camera_Model_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Domain Adaptive 3D Shape Retrieval from Monocular Images](https://openaccess.thecvf.com/content/WACV2024/html/Pal_Domain_Adaptive_3D_Shape_Retrieval_From_Monocular_Images_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Pal_Domain_Adaptive_3D_Shape_Retrieval_From_Monocular_Images_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [HD-Fusion: Detailed Text-to-3D Generation Leveraging Multiple Noise Estimation](https://openaccess.thecvf.com/content/WACV2024/html/Wu_HD-Fusion_Detailed_Text-to-3D_Generation_Leveraging_Multiple_Noise_Estimation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wu_HD-Fusion_Detailed_Text-to-3D_Generation_Leveraging_Multiple_Noise_Estimation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.16183-b31b1b.svg)](http://arxiv.org/abs/2307.16183) | :heavy_minus_sign: |
| [Sparse Convolutional Networks for Surface Reconstruction from Noisy Point Clouds](https://openaccess.thecvf.com/content/WACV2024/html/Wang_Sparse_Convolutional_Networks_for_Surface_Reconstruction_From_Noisy_Point_Clouds_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Sparse_Convolutional_Networks_for_Surface_Reconstruction_From_Noisy_Point_Clouds_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Single Frame Semantic Segmentation using Multi-Modal Spherical Images](https://openaccess.thecvf.com/content/WACV2024/html/Guttikonda_Single_Frame_Semantic_Segmentation_Using_Multi-Modal_Spherical_Images_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/sguttikon/SFSS-MMSI?style=flat)](https://github.com/sguttikon/SFSS-MMSI) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Guttikonda_Single_Frame_Semantic_Segmentation_Using_Multi-Modal_Spherical_Images_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09369-b31b1b.svg)](http://arxiv.org/abs/2308.09369) | :heavy_minus_sign: |
| [Context-based Interpretable Spatio-Temporal Graph Convolutional Network for Human Motion Forecasting](https://openaccess.thecvf.com/content/WACV2024/html/Medina_Context-Based_Interpretable_Spatio-Temporal_Graph_Convolutional_Network_for_Human_Motion_Forecasting_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Medina_Context-Based_Interpretable_Spatio-Temporal_Graph_Convolutional_Network_for_Human_Motion_Forecasting_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent Multi-View Stereo](https://openaccess.thecvf.com/content/WACV2024/html/Vats_GC-MVSNet_Multi-View_Multi-Scale_Geometrically-Consistent_Multi-View_Stereo_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/vkvats/GC-MVSNet?style=flat)](https://github.com/vkvats/GC-MVSNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Vats_GC-MVSNet_Multi-View_Multi-Scale_Geometrically-Consistent_Multi-View_Stereo_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.19583-b31b1b.svg)](http://arxiv.org/abs/2310.19583) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7zq9OFMxsvw) |
| [SAM Fewshot Finetuning for Anatomical Segmentation in Medical Images](https://openaccess.thecvf.com/content/WACV2024/html/Xie_SAM_Fewshot_Finetuning_for_Anatomical_Segmentation_in_Medical_Images_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Xie_SAM_Fewshot_Finetuning_for_Anatomical_Segmentation_in_Medical_Images_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [MSCC: Multi-Scale Transformers for Camera Calibration](https://openaccess.thecvf.com/content/WACV2024/html/Song_MSCC_Multi-Scale_Transformers_for_Camera_Calibration_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Song_MSCC_Multi-Scale_Transformers_for_Camera_Calibration_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [A Geometry Loss Combination for 3D Human Pose Estimation](https://openaccess.thecvf.com/content/WACV2024/html/Matsune_A_Geometry_Loss_Combination_for_3D_Human_Pose_Estimation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Matsune_A_Geometry_Loss_Combination_for_3D_Human_Pose_Estimation_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [A Robust Diffusion Modeling Framework for Radar Camera 3D Object Detection](https://openaccess.thecvf.com/content/WACV2024/html/Wu_A_Robust_Diffusion_Modeling_Framework_for_Radar_Camera_3D_Object_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wu_A_Robust_Diffusion_Modeling_Framework_for_Radar_Camera_3D_Object_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [WalkFormer: Point Cloud Completion via Guided Walks](https://openaccess.thecvf.com/content/WACV2024/html/Zhang_WalkFormer_Point_Cloud_Completion_via_Guided_Walks_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_WalkFormer_Point_Cloud_Completion_via_Guided_Walks_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [MGM-AE: Self-Supervised Learning on 3D Shape using Mesh Graph Masked Autoencoders](https://openaccess.thecvf.com/content/WACV2024/html/Yang_MGM-AE_Self-Supervised_Learning_on_3D_Shape_Using_Mesh_Graph_Masked_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Yang_MGM-AE_Self-Supervised_Learning_on_3D_Shape_Using_Mesh_Graph_Masked_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Unsupervised 3D Pose Estimation with Non-Rigid Structure-from-Motion Modeling](https://openaccess.thecvf.com/content/WACV2024/html/Ji_Unsupervised_3D_Pose_Estimation_With_Non-Rigid_Structure-From-Motion_Modeling_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ji_Unsupervised_3D_Pose_Estimation_With_Non-Rigid_Structure-From-Motion_Modeling_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10705-b31b1b.svg)](http://arxiv.org/abs/2308.10705) | :heavy_minus_sign: |
| [Residual Graph Convolutional Network for Bird's-Eye-View Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Chen_Residual_Graph_Convolutional_Network_for_Birds-Eye-View_Semantic_Segmentation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Residual_Graph_Convolutional_Network_for_Birds-Eye-View_Semantic_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.04044-b31b1b.svg)](http://arxiv.org/abs/2312.04044) | :heavy_minus_sign: |
| [3D Human Pose Estimation with Two-Step Mixed-Training Strategy](https://openaccess.thecvf.com/content/WACV2024/html/Wang_3D_Human_Pose_Estimation_With_Two-Step_Mixed-Training_Strategy_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_3D_Human_Pose_Estimation_With_Two-Step_Mixed-Training_Strategy_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [RGB-D Mapping and Tracking in a Plenoxel Radiance Field](https://openaccess.thecvf.com/content/WACV2024/html/Teigen_RGB-D_Mapping_and_Tracking_in_a_Plenoxel_Radiance_Field_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/ysus33/RGB-D_Plenoxel_Mapping_Tracking?style=flat)](https://github.com/ysus33/RGB-D_Plenoxel_Mapping_Tracking) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Teigen_RGB-D_Mapping_and_Tracking_in_a_Plenoxel_Radiance_Field_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.03404-b31b1b.svg)](http://arxiv.org/abs/2307.03404) | :heavy_minus_sign: |
| [SOAP: Cross-Sensor Domain Adaptation for 3D Object Detection using Stationary Object Aggregation Pseudo-Labelling](https://openaccess.thecvf.com/content/WACV2024/html/Huang_SOAP_Cross-Sensor_Domain_Adaptation_for_3D_Object_Detection_Using_Stationary_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Huang_SOAP_Cross-Sensor_Domain_Adaptation_for_3D_Object_Detection_Using_Stationary_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.04230-b31b1b.svg)](http://arxiv.org/abs/2401.04230) | :heavy_minus_sign: |
| [BALF: Simple and Efficient Blur Aware Local Feature Detector](https://openaccess.thecvf.com/content/WACV2024/html/Zhao_BALF_Simple_and_Efficient_Blur_Aware_Local_Feature_Detector_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/ericzzj1989/BALF?style=flat)](https://github.com/ericzzj1989/BALF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zhao_BALF_Simple_and_Efficient_Blur_Aware_Local_Feature_Detector_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.14731-b31b1b.svg)](http://arxiv.org/abs/2211.14731) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=pSn80SS9mTM) |
| [MACP: Efficient Model Adaptation for Cooperative Perception](https://openaccess.thecvf.com/content/WACV2024/html/Ma_MACP_Efficient_Model_Adaptation_for_Cooperative_Perception_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://purduedigitaltwin.github.io/MACP/) <br /> [![GitHub](https://img.shields.io/github/stars/PurdueDigitalTwin/MACP?style=flat)](https://github.com/PurdueDigitalTwin/MACP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ma_MACP_Efficient_Model_Adaptation_for_Cooperative_Perception_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.16870-b31b1b.svg)](http://arxiv.org/abs/2310.16870) | :heavy_minus_sign: |
| [MAELi: Masked Autoencoder for Large-Scale LiDAR Point Clouds](https://openaccess.thecvf.com/content/WACV2024/html/Krispel_MAELi_Masked_Autoencoder_for_Large-Scale_LiDAR_Point_Clouds_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Krispel_MAELi_Masked_Autoencoder_for_Large-Scale_LiDAR_Point_Clouds_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.07207-b31b1b.svg)](http://arxiv.org/abs/2212.07207) |  |
| [HDMNet: A Hierarchical Matching Network with Double Attention for Large-Scale Outdoor LiDAR Point Cloud Registration](https://openaccess.thecvf.com/content/WACV2024/html/Xue_HDMNet_A_Hierarchical_Matching_Network_With_Double_Attention_for_Large-Scale_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Xue_HDMNet_A_Hierarchical_Matching_Network_With_Double_Attention_for_Large-Scale_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.18874-b31b1b.svg)](http://arxiv.org/abs/2310.18874) | :heavy_minus_sign: |
| [SGRec3D: Self-Supervised 3D Scene Graph Learning via Object-Level Scene Reconstruction](https://openaccess.thecvf.com/content/WACV2024/html/Koch_SGRec3D_Self-Supervised_3D_Scene_Graph_Learning_via_Object-Level_Scene_Reconstruction_WACV_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://kochsebastian.com/sgrec3d) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Koch_SGRec3D_Self-Supervised_3D_Scene_Graph_Learning_via_Object-Level_Scene_Reconstruction_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.15702-b31b1b.svg)](http://arxiv.org/abs/2309.15702) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=YB4n_vi0RoE) |
| [MPT: Mesh Pre-Training with Transformers for Human Pose and Mesh Reconstruction](https://openaccess.thecvf.com/content/WACV2024/html/Lin_MPT_Mesh_Pre-Training_With_Transformers_for_Human_Pose_and_Mesh_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Lin_MPT_Mesh_Pre-Training_With_Transformers_for_Human_Pose_and_Mesh_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13357-b31b1b.svg)](http://arxiv.org/abs/2211.13357) |  |
| [LInKs ''Lifting Independent Keypoints'' â€“ Partial Pose Lifting for Occlusion Handling with Improved Accuracy in 2D-3D Human Pose Estimation](https://openaccess.thecvf.com/content/WACV2024/html/Hardy_LInKs_Lifting_Independent_Keypoints_-_Partial_Pose_Lifting_for_Occlusion_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Hardy_LInKs_Lifting_Independent_Keypoints_-_Partial_Pose_Lifting_for_Occlusion_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.07243-b31b1b.svg)](http://arxiv.org/abs/2309.07243) | :heavy_minus_sign: |
| [ECSIC: Epipolar Cross Attention for Stereo Image Compression](https://openaccess.thecvf.com/content/WACV2024/html/Wodlinger_ECSIC_Epipolar_Cross_Attention_for_Stereo_Image_Compression_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/mwoedlinger/ecsic?style=flat)](https://github.com/mwoedlinger/ecsic) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wodlinger_ECSIC_Epipolar_Cross_Attention_for_Stereo_Image_Compression_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.10284-b31b1b.svg)](http://arxiv.org/abs/2307.10284) | :heavy_minus_sign: |
| [Robust Category-Level 3D Pose Estimation from Diffusion-Enhanced Synthetic Data](https://openaccess.thecvf.com/content/WACV2024/html/Yang_Robust_Category-Level_3D_Pose_Estimation_From_Diffusion-Enhanced_Synthetic_Data_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/YangYY06/synthetic_3d?style=flat)](https://github.com/YangYY06/synthetic_3d) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Yang_Robust_Category-Level_3D_Pose_Estimation_From_Diffusion-Enhanced_Synthetic_Data_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Open-NeRF: Towards Open Vocabulary NeRF Decomposition](https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Open-NeRF_Towards_Open_Vocabulary_NeRF_Decomposition_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/haoz19/Open-NeRF?style=flat)](https://github.com/haoz19/Open-NeRF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Open-NeRF_Towards_Open_Vocabulary_NeRF_Decomposition_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.16383-b31b1b.svg)](http://arxiv.org/abs/2310.16383) | :heavy_minus_sign: |
| [HAMMER: Learning Entropy Maps to Create Accurate 3D Models in Multi-View Stereo](https://openaccess.thecvf.com/content/WACV2024/html/Weilharter_HAMMER_Learning_Entropy_Maps_To_Create_Accurate_3D_Models_in_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Weilharter_HAMMER_Learning_Entropy_Maps_To_Create_Accurate_3D_Models_in_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Polarimetric PatchMatch Multi-View Stereo](https://openaccess.thecvf.com/content/WACV2024/html/Zhao_Polarimetric_PatchMatch_Multi-View_Stereo_WACV_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](http://www.ok.sc.e.titech.ac.jp/res/PolarPMS/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zhao_Polarimetric_PatchMatch_Multi-View_Stereo_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.07600-b31b1b.svg)](http://arxiv.org/abs/2311.07600) | :heavy_minus_sign: |
| [Solving the Plane-Sphere Ambiguity in Top-Down Structure-from-Motion](https://openaccess.thecvf.com/content/WACV2024/html/Haalck_Solving_the_Plane-Sphere_Ambiguity_in_Top-Down_Structure-From-Motion_WACV_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://top-down-sfm.cvmls.org/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Haalck_Solving_the_Plane-Sphere_Ambiguity_in_Top-Down_Structure-From-Motion_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Self-Annotated 3D Geometric Learning for Smeared Points Removal](https://openaccess.thecvf.com/content/WACV2024/html/Wang_Self-Annotated_3D_Geometric_Learning_for_Smeared_Points_Removal_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/wangmiaowei/wacv2024_smearedremover?style=flat)](https://github.com/wangmiaowei/wacv2024_smearedremover) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Self-Annotated_3D_Geometric_Learning_for_Smeared_Points_Removal_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.09029-b31b1b.svg)](http://arxiv.org/abs/2311.09029) | :heavy_minus_sign: |
| [3D Face Style Transfer with a Hybrid Solution of NeRF and Mesh Rasterization](https://openaccess.thecvf.com/content/WACV2024/html/Feng_3D_Face_Style_Transfer_With_a_Hybrid_Solution_of_NeRF_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Feng_3D_Face_Style_Transfer_With_a_Hybrid_Solution_of_NeRF_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.13168-b31b1b.svg)](http://arxiv.org/abs/2311.13168) | :heavy_minus_sign: |
| [A Sequential Learning-based Approach for Monocular Human Performance Capture](https://openaccess.thecvf.com/content/WACV2024/html/Chen_A_Sequential_Learning-Based_Approach_for_Monocular_Human_Performance_Capture_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Chen_A_Sequential_Learning-Based_Approach_for_Monocular_Human_Performance_Capture_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [ScanEnts3D: Exploiting Phrase-to-3D-Object Correspondences for Improved Visio-Linguistic Models in 3D Scenes](https://openaccess.thecvf.com/content/WACV2024/html/Abdelreheem_ScanEnts3D_Exploiting_Phrase-to-3D-Object_Correspondences_for_Improved_Visio-Linguistic_Models_in_3D_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://scanents3d.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/ScanEnts3D/ScanEnts3D.github.io?style=flat)](https://github.com/ScanEnts3D/ScanEnts3D.github.io) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Abdelreheem_ScanEnts3D_Exploiting_Phrase-to-3D-Object_Correspondences_for_Improved_Visio-Linguistic_Models_in_3D_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.06250-b31b1b.svg)](http://arxiv.org/abs/2212.06250) | :heavy_minus_sign: |
| [Global Occlusion-Aware Transformer for Robust Stereo Matching](https://openaccess.thecvf.com/content/WACV2024/html/Liu_Global_Occlusion-Aware_Transformer_for_Robust_Stereo_Matching_WACV_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](http://www.ok.sc.e.titech.ac.jp/res/DeepSM/wacv2024.html) <br /> [![GitHub](https://img.shields.io/github/stars/Magicboomliu/GOAT?style=flat)](https://github.com/Magicboomliu/GOAT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Global_Occlusion-Aware_Transformer_for_Robust_Stereo_Matching_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.14650-b31b1b.svg)](http://arxiv.org/abs/2312.14650) | :heavy_minus_sign: |
| MoRF: Mobile Realistic Fullbody Avatars from a Monocular Video |  |  |  |
| PointCT: Point Central Transformer Network for Weakly-Supervised Point Cloud Semantic Segmentation |  |  |  |
| Top-Down Beats Bottom-Up in 3D Instance Segmentation |  |  |  |
| Longformer: Longitudinal Transformer for Alzheimer's Disease Classification with Structural MRIs |  |  |  |
| TEGLO: High Fidelity Canonical Texture Mapping from Single-View Images |  |  |  |
| DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification |  |  |  |
| FocusTune: Tuning Visual Localization through Focus-Guided Sampling |  |  |  |
| Indoor Visual Localization using Point and Line Correspondences in Dense Colored Point Cloud |  |  |  |
| Fast Sun-Aligned Outdoor Scene Relighting based on TensoRF |  |  |  |
| MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty |  |  |  |
| AvatarOne: Monocular 3D Human Animation |  |  |  |
| Deblur-NSFF: Neural Scene Flow Fields for Blurry Dynamic Scenes |  |  |  |
| SimpliMix: A Simplified Manifold Mixup for Few-Shot Point Cloud Classification |  |  |  |
| PMVC: Promoting Multi-View Consistency for 3D Scene Reconstruction |  |  |  |
| Hyb-NeRF: A Multiresolution Hybrid Encoding for Neural Radiance Fields |  |  |  |
| Joint 3D Shape and Motion Estimation from Rolling Shutter Light-Field Images |  |  |  |
| Sharp-NeRF: Grid-based Fast Deblurring Neural Radiance Fields using Sharpness Prior |  |  |  |
| When 3D Bounding-Box Meets SAM: Point Cloud Instance Segmentation with Weak-and-Noisy Supervision |  |  |  |
| Auto-BPA: An Enhanced Ball-Pivoting Algorithm with Adaptive Radius using Contextual Bandits |  |  |  |
| Towards Realistic Generative 3D Face Models |  |  |  |
| Camera-Independent Single Image Depth Estimation from Defocus Blur |  |  |  |
| U3DS3: Unsupervised 3D Semantic Scene Segmentation |  |  |  |
| SSP: Semi-Signed Prioritized Neural Fitting for Surface Reconstruction from Unoriented Point Clouds |  |  |  |
