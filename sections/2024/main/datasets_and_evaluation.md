# WACV-2024-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/2024/main/computational_photography_image_and_video_synthesis.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/2024/main/explainable_fair_accountable-privacy-preserving.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Datasets and Evaluation

![Section Papers](https://img.shields.io/badge/Section%20Papers-28-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-20-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-16-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-22-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| [IKEA Ego 3D Dataset: Understanding Furniture Assembly Actions from Ego-View 3D Point Clouds](https://openaccess.thecvf.com/content/WACV2024/html/Ben-Shabat_IKEA_Ego_3D_Dataset_Understanding_Furniture_Assembly_Actions_From_Ego-View_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sitzikbs.github.io/IKEAEgo3D.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ben-Shabat_IKEA_Ego_3D_Dataset_Understanding_Furniture_Assembly_Actions_From_Ego-View_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=V7n1BGRMyBk) |
| [IndustReal: A Dataset for Procedure Step Recognition Handling Execution Errors in Egocentric Videos in an Industrial-Like Setting](https://openaccess.thecvf.com/content/WACV2024/html/Schoonbeek_IndustReal_A_Dataset_for_Procedure_Step_Recognition_Handling_Execution_Errors_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://timschoonbeek.github.io/industreal.html) <br /> [![GitHub](https://img.shields.io/github/stars/TimSchoonbeek/IndustReal?style=flat)](https://github.com/TimSchoonbeek/IndustReal) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Schoonbeek_IndustReal_A_Dataset_for_Procedure_Step_Recognition_Handling_Execution_Errors_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.17323-b31b1b.svg)](http://arxiv.org/abs/2310.17323) | :heavy_minus_sign: |
| [Ego2HandsPose: A Dataset for Egocentric Two-Hand 3D Global Pose Estimation](https://openaccess.thecvf.com/content/WACV2024/html/Lin_Ego2HandsPose_A_Dataset_for_Egocentric_Two-Hand_3D_Global_Pose_Estimation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Lin_Ego2HandsPose_A_Dataset_for_Egocentric_Two-Hand_3D_Global_Pose_Estimation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.04927-b31b1b.svg)](http://arxiv.org/abs/2206.04927) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vs-xLJ-wAmw) |
| [ISAR: A Benchmark for Single- and Few-Shot Object Instance Segmentation and Re-Identification](https://openaccess.thecvf.com/content/WACV2024/html/Gorlo_ISAR_A_Benchmark_for_Single-_and_Few-Shot_Object_Instance_Segmentation_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://nicogorlo.github.io/isar_wacv24/) <br /> [![GitHub](https://img.shields.io/github/stars/nicogorlo/isar?style=flat)](https://github.com/nicogorlo/isar) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Gorlo_ISAR_A_Benchmark_for_Single-_and_Few-Shot_Object_Instance_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.02734-b31b1b.svg)](http://arxiv.org/abs/2311.02734) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lzXE-Pk020Q) |
| [Towards Addressing the Misalignment of Object Proposal Evaluation for Vision-Language Tasks via Semantic Grounding](https://openaccess.thecvf.com/content/WACV2024/html/Feinglass_Towards_Addressing_the_Misalignment_of_Object_Proposal_Evaluation_for_Vision-Language_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/JoshuaFeinglass/VL-detector-eval?style=flat)](https://github.com/JoshuaFeinglass/VL-detector-eval) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Feinglass_Towards_Addressing_the_Misalignment_of_Object_Proposal_Evaluation_for_Vision-Language_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.00215-b31b1b.svg)](http://arxiv.org/abs/2309.00215) | :heavy_minus_sign: |
| [SphereCraft: A Dataset for Spherical Keypoint Detection, Matching and Camera Pose Estimation](https://openaccess.thecvf.com/content/WACV2024/html/Gava_SphereCraft_A_Dataset_for_Spherical_Keypoint_Detection_Matching_and_Camera_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dfki.github.io/spherecraftweb/) <br /> [![GitHub](https://img.shields.io/github/stars/DFKI/spherecrafthub?style=flat)](https://github.com/DFKI/spherecrafthub) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Gava_SphereCraft_A_Dataset_for_Spherical_Keypoint_Detection_Matching_and_Camera_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Wz09mdMs1-Q) |
| [Benchmark Generation Framework with Customizable Distortions for Image Classifier Robustness](Sarkar_Benchmark_Generation_Framework_With_Customizable_Distortions_for_Image_Classifier_Robustness_WACV_2024_paper) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Sarkar_Benchmark_Generation_Framework_With_Customizable_Distortions_for_Image_Classifier_Robustness_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.18626-b31b1b.svg)](http://arxiv.org/abs/2310.18626) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-MY2hURq8uQ) |
| [UOW-Vessel: A Benchmark Dataset of High-Resolution Optical Satellite Images for Vessel Detection and Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Bui_UOW-Vessel_A_Benchmark_Dataset_of_High-Resolution_Optical_Satellite_Images_for_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/yangdi-cv/UOW-Vessel?style=flat)](https://github.com/yangdi-cv/UOW-Vessel) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Bui_UOW-Vessel_A_Benchmark_Dataset_of_High-Resolution_Optical_Satellite_Images_for_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [NITEC: Versatile Hand-Annotated Eye Contact Dataset for Ego-Vision Interaction](https://openaccess.thecvf.com/content/WACV2024/html/Hempel_NITEC_Versatile_Hand-Annotated_Eye_Contact_Dataset_for_Ego-Vision_Interaction_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/thohemp/nitec?style=flat)](https://github.com/thohemp/nitec) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Hempel_NITEC_Versatile_Hand-Annotated_Eye_Contact_Dataset_for_Ego-Vision_Interaction_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.04505-b31b1b.svg)](http://arxiv.org/abs/2311.04505) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=BOksotevEgs) |
| [Time to Shine: Fine-Tuning Object Detection Models with Synthetic Adverse Weather Images](https://openaccess.thecvf.com/content/WACV2024/html/Rothmeier_Time_To_Shine_Fine-Tuning_Object_Detection_Models_With_Synthetic_Adverse_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Rothmeier_Time_To_Shine_Fine-Tuning_Object_Detection_Models_With_Synthetic_Adverse_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6ko_LsO7b24) |
| [Effects of Markers in Training Datasets on the Accuracy of 6D Pose Estimation](https://openaccess.thecvf.com/content/WACV2024/html/Rosskamp_Effects_of_Markers_in_Training_Datasets_on_the_Accuracy_of_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/JHRosskamp/6DPoseDataGenTools?style=flat)](https://github.com/JHRosskamp/6DPoseDataGenTools) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Rosskamp_Effects_of_Markers_in_Training_Datasets_on_the_Accuracy_of_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5vQOtByhOds) |
| [VEATIC: Video-based Emotion and Affect Tracking in Context Dataset](https://openaccess.thecvf.com/content/WACV2024/html/Ren_VEATIC_Video-Based_Emotion_and_Affect_Tracking_in_Context_Dataset_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://veatic.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/AlbusPeter/VEATIC?style=flat)](https://github.com/AlbusPeter/VEATIC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ren_VEATIC_Video-Based_Emotion_and_Affect_Tracking_in_Context_Dataset_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.06745-b31b1b.svg)](http://arxiv.org/abs/2309.06745) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vlQwantuVZ0) |
| [Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased Question-Answering](https://openaccess.thecvf.com/content/WACV2024/html/Liu_Tackling_Data_Bias_in_MUSIC-AVQA_Crafting_a_Balanced_Dataset_for_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/DragonLiu1995/MUSIC-AVQA-v2.0?style=flat)](https://github.com/DragonLiu1995/MUSIC-AVQA-v2.0) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Tackling_Data_Bias_in_MUSIC-AVQA_Crafting_a_Balanced_Dataset_for_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.06238-b31b1b.svg)](http://arxiv.org/abs/2310.06238) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Sq7WvaHgUMs) |
| [An Empirical Investigation Into Benchmarking Model Multiplicity for Trustworthy Machine Learning: A Case Study on Image Classification](https://openaccess.thecvf.com/content/WACV2024/html/Ganesh_An_Empirical_Investigation_Into_Benchmarking_Model_Multiplicity_for_Trustworthy_Machine_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ganesh_An_Empirical_Investigation_Into_Benchmarking_Model_Multiplicity_for_Trustworthy_Machine_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.14859-b31b1b.svg)](http://arxiv.org/abs/2311.14859) | :heavy_minus_sign: |
| [Can You Even Tell Left from Right? Presenting a New Challenge for VQA](https://openaccess.thecvf.com/content/WACV2024/html/Venkataraman_Can_You_Even_Tell_Left_From_Right_Presenting_a_New_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Venkataraman_Can_You_Even_Tell_Left_From_Right_Presenting_a_New_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2203.07664-b31b1b.svg)](http://arxiv.org/abs/2203.07664) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FAOWMHtYeRM) |
| [MuSHRoom: Multi-Sensor Hybrid Room Dataset for Joint 3D Reconstruction and Novel View Synthesis](https://openaccess.thecvf.com/content/WACV2024/html/Ren_MuSHRoom_Multi-Sensor_Hybrid_Room_Dataset_for_Joint_3D_Reconstruction_and_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://xuqianren.github.io/publications/MuSHRoom/) <br /> [![GitHub](https://img.shields.io/github/stars/TUTvision/MuSHRoom?style=flat)](https://github.com/TUTvision/MuSHRoom) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ren_MuSHRoom_Multi-Sensor_Hybrid_Room_Dataset_for_Joint_3D_Reconstruction_and_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.02778-b31b1b.svg)](http://arxiv.org/abs/2311.02778) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Xyz7MgpjnUg) |
| [RobustCLEVR: A Benchmark and Framework for Evaluating Robustness in Object-Centric Learning](https://openaccess.thecvf.com/content/WACV2024/html/Drenkow_RobustCLEVR_A_Benchmark_and_Framework_for_Evaluating_Robustness_in_Object-Centric_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Drenkow_RobustCLEVR_A_Benchmark_and_Framework_for_Evaluating_Robustness_in_Object-Centric_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.14899-b31b1b.svg)](http://arxiv.org/abs/2308.14899) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=oGnKTrh5ZI4) |
| [So You Think You Can Track?](https://openaccess.thecvf.com/content/WACV2024/html/Gloudemans_So_You_Think_You_Can_Track_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Gloudemans_So_You_Think_You_Can_Track_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.07268-b31b1b.svg)](http://arxiv.org/abs/2309.07268) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=yWae3YO_4Ds) |
| [Estimating Blood Alcohol Level through Facial Features for Driver Impairment Assessment](https://openaccess.thecvf.com/content/WACV2024/html/Keshtkaran_Estimating_Blood_Alcohol_Level_Through_Facial_Features_for_Driver_Impairment_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Keshtkaran_Estimating_Blood_Alcohol_Level_Through_Facial_Features_for_Driver_Impairment_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jS9dCihWcZU) |
| [ENIGMA-51: Towards a Fine-Grained Understanding of Human Behavior in Industrial Scenarios](https://openaccess.thecvf.com/content/WACV2024/html/Ragusa_ENIGMA-51_Towards_a_Fine-Grained_Understanding_of_Human_Behavior_in_Industrial_WACV_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://iplab.dmi.unict.it/ENIGMA-51/) <br /> [![GitHub](https://img.shields.io/github/stars/fpv-iplab/ENIGMA-51?style=flat)](https://github.com/fpv-iplab/ENIGMA-51) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ragusa_ENIGMA-51_Towards_a_Fine-Grained_Understanding_of_Human_Behavior_in_Industrial_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.14809-b31b1b.svg)](http://arxiv.org/abs/2309.14809) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=X2rVWTMwz1M) |
| [SciOL and MuLMS-Img: Introducing a Large-Scale Multimodal Scientific Dataset and Models for Image-Text Tasks in the Scientific Domain](https://openaccess.thecvf.com/content/WACV2024/html/Tarsi_SciOL_and_MuLMS-Img_Introducing_a_Large-Scale_Multimodal_Scientific_Dataset_and_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Tarsi_SciOL_and_MuLMS-Img_Introducing_a_Large-Scale_Multimodal_Scientific_Dataset_and_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [HaGRID - HAnd Gesture Recognition Image Dataset](https://openaccess.thecvf.com/content/WACV2024/html/Kapitanov_HaGRID_--_HAnd_Gesture_Recognition_Image_Dataset_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/hukenovs/hagrid?style=flat)](https://github.com/hukenovs/hagrid) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Kapitanov_HaGRID_--_HAnd_Gesture_Recognition_Image_Dataset_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.08219-b31b1b.svg)](http://arxiv.org/abs/2206.08219) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bSAcjaIPPR8) |
| [Identifying Label Errors in Object Detection Datasets by Loss Inspection](https://openaccess.thecvf.com/content/WACV2024/html/Schubert_Identifying_Label_Errors_in_Object_Detection_Datasets_by_Loss_Inspection_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Schubert_Identifying_Label_Errors_in_Object_Detection_Datasets_by_Loss_Inspection_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06999-b31b1b.svg)](http://arxiv.org/abs/2303.06999) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mHzYdbdjkKY) |
| [Exploring the Impact of Rendering Method and Motion Quality on Model Performance when using Multi-View Synthetic Data for Action Recognition](https://openaccess.thecvf.com/content/WACV2024/html/Panev_Exploring_the_Impact_of_Rendering_Method_and_Motion_Quality_on_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://humansensinglab.github.io/REMAG/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Panev_Exploring_the_Impact_of_Rendering_Method_and_Motion_Quality_on_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hDlpJMyibBQ) |
| [PsyMo: A Dataset for Estimating Self-Reported Psychological Traits from Gait](https://openaccess.thecvf.com/content/WACV2024/html/Cosma_PsyMo_A_Dataset_for_Estimating_Self-Reported_Psychological_Traits_From_Gait_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/cosmaadrian/psymo?style=flat)](https://github.com/cosmaadrian/psymo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Cosma_PsyMo_A_Dataset_for_Estimating_Self-Reported_Psychological_Traits_From_Gait_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10631-b31b1b.svg)](http://arxiv.org/abs/2308.10631) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DWOLOSBeW6g) |
| [IDD-AW: A Benchmark for Safe and Robust Segmentation of Drive Scenes in Unstructured Traffic and Adverse Weather](https://openaccess.thecvf.com/content/WACV2024/html/Shaik_IDD-AW_A_Benchmark_for_Safe_and_Robust_Segmentation_of_Drive_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://iddaw.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/Furqan7007/IDDAW_kit?style=flat)](https://github.com/Furqan7007/IDDAW_kit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Shaik_IDD-AW_A_Benchmark_for_Safe_and_Robust_Segmentation_of_Drive_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.14459-b31b1b.svg)](http://arxiv.org/abs/2311.14459) | :heavy_minus_sign: |
| [CrashCar101: Procedural Generation for Damage Assessment](https://openaccess.thecvf.com/content/WACV2024/html/Parslov_CrashCar101_Procedural_Generation_for_Damage_Assessment_WACV_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://crashcar.compute.dtu.dk/) <br /> [![GitHub](https://img.shields.io/github/stars/JensPars/CrashCar_procedural_generation?style=flat)](https://github.com/JensPars/CrashCar_procedural_generation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Parslov_CrashCar101_Procedural_Generation_for_Damage_Assessment_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.06536-b31b1b.svg)](http://arxiv.org/abs/2311.06536) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=WRdVyfPzB-w) |
| [ZRG: A Dataset for Multimodal 3D Residential Rooftop Understanding](https://openaccess.thecvf.com/content/WACV2024/html/Corley_ZRG_A_Dataset_for_Multimodal_3D_Residential_Rooftop_Understanding_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/isaaccorley/zrg-dataset?style=flat)](https://github.com/isaaccorley/zrg-dataset) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Corley_ZRG_A_Dataset_for_Multimodal_3D_Residential_Rooftop_Understanding_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.13219-b31b1b.svg)](http://arxiv.org/abs/2304.13219) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=05wPrlRdQkA) |
