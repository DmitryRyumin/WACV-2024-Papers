# WACV-2024-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/2024/main/oral_3d_ad_efpe_vx.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/2024/main/image_recognition_and_understanding.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## People; Automotive; Medical; Environmental; Robotics

![Section Papers](https://img.shields.io/badge/Section%20Papers-12-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-9-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-9-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-9-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| [You Can Run but Not Hide: Improving Gait Recognition with Intrinsic Occlusion Type Awareness](https://openaccess.thecvf.com/content/WACV2024/html/Gupta_You_Can_Run_but_Not_Hide_Improving_Gait_Recognition_With_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Gupta_You_Can_Run_but_Not_Hide_Improving_Gait_Recognition_With_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.02290-b31b1b.svg)](http://arxiv.org/abs/2312.02290) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mVF0AP7xnsw) |
| [RMFER: Semi-Supervised Contrastive Learning for Facial Expression Recognition with Reaction Mashup Video](https://openaccess.thecvf.com/content/WACV2024/html/Cho_RMFER_Semi-Supervised_Contrastive_Learning_for_Facial_Expression_Recognition_With_Reaction_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/yunseongcho/RMFER?style=flat)](https://github.com/yunseongcho/RMFER) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Cho_RMFER_Semi-Supervised_Contrastive_Learning_for_Facial_Expression_Recognition_With_Reaction_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [CSAM: A 2.5D Cross-Slice Attention Module for Anisotropic Volumetric Medical Image Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Hung_CSAM_A_2.5D_Cross-Slice_Attention_Module_for_Anisotropic_Volumetric_Medical_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/aL3x-O-o-Hung/CSAM?style=flat)](https://github.com/aL3x-O-o-Hung/CSAM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Hung_CSAM_A_2.5D_Cross-Slice_Attention_Module_for_Anisotropic_Volumetric_Medical_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.04942-b31b1b.svg)](http://arxiv.org/abs/2311.04942) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=miwi-3l1NZI) |
| [Motion Matters: Neural Motion Transfer for Better Camera Physiological Measurement](https://openaccess.thecvf.com/content/WACV2024/html/Paruchuri_Motion_Matters_Neural_Motion_Transfer_for_Better_Camera_Physiological_Measurement_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://motion-matters.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/Roni-Lab/MA-rPPG-Video-Toolbox?style=flat)](https://github.com/Roni-Lab/MA-rPPG-Video-Toolbox) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Paruchuri_Motion_Matters_Neural_Motion_Transfer_for_Better_Camera_Physiological_Measurement_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12059-b31b1b.svg)](http://arxiv.org/abs/2303.12059) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wOSZvWIEkpA) |
| [Image Labels Are All You Need for Coarse Seagrass Segmentation](https://openaccess.thecvf.com/content/WACV2024/html/Raine_Image_Labels_Are_All_You_Need_for_Coarse_Seagrass_Segmentation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/sgraine/bag-of-seagrass?style=flat)](https://github.com/sgraine/bag-of-seagrass) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Raine_Image_Labels_Are_All_You_Need_for_Coarse_Seagrass_Segmentation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.00973-b31b1b.svg)](http://arxiv.org/abs/2303.00973) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=K_KJfyRrtn4) |
| [WildlifeDatasets: An Open-Source Toolkit for Animal Re-Identification](https://openaccess.thecvf.com/content/WACV2024/html/Cermak_WildlifeDatasets_An_Open-Source_Toolkit_for_Animal_Re-Identification_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://wildlifedatasets.github.io/wildlife-datasets/) <br /> [![GitHub](https://img.shields.io/github/stars/WildlifeDatasets/wildlife-datasets?style=flat)](https://github.com/WildlifeDatasets/wildlife-datasets) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Cermak_WildlifeDatasets_An_Open-Source_Toolkit_for_Animal_Re-Identification_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.09118-b31b1b.svg)](http://arxiv.org/abs/2311.09118) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9vBEgmdii2w) |
| [HALSIE: Hybrid Approach to Learning Segmentation by Simultaneously Exploiting Image and Event Modalities](https://openaccess.thecvf.com/content/WACV2024/html/Biswas_HALSIE_Hybrid_Approach_to_Learning_Segmentation_by_Simultaneously_Exploiting_Image_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Biswas_HALSIE_Hybrid_Approach_to_Learning_Segmentation_by_Simultaneously_Exploiting_Image_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.10754-b31b1b.svg)](http://arxiv.org/abs/2211.10754) | :heavy_minus_sign: |
| [ParticleNeRF: A Particle-based Encoding for Online Neural Radiance Fields](https://openaccess.thecvf.com/content/WACV2024/html/Abou-Chakra_ParticleNeRF_A_Particle-Based_Encoding_for_Online_Neural_Radiance_Fields_WACV_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://sites.google.com/view/particlenerf) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Abou-Chakra_ParticleNeRF_A_Particle-Based_Encoding_for_Online_Neural_Radiance_Fields_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.04041-b31b1b.svg)](http://arxiv.org/abs/2211.04041) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DgBoFgCyXvs) |
| [Rethinking Visibility in Human Pose Estimation: Occluded Pose Reasoning via Transformers](https://openaccess.thecvf.com/content/WACV2024/html/Sun_Rethinking_Visibility_in_Human_Pose_Estimation_Occluded_Pose_Reasoning_via_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/pengzhansun/Occluded-Pose-Reasoning?style=flat)](https://github.com/pengzhansun/Occluded-Pose-Reasoning) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Sun_Rethinking_Visibility_in_Human_Pose_Estimation_Occluded_Pose_Reasoning_via_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nh2zy8STl9s) |
| [SICKLE: A Multi-Sensor Satellite Imagery Dataset Annotated with Multiple Key Cropping Parameters](https://openaccess.thecvf.com/content/WACV2024/html/Sani_SICKLE_A_Multi-Sensor_Satellite_Imagery_Dataset_Annotated_With_Multiple_Key_WACV_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://sites.google.com/iiitd.ac.in/sickle/home) <br /> [![GitHub](https://img.shields.io/github/stars/Depanshu-Sani/SICKLE?style=flat)](https://github.com/Depanshu-Sani/SICKLE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Sani_SICKLE_A_Multi-Sensor_Satellite_Imagery_Dataset_Annotated_With_Multiple_Key_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.00069-b31b1b.svg)](http://arxiv.org/abs/2312.00069) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=RxgTQYydH34) |
| [Semantic Labels-Aware Transformer Model for Searching Over a Large Collection of Lecture-Slides](https://openaccess.thecvf.com/content/WACV2024/html/Jobin_Semantic_Labels-Aware_Transformer_Model_for_Searching_Over_a_Large_Collection_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/jobinkv/LecSD?style=flat)](https://github.com/jobinkv/LecSD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Jobin_Semantic_Labels-Aware_Transformer_Model_for_Searching_Over_a_Large_Collection_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6BtL4B7wduQ) |
| [ConeQuest: A Benchmark for Cone Segmentation on Mars](https://openaccess.thecvf.com/content/WACV2024/html/Purohit_ConeQuest_A_Benchmark_for_Cone_Segmentation_on_Mars_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/kerner-lab/ConeQuest?style=flat)](https://github.com/kerner-lab/ConeQuest) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Purohit_ConeQuest_A_Benchmark_for_Cone_Segmentation_on_Mars_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.08657-b31b1b.svg)](http://arxiv.org/abs/2311.08657) | :heavy_minus_sign: |
