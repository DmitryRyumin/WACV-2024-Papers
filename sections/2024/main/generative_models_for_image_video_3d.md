# WACV-2024-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/2024/main/explainable_fair_accountable-privacy-preserving.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/2024/main/vision_language_and_other_modalities.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Generative Models for Image, Video, 3D, etc

![Section Papers](https://img.shields.io/badge/Section%20Papers-54-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-36-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-25-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-43-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| [Spiking Denoising Diffusion Probabilistic Models](https://openaccess.thecvf.com/content/WACV2024/html/Cao_Spiking_Denoising_Diffusion_Probabilistic_Models_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/AndyCao1125/SDDPM?style=flat)](https://github.com/AndyCao1125/SDDPM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Cao_Spiking_Denoising_Diffusion_Probabilistic_Models_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.17046-b31b1b.svg)](https://arxiv.org/abs/2306.17046) | :heavy_minus_sign: |
| [Improving the Effectiveness of Deep Generative Data](https://openaccess.thecvf.com/content/WACV2024/html/Wang_Improving_the_Effectiveness_of_Deep_Generative_Data_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Improving_the_Effectiveness_of_Deep_Generative_Data_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.03959-b31b1b.svg)](https://arxiv.org/abs/2311.03959) | :heavy_minus_sign: |
| [Customizing 360-Degree Panoramas through Text-to-Image Diffusion Models](https://openaccess.thecvf.com/content/WACV2024/html/Wang_Customizing_360-Degree_Panoramas_Through_Text-to-Image_Diffusion_Models_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://littlewhitesea.github.io/stitchdiffusion.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/littlewhitesea/StitchDiffusion?style=flat)](https://github.com/littlewhitesea/StitchDiffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Customizing_360-Degree_Panoramas_Through_Text-to-Image_Diffusion_Models_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.18840-b31b1b.svg)](https://arxiv.org/abs/2310.18840) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nf9uKNTNWzw) |
| [Label Augmentation as Inter-Class Data Augmentation for Conditional Image Synthesis with Imbalanced Data](https://openaccess.thecvf.com/content/WACV2024/html/Katsumata_Label_Augmentation_As_Inter-Class_Data_Augmentation_for_Conditional_Image_Synthesis_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/raven38/softlabel-gan?style=flat)](https://github.com/raven38/softlabel-gan) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Katsumata_Label_Augmentation_As_Inter-Class_Data_Augmentation_for_Conditional_Image_Synthesis_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [3D-Aware Talking-Head Video Motion Transfer](https://openaccess.thecvf.com/content/WACV2024/html/Ni_3D-Aware_Talking-Head_Video_Motion_Transfer_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ni_3D-Aware_Talking-Head_Video_Motion_Transfer_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.02549-b31b1b.svg)](https://arxiv.org/abs/2311.02549) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=t0j_jBNJ0eM) |
| [GRIT: GAN Residuals for Paired Image-to-Image Translation](https://openaccess.thecvf.com/content/WACV2024/html/Suri_GRIT_GAN_Residuals_for_Paired_Image-to-Image_Translation_WACV_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.cs.umd.edu/~sakshams/grit/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Suri_GRIT_GAN_Residuals_for_Paired_Image-to-Image_Translation_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dpQqNejjEy8) |
| [Multimodality-Guided Image Style Transfer using Cross-Modal GAN Inversion](https://openaccess.thecvf.com/content/WACV2024/html/Wang_Multimodality-Guided_Image_Style_Transfer_Using_Cross-Modal_GAN_Inversion_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://hywang66.github.io/mmist/) <br /> [![GitHub](https://img.shields.io/github/stars/hywang66/mmist?style=flat)](https://github.com/hywang66/mmist) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Multimodality-Guided_Image_Style_Transfer_Using_Cross-Modal_GAN_Inversion_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.01671-b31b1b.svg)](https://arxiv.org/abs/2312.01671) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=n8AoUe6zmWc) |
| [ZIGNeRF: Zero-Shot 3D Scene Representation with Invertible Generative Neural Radiance Fields](https://openaccess.thecvf.com/content/WACV2024/html/Ko_ZIGNeRF_Zero-Shot_3D_Scene_Representation_With_Invertible_Generative_Neural_Radiance_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ko_ZIGNeRF_Zero-Shot_3D_Scene_Representation_With_Invertible_Generative_Neural_Radiance_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.02741-b31b1b.svg)](https://arxiv.org/abs/2306.02741) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mMwmddW_gdU) |
| [GraphFill: Deep Image Inpainting using Graphs](https://openaccess.thecvf.com/content/WACV2024/html/Verma_GraphFill_Deep_Image_Inpainting_Using_Graphs_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://shash29-dev.github.io/GraphFill/) <br /> [![GitHub](https://img.shields.io/github/stars/shash29-dev/GraphFill?style=flat)](https://github.com/shash29-dev/GraphFill) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Verma_GraphFill_Deep_Image_Inpainting_Using_Graphs_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=09_zy4YhyDA) |
| [PoseDiff: Pose-Conditioned Multimodal Diffusion Model for Unbounded Scene Synthesis from Sparse Inputs](https://openaccess.thecvf.com/content/WACV2024/html/Lee_PoseDiff_Pose-Conditioned_Multimodal_Diffusion_Model_for_Unbounded_Scene_Synthesis_From_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Lee_PoseDiff_Pose-Conditioned_Multimodal_Diffusion_Model_for_Unbounded_Scene_Synthesis_From_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Nested Diffusion Processes for Anytime Image Generation](https://openaccess.thecvf.com/content/WACV2024/html/Elata_Nested_Diffusion_Processes_for_Anytime_Image_Generation_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://noamelata.github.io/NestedDiffusion/) <br /> [![GitHub](https://img.shields.io/github/stars/noamelata/NestedDiffusion?style=flat)](https://github.com/noamelata/NestedDiffusion) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-demo-FFD21F.svg)](https://huggingface.co/spaces/noamelata/Nested-Diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Elata_Nested_Diffusion_Processes_for_Anytime_Image_Generation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.19066-b31b1b.svg)](https://arxiv.org/abs/2305.19066) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=UaaUOea3MiY) |
| [Expanding Expressiveness of Diffusion Models with Limited Data via Self-Distillation based Fine-Tuning](https://openaccess.thecvf.com/content/WACV2024/html/Hur_Expanding_Expressiveness_of_Diffusion_Models_With_Limited_Data_via_Self-Distillation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Hur_Expanding_Expressiveness_of_Diffusion_Models_With_Limited_Data_via_Self-Distillation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.01018-b31b1b.svg)](https://arxiv.org/abs/2311.01018) | :heavy_minus_sign: |
| [One Style is All You Need to Generate a Video](https://openaccess.thecvf.com/content/WACV2024/html/Manandhar_One_Style_Is_All_You_Need_To_Generate_a_Video_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sandman002.github.io/CTSVG/) <br /> [![GitHub](https://img.shields.io/github/stars/sandman002/One-Style-is-All-You-Need-to-Generate-a-Video?style=flat)](https://github.com/sandman002/One-Style-is-All-You-Need-to-Generate-a-Video) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Manandhar_One_Style_Is_All_You_Need_To_Generate_a_Video_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.17835-b31b1b.svg)](https://arxiv.org/abs/2310.17835) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jr3h0bT05oY) |
| [Consistent Multimodal Generation via a Unified GAN Framework](https://openaccess.thecvf.com/content/WACV2024/html/Zhu_Consistent_Multimodal_Generation_via_a_Unified_GAN_Framework_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/jessemelpolio/MultimodalGAN?style=flat)](https://github.com/jessemelpolio/MultimodalGAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zhu_Consistent_Multimodal_Generation_via_a_Unified_GAN_Framework_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.01425-b31b1b.svg)](https://arxiv.org/abs/2307.01425) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=e9U79ePh0Wo) |
| [Unsupervised Co-Generation of Foreground-Background Segmentation from Text-to-Image Synthesis](https://openaccess.thecvf.com/content/WACV2024/html/Ahmed_Unsupervised_Co-Generation_of_Foreground-Background_Segmentation_From_Text-to-Image_Synthesis_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ahmed_Unsupervised_Co-Generation_of_Foreground-Background_Segmentation_From_Text-to-Image_Synthesis_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JiiPVhVLowE) |
| [MotionGPT: Human Motion Synthesis with Improved Diversity and Realism via GPT-3 Prompting](https://openaccess.thecvf.com/content/WACV2024/html/Ribeiro-Gomes_MotionGPT_Human_Motion_Synthesis_With_Improved_Diversity_and_Realism_via_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ribeiro-Gomes_MotionGPT_Human_Motion_Synthesis_With_Improved_Diversity_and_Realism_via_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Y5OyFSL8-Gs) |
| [Human Motion Aware Text-to-Video Generation with Explicit Camera Control](https://openaccess.thecvf.com/content/WACV2024/html/Kim_Human_Motion_Aware_Text-to-Video_Generation_With_Explicit_Camera_Control_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://csjasper.github.io/HMTV/) <br /> [![GitHub](https://img.shields.io/github/stars/CSJasper/HMTV?style=flat)](https://github.com/CSJasper/HMTV) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Kim_Human_Motion_Aware_Text-to-Video_Generation_With_Explicit_Camera_Control_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=IK_hqok5mJg) |
| [Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation](https://openaccess.thecvf.com/content/WACV2024/html/Stypulkowski_Diffused_Heads_Diffusion_Models_Beat_GANs_on_Talking-Face_Generation_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mstypulkowski.github.io/diffusedheads/) <br /> [![GitHub](https://img.shields.io/github/stars/MStypulkowski/diffused-heads?style=flat)](https://github.com/MStypulkowski/diffused-heads) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Stypulkowski_Diffused_Heads_Diffusion_Models_Beat_GANs_on_Talking-Face_Generation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.03396-b31b1b.svg)](https://arxiv.org/abs/2301.03396) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=BhGq-9d1Ahs) |
| [Second-Order Graph ODEs for Multi-Agent Trajectory Forecasting](https://openaccess.thecvf.com/content/WACV2024/html/Wen_Second-Order_Graph_ODEs_for_Multi-Agent_Trajectory_Forecasting_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wen_Second-Order_Graph_ODEs_for_Multi-Agent_Trajectory_Forecasting_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=C_t4UeEIURg) |
| [Unified Concept Editing in Diffusion Models](https://openaccess.thecvf.com/content/WACV2024/html/Gandikota_Unified_Concept_Editing_in_Diffusion_Models_WACV_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://unified.baulab.info/) <br /> [![GitHub](https://img.shields.io/github/stars/rohitgandikota/unified-concept-editing?style=flat)](https://github.com/rohitgandikota/unified-concept-editing) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Gandikota_Unified_Concept_Editing_in_Diffusion_Models_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.14761-b31b1b.svg)](https://arxiv.org/abs/2308.14761) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KoDBaw60qMI) |
| [SpectralCLIP: Preventing Artifacts in Text-Guided Style Transfer from a Spectral Perspective](https://openaccess.thecvf.com/content/WACV2024/html/Xu_SpectralCLIP_Preventing_Artifacts_in_Text-Guided_Style_Transfer_From_a_Spectral_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/zipengxuc/SpectralCLIP?style=flat)](https://github.com/zipengxuc/SpectralCLIP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Xu_SpectralCLIP_Preventing_Artifacts_in_Text-Guided_Style_Transfer_From_a_Spectral_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09270-b31b1b.svg)](https://arxiv.org/abs/2303.09270) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rzh8T19vK-U) |
| [Diffusion-based Generation of Histopathological whole Slide Images at a Gigapixel Scale](https://openaccess.thecvf.com/content/WACV2024/html/Harb_Diffusion-Based_Generation_of_Histopathological_Whole_Slide_Images_at_a_Gigapixel_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Harb_Diffusion-Based_Generation_of_Histopathological_Whole_Slide_Images_at_a_Gigapixel_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.08199-b31b1b.svg)](https://arxiv.org/abs/2311.08199) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=b6HtHfIOJWg) |
| [Painterly Image Harmonization via Adversarial Residual Learning](https://openaccess.thecvf.com/content/WACV2024/html/Wang_Painterly_Image_Harmonization_via_Adversarial_Residual_Learning_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Painterly_Image_Harmonization_via_Adversarial_Residual_Learning_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.08646-b31b1b.svg)](https://arxiv.org/abs/2311.08646) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=IZrCsDuUHWc) |
| [Training-Free Content Injection using H-Space in Diffusion Models](https://openaccess.thecvf.com/content/WACV2024/html/Jeong_Training-Free_Content_Injection_Using_H-Space_in_Diffusion_Models_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://curryjung.github.io/InjectFusion/) <br /> [![GitHub](https://img.shields.io/github/stars/curryjung/InjectFusion_official?style=flat)](https://github.com/curryjung/InjectFusion_official) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Jeong_Training-Free_Content_Injection_Using_H-Space_in_Diffusion_Models_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15403-b31b1b.svg)](https://arxiv.org/abs/2303.15403) | :heavy_minus_sign: |
| [ENTED: Enhanced Neural Texture Extraction and Distribution for Reference-based Blind Face Restoration](https://openaccess.thecvf.com/content/WACV2024/html/Lau_ENTED_Enhanced_Neural_Texture_Extraction_and_Distribution_for_Reference-Based_Blind_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Lau_ENTED_Enhanced_Neural_Texture_Extraction_and_Distribution_for_Reference-Based_Blind_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.06978-b31b1b.svg)](https://arxiv.org/abs/2401.06978) | :heavy_minus_sign: |
| [PIDiffu: Pixel-Aligned Diffusion Model for High-Fidelity Clothed Human Reconstruction](https://openaccess.thecvf.com/content/WACV2024/html/Lee_PIDiffu_Pixel-Aligned_Diffusion_Model_for_High-Fidelity_Clothed_Human_Reconstruction_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Lee_PIDiffu_Pixel-Aligned_Diffusion_Model_for_High-Fidelity_Clothed_Human_Reconstruction_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JAVmEthOeJk) |
| [PathLDM: Text Conditioned Latent Diffusion Model for Histopathology](https://openaccess.thecvf.com/content/WACV2024/html/Yellapragada_PathLDM_Text_Conditioned_Latent_Diffusion_Model_for_Histopathology_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/cvlab-stonybrook/PathLDM?style=flat)](https://github.com/cvlab-stonybrook/PathLDM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Yellapragada_PathLDM_Text_Conditioned_Latent_Diffusion_Model_for_Histopathology_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.00748-b31b1b.svg)](https://arxiv.org/abs/2309.00748) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_wHyqLGaqfg) |
| [Content-Aware Image Color Editing with Auxiliary Color Restoration Tasks](https://openaccess.thecvf.com/content/WACV2024/html/Ren_Content-Aware_Image_Color_Editing_With_Auxiliary_Color_Restoration_Tasks_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ren_Content-Aware_Image_Color_Editing_With_Auxiliary_Color_Restoration_Tasks_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=W0g0tyyzNp8) |
| [On Manipulating Scene Text in the Wild with Diffusion Models](https://openaccess.thecvf.com/content/WACV2024/html/Santoso_On_Manipulating_Scene_Text_in_the_Wild_With_Diffusion_Models_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Santoso_On_Manipulating_Scene_Text_in_the_Wild_With_Diffusion_Models_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.00734-b31b1b.svg)](https://arxiv.org/abs/2311.00734) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=O-vz6GuFG-0) |
| [CXR-IRGen: An Integrated Vision and Language Model for the Generation of Clinically Accurate Chest X-Ray Image-Report Pairs](https://openaccess.thecvf.com/content/WACV2024/html/Shentu_CXR-IRGen_An_Integrated_Vision_and_Language_Model_for_the_Generation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/junjie-shentu/CXR-IRGen?style=flat)](https://github.com/junjie-shentu/CXR-IRGen) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Shentu_CXR-IRGen_An_Integrated_Vision_and_Language_Model_for_the_Generation_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Face Identity-Aware Disentanglement in StyleGAN](https://openaccess.thecvf.com/content/WACV2024/html/Suwala_Face_Identity-Aware_Disentanglement_in_StyleGAN_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Suwala_Face_Identity-Aware_Disentanglement_in_StyleGAN_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.12033-b31b1b.svg)](https://arxiv.org/abs/2309.12033) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FQ5rVG1FuBA) |
| [Text-to-Image Editing by Image Information Removal](https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Text-to-Image_Editing_by_Image_Information_Removal_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Text-to-Image_Editing_by_Image_Information_Removal_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.17489-b31b1b.svg)](https://arxiv.org/abs/2305.17489) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rf0XjGVE5VA) |
| [Preserving Image Properties through Initializations in Diffusion Models](https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Preserving_Image_Properties_Through_Initializations_in_Diffusion_Models_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Preserving_Image_Properties_Through_Initializations_in_Diffusion_Models_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.02097-b31b1b.svg)](https://arxiv.org/abs/2401.02097) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=j7nL_Kn15js) |
| [GC-VTON: Predicting Globally Consistent and Occlusion Aware Local Flows with Neighborhood Integrity Preservation for Virtual Try-On](https://openaccess.thecvf.com/content/WACV2024/html/Rawal_GC-VTON_Predicting_Globally_Consistent_and_Occlusion_Aware_Local_Flows_With_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Rawal_GC-VTON_Predicting_Globally_Consistent_and_Occlusion_Aware_Local_Flows_With_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.04932-b31b1b.svg)](https://arxiv.org/abs/2311.04932) | :heavy_minus_sign: |
| [Generation of Upright Panoramic Image from Non-Upright Panoramic Image](https://openaccess.thecvf.com/content/WACV2024/html/Liu_Generation_of_Upright_Panoramic_Image_From_Non-Upright_Panoramic_Image_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/53jingguo/Generation-of-Upright-Panoramic-Image-from-Non-upright-Panoramic-Image?style=flat)](https://github.com/53jingguo/Generation-of-Upright-Panoramic-Image-from-Non-upright-Panoramic-Image) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Generation_of_Upright_Panoramic_Image_From_Non-Upright_Panoramic_Image_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=RU4cPC0k2K4) |
| [Fast Diffusion EM: A Diffusion Model for Blind Inverse Problems with Application to Deconvolution](https://openaccess.thecvf.com/content/WACV2024/html/Laroche_Fast_Diffusion_EM_A_Diffusion_Model_for_Blind_Inverse_Problems_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/claroche-r/FastDiffusionEM?style=flat)](https://github.com/claroche-r/FastDiffusionEM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Laroche_Fast_Diffusion_EM_A_Diffusion_Model_for_Blind_Inverse_Problems_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.00287-b31b1b.svg)](https://arxiv.org/abs/2309.00287) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=z1xIat4vpj8) |
| [Enforcing Sparsity on Latent Space for Robust and Explainable Representations](https://openaccess.thecvf.com/content/WACV2024/html/Li_Enforcing_Sparsity_on_Latent_Space_for_Robust_and_Explainable_Representations_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Li_Enforcing_Sparsity_on_Latent_Space_for_Robust_and_Explainable_Representations_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KxnHhf-8Fwg) |
| [Diff2Lip: Audio Conditioned Diffusion Models for Lip-Synchronization](https://openaccess.thecvf.com/content/WACV2024/html/Mukhopadhyay_Diff2Lip_Audio_Conditioned_Diffusion_Models_for_Lip-Synchronization_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://soumik-kanad.github.io/diff2lip/) <br /> [![GitHub](https://img.shields.io/github/stars/soumik-kanad/diff2lip?style=flat)](https://github.com/soumik-kanad/diff2lip) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Mukhopadhyay_Diff2Lip_Audio_Conditioned_Diffusion_Models_for_Lip-Synchronization_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09716-b31b1b.svg)](https://arxiv.org/abs/2308.09716) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CNX7aJNUw54) |
| [3D Reconstruction of Interacting Multi-Person in Clothing from a Single Image](https://openaccess.thecvf.com/content/WACV2024/html/Cha_3D_Reconstruction_of_Interacting_Multi-Person_in_Clothing_From_a_Single_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Cha_3D_Reconstruction_of_Interacting_Multi-Person_in_Clothing_From_a_Single_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.06415-b31b1b.svg)](https://arxiv.org/abs/2401.06415) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=E1Y2EW9oTM0) |
| [Revisiting Latent Space of GAN Inversion for Robust Real Image Editing](https://openaccess.thecvf.com/content/WACV2024/html/Katsumata_Revisiting_Latent_Space_of_GAN_Inversion_for_Robust_Real_Image_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/raven38/hypershpere-gan-inversion?style=flat)](https://github.com/raven38/hypershpere-gan-inversion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Katsumata_Revisiting_Latent_Space_of_GAN_Inversion_for_Robust_Real_Image_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=36hLx1CtKr4) |
| [Soft Curriculum for Learning Conditional GANs with Noisy-Labeled and Uncurated Unlabeled Data](https://openaccess.thecvf.com/content/WACV2024/html/Katsumata_Soft_Curriculum_for_Learning_Conditional_GANs_With_Noisy-Labeled_and_Uncurated_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/raven38/NOSSGAN?style=flat)](https://github.com/raven38/NOSSGAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Katsumata_Soft_Curriculum_for_Learning_Conditional_GANs_With_Noisy-Labeled_and_Uncurated_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08319-b31b1b.svg)](https://arxiv.org/abs/2307.08319) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7bmI0_kAAkw) |
| [Bipartite Graph Diffusion Model for Human Interaction Generation](https://openaccess.thecvf.com/content/WACV2024/html/Chopin_Bipartite_Graph_Diffusion_Model_for_Human_Interaction_Generation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/CRISTAL-3DSAM/BiGraphDiff?style=flat)](https://github.com/CRISTAL-3DSAM/BiGraphDiff) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Chopin_Bipartite_Graph_Diffusion_Model_for_Human_Interaction_Generation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.10134-b31b1b.svg)](https://arxiv.org/abs/2301.10134) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CkNckDbiDsI) |
| [Training-Free Layout Control with Cross-Attention Guidance](https://openaccess.thecvf.com/content/WACV2024/html/Chen_Training-Free_Layout_Control_With_Cross-Attention_Guidance_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://silent-chen.github.io/layout-guidance/) <br /> [![GitHub](https://img.shields.io/github/stars/nipunjindal/diffusers-layout-guidance?style=flat)](https://github.com/nipunjindal/diffusers-layout-guidance) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Training-Free_Layout_Control_With_Cross-Attention_Guidance_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03373-b31b1b.svg)](https://arxiv.org/abs/2304.03373) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FeL0JqEvM-0) |
| [Controllable Image Synthesis of Industrial Data using Stable Diffusion](https://openaccess.thecvf.com/content/WACV2024/html/Valvano_Controllable_Image_Synthesis_of_Industrial_Data_Using_Stable_Diffusion_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Valvano_Controllable_Image_Synthesis_of_Industrial_Data_Using_Stable_Diffusion_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.03152-b31b1b.svg)](https://arxiv.org/abs/2401.03152) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=GHmaoFyZNzQ) |
| [Removing the Quality Tax in Controllable Face Generation](https://openaccess.thecvf.com/content/WACV2024/html/Huang_Removing_the_Quality_Tax_in_Controllable_Face_Generation_WACV_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://visual.cs.brown.edu/projects/taxfreegan-webpage/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Huang_Removing_the_Quality_Tax_in_Controllable_Face_Generation_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=h7iTZqE-G0k) |
| [Hierarchical Diffusion Autoencoders and Disentangled Image Manipulation](https://openaccess.thecvf.com/content/WACV2024/html/Lu_Hierarchical_Diffusion_Autoencoders_and_Disentangled_Image_Manipulation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Lu_Hierarchical_Diffusion_Autoencoders_and_Disentangled_Image_Manipulation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.11829-b31b1b.svg)](https://arxiv.org/abs/2304.11829) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mitnigVpaNQ) |
| [FacadeNet: Conditional Facade Synthesis via Selective Editing](https://openaccess.thecvf.com/content/WACV2024/html/Georgiou_FacadeNet_Conditional_Facade_Synthesis_via_Selective_Editing_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ygeorg01.github.io/FacadeNet/) <br /> [![GitHub](https://img.shields.io/github/stars/ygeorg01/FacadeNet?style=flat)](https://github.com/ygeorg01/FacadeNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Georgiou_FacadeNet_Conditional_Facade_Synthesis_via_Selective_Editing_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.01240-b31b1b.svg)](https://arxiv.org/abs/2311.01240) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=eZ891uazc38) |
| [TPSeNCE: Towards Artifact-Free Realistic Rain Generation for Deraining and Object Detection in Rain](https://openaccess.thecvf.com/content/WACV2024/html/Zheng_TPSeNCE_Towards_Artifact-Free_Realistic_Rain_Generation_for_Deraining_and_Object_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/ShenZheng2000/TPSeNCE?style=flat)](https://github.com/ShenZheng2000/TPSeNCE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zheng_TPSeNCE_Towards_Artifact-Free_Realistic_Rain_Generation_for_Deraining_and_Object_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.00660-b31b1b.svg)](https://arxiv.org/abs/2311.00660) | :heavy_minus_sign: |
| [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://openaccess.thecvf.com/content/WACV2024/html/Lin_Common_Diffusion_Noise_Schedules_and_Sample_Steps_Are_Flawed_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Lin_Common_Diffusion_Noise_Schedules_and_Sample_Steps_Are_Flawed_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.08891-b31b1b.svg)](https://arxiv.org/abs/2305.08891) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PXmNdDZ2JxM) |
| [Improving the Leaking of Augmentations in Data-Efficient GANs via Adaptive Negative Data Augmentation](https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Improving_the_Leaking_of_Augmentations_in_Data-Efficient_GANs_via_Adaptive_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/zzhang05/ANDA?style=flat)](https://github.com/zzhang05/ANDA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Improving_the_Leaking_of_Augmentations_in_Data-Efficient_GANs_via_Adaptive_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [P2D: Plug and Play Discriminator for Accelerating GAN Frameworks](https://openaccess.thecvf.com/content/WACV2024/html/Chong_P2D_Plug_and_Play_Discriminator_for_Accelerating_GAN_Frameworks_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Chong_P2D_Plug_and_Play_Discriminator_for_Accelerating_GAN_Frameworks_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=1vEVanV4Czk) |
| [Personalized Face Inpainting with Diffusion Models by Parallel Visual Attention](https://openaccess.thecvf.com/content/WACV2024/html/Xu_Personalized_Face_Inpainting_With_Diffusion_Models_by_Parallel_Visual_Attention_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Xu_Personalized_Face_Inpainting_With_Diffusion_Models_by_Parallel_Visual_Attention_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.03556-b31b1b.svg)](https://arxiv.org/abs/2312.03556) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CJ6ap4hiyDM) |
| [Semantic Generative Augmentations for Few-Shot Counting](https://openaccess.thecvf.com/content/WACV2024/html/Doubinsky_Semantic_Generative_Augmentations_for_Few-Shot_Counting_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Doubinsky_Semantic_Generative_Augmentations_for_Few-Shot_Counting_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.16122-b31b1b.svg)](https://arxiv.org/abs/2311.16122) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6coxHpfxxDA) |
| [StyleGAN-Fusion: Diffusion Guided Domain Adaptation of Image Generators](https://openaccess.thecvf.com/content/WACV2024/html/Song_StyleGAN-Fusion_Diffusion_Guided_Domain_Adaptation_of_Image_Generators_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Song_StyleGAN-Fusion_Diffusion_Guided_Domain_Adaptation_of_Image_Generators_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qSlC6MT_q-E) |
