# WACV-2024-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/explainable_fair_accountable-privacy-preserving.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/vision_language_and_other_modalities.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Generative Models for Image, Video, 3D, etc

![Section Papers](https://img.shields.io/badge/Section%20Papers-14-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-10-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-8-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-2-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| [Spiking Denoising Diffusion Probabilistic Models](https://openaccess.thecvf.com/content/WACV2024/html/Cao_Spiking_Denoising_Diffusion_Probabilistic_Models_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/AndyCao1125/SDDPM?style=flat)](https://github.com/AndyCao1125/SDDPM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Cao_Spiking_Denoising_Diffusion_Probabilistic_Models_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.17046-b31b1b.svg)](https://arxiv.org/abs/2306.17046) | :heavy_minus_sign: |
| [Improving the Effectiveness of Deep Generative Data](https://openaccess.thecvf.com/content/WACV2024/html/Wang_Improving_the_Effectiveness_of_Deep_Generative_Data_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Improving_the_Effectiveness_of_Deep_Generative_Data_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.03959-b31b1b.svg)](https://arxiv.org/abs/2311.03959) | :heavy_minus_sign: |
| [Customizing 360-Degree Panoramas through Text-to-Image Diffusion Models](https://openaccess.thecvf.com/content/WACV2024/html/Wang_Customizing_360-Degree_Panoramas_Through_Text-to-Image_Diffusion_Models_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://littlewhitesea.github.io/stitchdiffusion.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/littlewhitesea/StitchDiffusion?style=flat)](https://github.com/littlewhitesea/StitchDiffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Customizing_360-Degree_Panoramas_Through_Text-to-Image_Diffusion_Models_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.18840-b31b1b.svg)](https://arxiv.org/abs/2310.18840) | :heavy_minus_sign: |
| [Label Augmentation as Inter-Class Data Augmentation for Conditional Image Synthesis with Imbalanced Data](https://openaccess.thecvf.com/content/WACV2024/html/Katsumata_Label_Augmentation_As_Inter-Class_Data_Augmentation_for_Conditional_Image_Synthesis_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/raven38/softlabel-gan?style=flat)](https://github.com/raven38/softlabel-gan) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Katsumata_Label_Augmentation_As_Inter-Class_Data_Augmentation_for_Conditional_Image_Synthesis_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [3D-Aware Talking-Head Video Motion Transfer](https://openaccess.thecvf.com/content/WACV2024/html/Ni_3D-Aware_Talking-Head_Video_Motion_Transfer_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ni_3D-Aware_Talking-Head_Video_Motion_Transfer_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.02549-b31b1b.svg)](https://arxiv.org/abs/2311.02549) | :heavy_minus_sign: |
| [GRIT: GAN Residuals for Paired Image-to-Image Translation](https://openaccess.thecvf.com/content/WACV2024/html/Suri_GRIT_GAN_Residuals_for_Paired_Image-to-Image_Translation_WACV_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.cs.umd.edu/~sakshams/grit/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Suri_GRIT_GAN_Residuals_for_Paired_Image-to-Image_Translation_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Multimodality-Guided Image Style Transfer using Cross-Modal GAN Inversion](https://openaccess.thecvf.com/content/WACV2024/html/Wang_Multimodality-Guided_Image_Style_Transfer_Using_Cross-Modal_GAN_Inversion_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://hywang66.github.io/mmist/) <br /> [![GitHub](https://img.shields.io/github/stars/hywang66/mmist?style=flat)](https://github.com/hywang66/mmist) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Multimodality-Guided_Image_Style_Transfer_Using_Cross-Modal_GAN_Inversion_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.01671-b31b1b.svg)](https://arxiv.org/abs/2312.01671) | :heavy_minus_sign: |
| [ZIGNeRF: Zero-Shot 3D Scene Representation with Invertible Generative Neural Radiance Fields](https://openaccess.thecvf.com/content/WACV2024/html/Ko_ZIGNeRF_Zero-Shot_3D_Scene_Representation_With_Invertible_Generative_Neural_Radiance_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ko_ZIGNeRF_Zero-Shot_3D_Scene_Representation_With_Invertible_Generative_Neural_Radiance_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.02741-b31b1b.svg)](https://arxiv.org/abs/2306.02741) | :heavy_minus_sign: |
| [GraphFill: Deep Image Inpainting using Graphs](https://openaccess.thecvf.com/content/WACV2024/html/Verma_GraphFill_Deep_Image_Inpainting_Using_Graphs_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://shash29-dev.github.io/GraphFill/) <br /> [![GitHub](https://img.shields.io/github/stars/shash29-dev/GraphFill?style=flat)](https://github.com/shash29-dev/GraphFill) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Verma_GraphFill_Deep_Image_Inpainting_Using_Graphs_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vT1mfen7Kgw) |
| [PoseDiff: Pose-Conditioned Multimodal Diffusion Model for Unbounded Scene Synthesis from Sparse Inputs](https://openaccess.thecvf.com/content/WACV2024/html/Lee_PoseDiff_Pose-Conditioned_Multimodal_Diffusion_Model_for_Unbounded_Scene_Synthesis_From_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Lee_PoseDiff_Pose-Conditioned_Multimodal_Diffusion_Model_for_Unbounded_Scene_Synthesis_From_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Nested Diffusion Processes for Anytime Image Generation](https://openaccess.thecvf.com/content/WACV2024/html/Elata_Nested_Diffusion_Processes_for_Anytime_Image_Generation_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://noamelata.github.io/NestedDiffusion/) <br /> [![GitHub](https://img.shields.io/github/stars/noamelata/NestedDiffusion?style=flat)](https://github.com/noamelata/NestedDiffusion) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-demo-FFD21F.svg)](https://huggingface.co/spaces/noamelata/Nested-Diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Elata_Nested_Diffusion_Processes_for_Anytime_Image_Generation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.19066-b31b1b.svg)](https://arxiv.org/abs/2305.19066) | :heavy_minus_sign: |
| [Expanding Expressiveness of Diffusion Models with Limited Data via Self-Distillation based Fine-Tuning](https://openaccess.thecvf.com/content/WACV2024/html/Hur_Expanding_Expressiveness_of_Diffusion_Models_With_Limited_Data_via_Self-Distillation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Hur_Expanding_Expressiveness_of_Diffusion_Models_With_Limited_Data_via_Self-Distillation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.01018-b31b1b.svg)](https://arxiv.org/abs/2311.01018) | :heavy_minus_sign: |
| [One Style is All You Need to Generate a Video](https://openaccess.thecvf.com/content/WACV2024/html/Manandhar_One_Style_Is_All_You_Need_To_Generate_a_Video_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sandman002.github.io/CTSVG/) <br /> [![GitHub](https://img.shields.io/github/stars/sandman002/One-Style-is-All-You-Need-to-Generate-a-Video?style=flat)](https://github.com/sandman002/One-Style-is-All-You-Need-to-Generate-a-Video) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Manandhar_One_Style_Is_All_You_Need_To_Generate_a_Video_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.17835-b31b1b.svg)](https://arxiv.org/abs/2310.17835) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bsMazroFH6k) |
| [Consistent Multimodal Generation via a Unified GAN Framework](https://openaccess.thecvf.com/content/WACV2024/html/Zhu_Consistent_Multimodal_Generation_via_a_Unified_GAN_Framework_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/jessemelpolio/MultimodalGAN?style=flat)](https://github.com/jessemelpolio/MultimodalGAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zhu_Consistent_Multimodal_Generation_via_a_Unified_GAN_Framework_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.01425-b31b1b.svg)](https://arxiv.org/abs/2307.01425) | :heavy_minus_sign: |
| [Unsupervised Co-Generation of Foreground-Background Segmentation from Text-to-Image Synthesis](https://openaccess.thecvf.com/content/WACV2024/html/Ahmed_Unsupervised_Co-Generation_of_Foreground-Background_Segmentation_From_Text-to-Image_Synthesis_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ahmed_Unsupervised_Co-Generation_of_Foreground-Background_Segmentation_From_Text-to-Image_Synthesis_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [MotionGPT: Human Motion Synthesis with Improved Diversity and Realism via GPT-3 Prompting](https://openaccess.thecvf.com/content/WACV2024/html/Ribeiro-Gomes_MotionGPT_Human_Motion_Synthesis_With_Improved_Diversity_and_Realism_via_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ribeiro-Gomes_MotionGPT_Human_Motion_Synthesis_With_Improved_Diversity_and_Realism_via_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Human Motion Aware Text-to-Video Generation with Explicit Camera Control](https://openaccess.thecvf.com/content/WACV2024/html/Kim_Human_Motion_Aware_Text-to-Video_Generation_With_Explicit_Camera_Control_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://csjasper.github.io/HMTV/) <br /> [![GitHub](https://img.shields.io/github/stars/CSJasper/HMTV?style=flat)](https://github.com/CSJasper/HMTV) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Kim_Human_Motion_Aware_Text-to-Video_Generation_With_Explicit_Camera_Control_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation](https://openaccess.thecvf.com/content/WACV2024/html/Stypulkowski_Diffused_Heads_Diffusion_Models_Beat_GANs_on_Talking-Face_Generation_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mstypulkowski.github.io/diffusedheads/) <br /> [![GitHub](https://img.shields.io/github/stars/MStypulkowski/diffused-heads?style=flat)](https://github.com/MStypulkowski/diffused-heads) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Stypulkowski_Diffused_Heads_Diffusion_Models_Beat_GANs_on_Talking-Face_Generation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.03396-b31b1b.svg)](https://arxiv.org/abs/2301.03396) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DSipIDj-5q0) |
| [Second-Order Graph ODEs for Multi-Agent Trajectory Forecasting](https://openaccess.thecvf.com/content/WACV2024/html/Wen_Second-Order_Graph_ODEs_for_Multi-Agent_Trajectory_Forecasting_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wen_Second-Order_Graph_ODEs_for_Multi-Agent_Trajectory_Forecasting_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Unified Concept Editing in Diffusion Models](https://openaccess.thecvf.com/content/WACV2024/html/Gandikota_Unified_Concept_Editing_in_Diffusion_Models_WACV_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://unified.baulab.info/) <br /> [![GitHub](https://img.shields.io/github/stars/rohitgandikota/unified-concept-editing?style=flat)](https://github.com/rohitgandikota/unified-concept-editing) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Gandikota_Unified_Concept_Editing_in_Diffusion_Models_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.14761-b31b1b.svg)](https://arxiv.org/abs/2308.14761) | :heavy_minus_sign: |
| [SpectralCLIP: Preventing Artifacts in Text-Guided Style Transfer from a Spectral Perspective](https://openaccess.thecvf.com/content/WACV2024/html/Xu_SpectralCLIP_Preventing_Artifacts_in_Text-Guided_Style_Transfer_From_a_Spectral_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/zipengxuc/SpectralCLIP?style=flat)](https://github.com/zipengxuc/SpectralCLIP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Xu_SpectralCLIP_Preventing_Artifacts_in_Text-Guided_Style_Transfer_From_a_Spectral_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09270-b31b1b.svg)](https://arxiv.org/abs/2303.09270) | :heavy_minus_sign: |
| [Diffusion-based Generation of Histopathological whole Slide Images at a Gigapixel Scale](https://openaccess.thecvf.com/content/WACV2024/html/Harb_Diffusion-Based_Generation_of_Histopathological_Whole_Slide_Images_at_a_Gigapixel_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Harb_Diffusion-Based_Generation_of_Histopathological_Whole_Slide_Images_at_a_Gigapixel_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.08199-b31b1b.svg)](https://arxiv.org/abs/2311.08199) | :heavy_minus_sign: |
| [Painterly Image Harmonization via Adversarial Residual Learning](https://openaccess.thecvf.com/content/WACV2024/html/Wang_Painterly_Image_Harmonization_via_Adversarial_Residual_Learning_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Painterly_Image_Harmonization_via_Adversarial_Residual_Learning_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.08646-b31b1b.svg)](https://arxiv.org/abs/2311.08646) | :heavy_minus_sign: |
| [Training-Free Content Injection using H-Space in Diffusion Models](https://openaccess.thecvf.com/content/WACV2024/html/Jeong_Training-Free_Content_Injection_Using_H-Space_in_Diffusion_Models_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://curryjung.github.io/InjectFusion/) <br /> [![GitHub](https://img.shields.io/github/stars/curryjung/InjectFusion_official?style=flat)](https://github.com/curryjung/InjectFusion_official) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Jeong_Training-Free_Content_Injection_Using_H-Space_in_Diffusion_Models_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15403-b31b1b.svg)](https://arxiv.org/abs/2303.15403) | :heavy_minus_sign: |
| [ENTED: Enhanced Neural Texture Extraction and Distribution for Reference-based Blind Face Restoration](https://openaccess.thecvf.com/content/WACV2024/html/Lau_ENTED_Enhanced_Neural_Texture_Extraction_and_Distribution_for_Reference-Based_Blind_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Lau_ENTED_Enhanced_Neural_Texture_Extraction_and_Distribution_for_Reference-Based_Blind_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.06978-b31b1b.svg)](https://arxiv.org/abs/2401.06978) | :heavy_minus_sign: |
| [PIDiffu: Pixel-Aligned Diffusion Model for High-Fidelity Clothed Human Reconstruction](https://openaccess.thecvf.com/content/WACV2024/html/Lee_PIDiffu_Pixel-Aligned_Diffusion_Model_for_High-Fidelity_Clothed_Human_Reconstruction_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Lee_PIDiffu_Pixel-Aligned_Diffusion_Model_for_High-Fidelity_Clothed_Human_Reconstruction_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [PathLDM: Text Conditioned Latent Diffusion Model for Histopathology](https://openaccess.thecvf.com/content/WACV2024/html/Yellapragada_PathLDM_Text_Conditioned_Latent_Diffusion_Model_for_Histopathology_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/cvlab-stonybrook/PathLDM?style=flat)](https://github.com/cvlab-stonybrook/PathLDM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Yellapragada_PathLDM_Text_Conditioned_Latent_Diffusion_Model_for_Histopathology_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.00748-b31b1b.svg)](https://arxiv.org/abs/2309.00748) | :heavy_minus_sign: |
| [Content-Aware Image Color Editing with Auxiliary Color Restoration Tasks](https://openaccess.thecvf.com/content/WACV2024/html/Ren_Content-Aware_Image_Color_Editing_With_Auxiliary_Color_Restoration_Tasks_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ren_Content-Aware_Image_Color_Editing_With_Auxiliary_Color_Restoration_Tasks_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [On Manipulating Scene Text in the Wild with Diffusion Models](https://openaccess.thecvf.com/content/WACV2024/html/Santoso_On_Manipulating_Scene_Text_in_the_Wild_With_Diffusion_Models_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Santoso_On_Manipulating_Scene_Text_in_the_Wild_With_Diffusion_Models_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.00734-b31b1b.svg)](https://arxiv.org/abs/2311.00734) | :heavy_minus_sign: |
| [CXR-IRGen: An Integrated Vision and Language Model for the Generation of Clinically Accurate Chest X-Ray Image-Report Pairs](https://openaccess.thecvf.com/content/WACV2024/html/Shentu_CXR-IRGen_An_Integrated_Vision_and_Language_Model_for_the_Generation_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/junjie-shentu/CXR-IRGen?style=flat)](https://github.com/junjie-shentu/CXR-IRGen) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Shentu_CXR-IRGen_An_Integrated_Vision_and_Language_Model_for_the_Generation_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Face Identity-Aware Disentanglement in StyleGAN](https://openaccess.thecvf.com/content/WACV2024/html/Suwala_Face_Identity-Aware_Disentanglement_in_StyleGAN_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Suwala_Face_Identity-Aware_Disentanglement_in_StyleGAN_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.12033-b31b1b.svg)](https://arxiv.org/abs/2309.12033) | :heavy_minus_sign: |
| [Text-to-Image Editing by Image Information Removal](https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Text-to-Image_Editing_by_Image_Information_Removal_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Text-to-Image_Editing_by_Image_Information_Removal_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.17489-b31b1b.svg)](https://arxiv.org/abs/2305.17489) | :heavy_minus_sign: |
| [Preserving Image Properties through Initializations in Diffusion Models](https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Preserving_Image_Properties_Through_Initializations_in_Diffusion_Models_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Preserving_Image_Properties_Through_Initializations_in_Diffusion_Models_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.02097-b31b1b.svg)](https://arxiv.org/abs/2401.02097) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VUoGc-Pjo-0) |
| [GC-VTON: Predicting Globally Consistent and Occlusion Aware Local Flows with Neighborhood Integrity Preservation for Virtual Try-On](https://openaccess.thecvf.com/content/WACV2024/html/Rawal_GC-VTON_Predicting_Globally_Consistent_and_Occlusion_Aware_Local_Flows_With_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Rawal_GC-VTON_Predicting_Globally_Consistent_and_Occlusion_Aware_Local_Flows_With_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.04932-b31b1b.svg)](https://arxiv.org/abs/2311.04932) | :heavy_minus_sign: |
| Generation of Upright Panoramic Image from Non-Upright Panoramic Image |  |  |  |
| Fast Diffusion EM: A Diffusion Model for Blind Inverse Problems with Application to Deconvolution |  |  |  |
| Enforcing Sparsity on Latent Space for Robust and Explainable Representations |  |  |  |
| Diff2Lip: Audio Conditioned Diffusion Models for Lip-Synchronization |  |  |  |
| 3D Reconstruction of Interacting Multi-Person in Clothing from a Single Image |  |  |  |
| Revisiting Latent Space of GAN Inversion for Robust Real Image Editing |  |  |  |
| Soft Curriculum for Learning Conditional GANs with Noisy-Labeled and Uncurated Unlabeled Data |  |  |  |
| Bipartite Graph Diffusion Model for Human Interaction Generation |  |  |  |
| Training-Free Layout Control with Cross-Attention Guidance |  |  |  |
| Controllable Image Synthesis of Industrial Data using Stable Diffusion |  |  |  |
| Removing the Quality Tax in Controllable Face Generation |  |  |  |
| Hierarchical Diffusion Autoencoders and Disentangled Image Manipulation |  |  |  |
| FacadeNet: Conditional Facade Synthesis via Selective Editing |  |  |  |
| TPSeNCE: Towards Artifact-Free Realistic Rain Generation for Deraining and Object Detection in Rain |  |  |  |
| Common Diffusion Noise Schedules and Sample Steps are Flawed |  |  |  |
| Improving the Leaking of Augmentations in Data-Efficient GANs via Adaptive Negative Data Augmentation |  |  |  |
| P2D: Plug and Play Discriminator for Accelerating GAN Frameworks |  |  |  |
| Personalized Face Inpainting with Diffusion Models by Parallel Visual Attention |  |  |  |
| Semantic Generative Augmentations for Few-Shot Counting |  |  |  |
| StyleGAN-Fusion: Diffusion Guided Domain Adaptation of Image Generators |  |  |  |
