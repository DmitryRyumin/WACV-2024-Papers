# WACV-2024-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/explainable_fair_accountable-privacy-preserving.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/WACV-2024-Papers/blob/main/sections/vision_language_and_other_modalities.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Generative Models for Image, Video, 3D, etc

![Section Papers](https://img.shields.io/badge/Section%20Papers-14-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-10-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-8-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-2-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| [Spiking Denoising Diffusion Probabilistic Models](https://openaccess.thecvf.com/content/WACV2024/html/Cao_Spiking_Denoising_Diffusion_Probabilistic_Models_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/AndyCao1125/SDDPM?style=flat)](https://github.com/AndyCao1125/SDDPM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Cao_Spiking_Denoising_Diffusion_Probabilistic_Models_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.17046-b31b1b.svg)](https://arxiv.org/abs/2306.17046) | :heavy_minus_sign: |
| [Improving the Effectiveness of Deep Generative Data](https://openaccess.thecvf.com/content/WACV2024/html/Wang_Improving_the_Effectiveness_of_Deep_Generative_Data_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Improving_the_Effectiveness_of_Deep_Generative_Data_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.03959-b31b1b.svg)](https://arxiv.org/abs/2311.03959) | :heavy_minus_sign: |
| [Customizing 360-Degree Panoramas through Text-to-Image Diffusion Models](https://openaccess.thecvf.com/content/WACV2024/html/Wang_Customizing_360-Degree_Panoramas_Through_Text-to-Image_Diffusion_Models_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://littlewhitesea.github.io/stitchdiffusion.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/littlewhitesea/StitchDiffusion?style=flat)](https://github.com/littlewhitesea/StitchDiffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Customizing_360-Degree_Panoramas_Through_Text-to-Image_Diffusion_Models_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.18840-b31b1b.svg)](https://arxiv.org/abs/2310.18840) | :heavy_minus_sign: |
| [Label Augmentation as Inter-Class Data Augmentation for Conditional Image Synthesis with Imbalanced Data](https://openaccess.thecvf.com/content/WACV2024/html/Katsumata_Label_Augmentation_As_Inter-Class_Data_Augmentation_for_Conditional_Image_Synthesis_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/raven38/softlabel-gan?style=flat)](https://github.com/raven38/softlabel-gan) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Katsumata_Label_Augmentation_As_Inter-Class_Data_Augmentation_for_Conditional_Image_Synthesis_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [3D-Aware Talking-Head Video Motion Transfer](https://openaccess.thecvf.com/content/WACV2024/html/Ni_3D-Aware_Talking-Head_Video_Motion_Transfer_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ni_3D-Aware_Talking-Head_Video_Motion_Transfer_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.02549-b31b1b.svg)](https://arxiv.org/abs/2311.02549) | :heavy_minus_sign: |
| [GRIT: GAN Residuals for Paired Image-to-Image Translation](https://openaccess.thecvf.com/content/WACV2024/html/Suri_GRIT_GAN_Residuals_for_Paired_Image-to-Image_Translation_WACV_2024_paper.html) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.cs.umd.edu/~sakshams/grit/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Suri_GRIT_GAN_Residuals_for_Paired_Image-to-Image_Translation_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Multimodality-Guided Image Style Transfer using Cross-Modal GAN Inversion](https://openaccess.thecvf.com/content/WACV2024/html/Wang_Multimodality-Guided_Image_Style_Transfer_Using_Cross-Modal_GAN_Inversion_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://hywang66.github.io/mmist/) <br /> [![GitHub](https://img.shields.io/github/stars/hywang66/mmist?style=flat)](https://github.com/hywang66/mmist) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Multimodality-Guided_Image_Style_Transfer_Using_Cross-Modal_GAN_Inversion_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.01671-b31b1b.svg)](https://arxiv.org/abs/2312.01671) | :heavy_minus_sign: |
| [ZIGNeRF: Zero-Shot 3D Scene Representation with Invertible Generative Neural Radiance Fields](https://openaccess.thecvf.com/content/WACV2024/html/Ko_ZIGNeRF_Zero-Shot_3D_Scene_Representation_With_Invertible_Generative_Neural_Radiance_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Ko_ZIGNeRF_Zero-Shot_3D_Scene_Representation_With_Invertible_Generative_Neural_Radiance_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.02741-b31b1b.svg)](https://arxiv.org/abs/2306.02741) | :heavy_minus_sign: |
| [GraphFill: Deep Image Inpainting using Graphs](https://openaccess.thecvf.com/content/WACV2024/html/Verma_GraphFill_Deep_Image_Inpainting_Using_Graphs_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://shash29-dev.github.io/GraphFill/) <br /> [![GitHub](https://img.shields.io/github/stars/shash29-dev/GraphFill?style=flat)](https://github.com/shash29-dev/GraphFill) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Verma_GraphFill_Deep_Image_Inpainting_Using_Graphs_WACV_2024_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vT1mfen7Kgw) |
| [PoseDiff: Pose-Conditioned Multimodal Diffusion Model for Unbounded Scene Synthesis from Sparse Inputs](https://openaccess.thecvf.com/content/WACV2024/html/Lee_PoseDiff_Pose-Conditioned_Multimodal_Diffusion_Model_for_Unbounded_Scene_Synthesis_From_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Lee_PoseDiff_Pose-Conditioned_Multimodal_Diffusion_Model_for_Unbounded_Scene_Synthesis_From_WACV_2024_paper.pdf) | :heavy_minus_sign: |
| [Nested Diffusion Processes for Anytime Image Generation](https://openaccess.thecvf.com/content/WACV2024/html/Elata_Nested_Diffusion_Processes_for_Anytime_Image_Generation_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://noamelata.github.io/NestedDiffusion/) <br /> [![GitHub](https://img.shields.io/github/stars/noamelata/NestedDiffusion?style=flat)](https://github.com/noamelata/NestedDiffusion) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-demo-FFD21F.svg)](https://huggingface.co/spaces/noamelata/Nested-Diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Elata_Nested_Diffusion_Processes_for_Anytime_Image_Generation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.19066-b31b1b.svg)](https://arxiv.org/abs/2305.19066) | :heavy_minus_sign: |
| [Expanding Expressiveness of Diffusion Models with Limited Data via Self-Distillation based Fine-Tuning](https://openaccess.thecvf.com/content/WACV2024/html/Hur_Expanding_Expressiveness_of_Diffusion_Models_With_Limited_Data_via_Self-Distillation_WACV_2024_paper.html) | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Hur_Expanding_Expressiveness_of_Diffusion_Models_With_Limited_Data_via_Self-Distillation_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2311.01018-b31b1b.svg)](https://arxiv.org/abs/2311.01018) | :heavy_minus_sign: |
| [One Style is All You Need to Generate a Video](https://openaccess.thecvf.com/content/WACV2024/html/Manandhar_One_Style_Is_All_You_Need_To_Generate_a_Video_WACV_2024_paper.html) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sandman002.github.io/CTSVG/) <br /> [![GitHub](https://img.shields.io/github/stars/sandman002/One-Style-is-All-You-Need-to-Generate-a-Video?style=flat)](https://github.com/sandman002/One-Style-is-All-You-Need-to-Generate-a-Video) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Manandhar_One_Style_Is_All_You_Need_To_Generate_a_Video_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.17835-b31b1b.svg)](https://arxiv.org/abs/2310.17835) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=bsMazroFH6k) |
| [Consistent Multimodal Generation via a Unified GAN Framework](https://openaccess.thecvf.com/content/WACV2024/html/Zhu_Consistent_Multimodal_Generation_via_a_Unified_GAN_Framework_WACV_2024_paper.html) | [![GitHub](https://img.shields.io/github/stars/jessemelpolio/MultimodalGAN?style=flat)](https://github.com/jessemelpolio/MultimodalGAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/WACV2024/papers/Zhu_Consistent_Multimodal_Generation_via_a_Unified_GAN_Framework_WACV_2024_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.01425-b31b1b.svg)](https://arxiv.org/abs/2307.01425) | :heavy_minus_sign: |
| Unsupervised Co-Generation of Foreground-Background Segmentation from Text-to-Image Synthesis |  |  |  |
| MotionGPT: Human Motion Synthesis with Improved Diversity and Realism via GPT-3 Prompting |  |  |  |
| Human Motion Aware Text-to-Video Generation with Explicit Camera Control |  |  |  |
| Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation |  |  |  |
| Second-Order Graph ODEs for Multi-Agent Trajectory Forecasting |  |  |  |
| Unified Concept Editing in Diffusion Models |  |  |  |
| SpectralCLIP: Preventing Artifacts in Text-Guided Style Transfer from a Spectral Perspective |  |  |  |
| Diffusion-based Generation of Histopathological whole Slide Images at a Gigapixel Scale |  |  |  |
| Painterly Image Harmonization via Adversarial Residual Learning |  |  |  |
| Training-Free Content Injection using H-Space in Diffusion Models |  |  |  |
| ENTED: Enhanced Neural Texture Extraction and Distribution for Reference-based Blind Face Restoration |  |  |  |
| PIDiffu: Pixel-Aligned Diffusion Model for High-Fidelity Clothed Human Reconstruction |  |  |  |
| PathLDM: Text Conditioned Latent Diffusion Model for Histopathology |  |  |  |
| Content-Aware Image Color Editing with Auxiliary Color Restoration Tasks |  |  |  |
| On Manipulating Scene Text in the Wild with Diffusion Models |  |  |  |
| CXR-IRGen: An Integrated Vision and Language Model for the Generation of Clinically Accurate Chest X-Ray Image-Report Pairs |  |  |  |
| Face Identity-Aware Disentanglement in StyleGAN |  |  |  |
| Text-to-Image Editing by Image Information Removal |  |  |  |
| Preserving Image Properties through Initializations in Diffusion Models |  |  |  |
| GC-VTON: Predicting Globally Consistent and Occlusion Aware Local Flows with Neighborhood Integrity Preservation for Virtual Try-On |  |  |  |
| Generation of Upright Panoramic Image from Non-Upright Panoramic Image |  |  |  |
| Fast Diffusion EM: A Diffusion Model for Blind Inverse Problems with Application to Deconvolution |  |  |  |
| Enforcing Sparsity on Latent Space for Robust and Explainable Representations |  |  |  |
| Diff2Lip: Audio Conditioned Diffusion Models for Lip-Synchronization |  |  |  |
| 3D Reconstruction of Interacting Multi-Person in Clothing from a Single Image |  |  |  |
| Revisiting Latent Space of GAN Inversion for Robust Real Image Editing |  |  |  |
| Soft Curriculum for Learning Conditional GANs with Noisy-Labeled and Uncurated Unlabeled Data |  |  |  |
| Bipartite Graph Diffusion Model for Human Interaction Generation |  |  |  |
| Training-Free Layout Control with Cross-Attention Guidance |  |  |  |
| Controllable Image Synthesis of Industrial Data using Stable Diffusion |  |  |  |
| Removing the Quality Tax in Controllable Face Generation |  |  |  |
| Hierarchical Diffusion Autoencoders and Disentangled Image Manipulation |  |  |  |
| FacadeNet: Conditional Facade Synthesis via Selective Editing |  |  |  |
| TPSeNCE: Towards Artifact-Free Realistic Rain Generation for Deraining and Object Detection in Rain |  |  |  |
| Common Diffusion Noise Schedules and Sample Steps are Flawed |  |  |  |
| Improving the Leaking of Augmentations in Data-Efficient GANs via Adaptive Negative Data Augmentation |  |  |  |
| P2D: Plug and Play Discriminator for Accelerating GAN Frameworks |  |  |  |
| Personalized Face Inpainting with Diffusion Models by Parallel Visual Attention |  |  |  |
| Semantic Generative Augmentations for Few-Shot Counting |  |  |  |
| StyleGAN-Fusion: Diffusion Guided Domain Adaptation of Image Generators |  |  |  |
