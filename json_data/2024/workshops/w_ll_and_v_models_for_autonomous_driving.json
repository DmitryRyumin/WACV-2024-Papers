[
  {
    "title": "Human-Centric Autonomous Systems with LLMs for User Command Reasoning",
    "base_url": "https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD",
    "title_page": "/html/Yang_Human-Centric_Autonomous_Systems_With_LLMs_for_User_Command_Reasoning_WACVW_2024_paper.html",
    "ieee_id": null,
    "github": "KTH-RPL/DriveCmd_LLM",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "/papers/Yang_Human-Centric_Autonomous_Systems_With_LLMs_for_User_Command_Reasoning_WACVW_2024_paper.pdf",
    "paper_arxiv_id": "2311.08206",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Large Language and Vision Models for Autonomous Driving"
  },
  {
    "title": "LIP-Loc: LiDAR Image Pretraining for Cross-Modal Localization",
    "base_url": "https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD",
    "title_page": "/html/Shubodh_LIP-Loc_LiDAR_Image_Pretraining_for_Cross-Modal_Localization_WACVW_2024_paper.html",
    "ieee_id": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "/papers/Shubodh_LIP-Loc_LiDAR_Image_Pretraining_for_Cross-Modal_Localization_WACVW_2024_paper.pdf",
    "paper_arxiv_id": "2312.16648",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Large Language and Vision Models for Autonomous Driving"
  },
  {
    "title": "A Survey on Multimodal Large Language Models for Autonomous Driving",
    "base_url": "https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD",
    "title_page": "/html/Cui_A_Survey_on_Multimodal_Large_Language_Models_for_Autonomous_Driving_WACVW_2024_paper.html",
    "ieee_id": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "/papers/Cui_A_Survey_on_Multimodal_Large_Language_Models_for_Autonomous_Driving_WACVW_2024_paper.pdf",
    "paper_arxiv_id": "2311.12320",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Large Language and Vision Models for Autonomous Driving"
  },
  {
    "title": "VLAAD: Vision and Language Assistant for Autonomous Driving",
    "base_url": "https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD",
    "title_page": "/html/Park_VLAAD_Vision_and_Language_Assistant_for_Autonomous_Driving_WACVW_2024_paper.html",
    "ieee_id": null,
    "github": "sungyeonparkk/vision-assistant-for-driving",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "/papers/Park_VLAAD_Vision_and_Language_Assistant_for_Autonomous_Driving_WACVW_2024_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Large Language and Vision Models for Autonomous Driving"
  },
  {
    "title": "Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles",
    "base_url": "https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD",
    "title_page": "/html/Cui_Drive_As_You_Speak_Enabling_Human-Like_Interaction_With_Large_Language_WACVW_2024_paper.html",
    "ieee_id": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "/papers/Cui_Drive_As_You_Speak_Enabling_Human-Like_Interaction_With_Large_Language_WACVW_2024_paper.pdf",
    "paper_arxiv_id": "2309.10228",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Large Language and Vision Models for Autonomous Driving"
  },
  {
    "title": "NuScenes-MQA: Integrated Evaluation of Captions and QA for Autonomous Driving Datasets using Markup Annotations",
    "base_url": "https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD",
    "title_page": "/html/Inoue_NuScenes-MQA_Integrated_Evaluation_of_Captions_and_QA_for_Autonomous_Driving_WACVW_2024_paper.html",
    "ieee_id": null,
    "github": "turingmotors/NuScenes-MQA",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "/papers/Inoue_NuScenes-MQA_Integrated_Evaluation_of_Captions_and_QA_for_Autonomous_Driving_WACVW_2024_paper.pdf",
    "paper_arxiv_id": "2312.06352",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Large Language and Vision Models for Autonomous Driving"
  },
  {
    "title": "Latency Driven Spatially Sparse Optimization for Multi-Branch CNNs for Semantic Segmentation",
    "base_url": "https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD",
    "title_page": "/html/Zampokas_Latency_Driven_Spatially_Sparse_Optimization_for_Multi-Branch_CNNs_for_Semantic_WACVW_2024_paper.html",
    "ieee_id": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "/papers/Zampokas_Latency_Driven_Spatially_Sparse_Optimization_for_Multi-Branch_CNNs_for_Semantic_WACVW_2024_paper.pdf",
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Large Language and Vision Models for Autonomous Driving"
  },
  {
    "title": "A Safer Vision-based Autonomous Planning System for Quadrotor UAVs with Dynamic Obstacle Trajectory Prediction and Its Application with LLMs",
    "base_url": "https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD",
    "title_page": "/html/Zhong_A_Safer_Vision-Based_Autonomous_Planning_System_for_Quadrotor_UAVs_With_WACVW_2024_paper.html",
    "ieee_id": null,
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": "/papers/Zhong_A_Safer_Vision-Based_Autonomous_Planning_System_for_Quadrotor_UAVs_With_WACVW_2024_paper.pdf",
    "paper_arxiv_id": "2311.12893",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Large Language and Vision Models for Autonomous Driving"
  }
]